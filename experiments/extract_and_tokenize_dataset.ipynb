{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The QED_data.txt, is a compiled version of all the collected datafrom the given dataset \n",
    "* It contains the compiled version of QED-2-to-2-diag-TreeLevel-{0 - 9}.txt \n",
    "* The Dataset will be converted into the csv format \n",
    "* the CSV format will consists of two attributes named, text and label \n",
    "* text attribute is the attribute, Where it consists information about the Particle Interaction, to Vertex to Amplitude \n",
    "* The Label column is the Squared Amplitude that is the prediction task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../QED_data/QED_data.txt'\n",
    "output_file = '../QED_data/processed_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'Interaction:(.*?)\\s*:\\s*.*:\\s*(.*?)\\s*:\\s*(.*)'\n",
    "\n",
    "data = []\n",
    "current_block = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file, 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if not line: \n",
    "            continue\n",
    "\n",
    "        last_colon_index = line.rfind(':')\n",
    "        if last_colon_index == -1:  \n",
    "            print(f\"Skipping line (no colon found): {line}\")\n",
    "            continue\n",
    "\n",
    "        input_text = line[:last_colon_index].strip()\n",
    "        squared_amplitude = line[last_colon_index + 1:].strip()\n",
    "\n",
    "        input_text = re.sub(r'\\bInteraction\\b', '', input_text)\n",
    "        input_text = re.sub(r'\\bOffShell\\b', '', input_text)\n",
    "        input_text = re.sub(r'\\bVertex\\b', '', input_text)\n",
    "        input_text = re.sub(r'(e|c|u|X)_(\\w+)_\\d+', r'\\1_\\2_[STATE_ID]', input_text)\n",
    "\n",
    "        input_text = re.sub(r'%\\w+_(\\d+)', r'%\\g<0>_[STATE_ID]', input_text)\n",
    "        input_text = re.sub(r'%(\\w+)_\\d+_[STATE_ID]', r'%\\1_[STATE_ID]', input_text)\n",
    "\n",
    "        input_text = re.sub(r'(\\w+)_{(\\w+)_(\\d+),(%\\w+_\\d+)}', r'\\1_{\\2_[STATE_ID],\\4_[STATE_ID]}', input_text)\n",
    "        input_text = re.sub(r'%\\w+_(\\d+)', r'%\\g<0>_[STATE_ID]', input_text)\n",
    "        input_text = re.sub(r'%(\\w+)_\\d+_[STATE_ID]', r'%\\1_[STATE_ID]', input_text)\n",
    "\n",
    "        input_text = re.sub(r'\\((.*?)\\)', '(X)', input_text)\n",
    "        input_text = re.sub(r'\\^\\((.*?)\\)', '^(*)', input_text)\n",
    "\n",
    "        input_text = re.sub(r'u_\\(\\*\\)', 'u_(*)', input_text)\n",
    "        input_text = re.sub(r'v_\\(\\*\\)', 'v_(*)', input_text)\n",
    "\n",
    "        input_text = re.sub(r':+', ' ', input_text)\n",
    "\n",
    "        input_text = re.sub(r'[\"\\']', '', input_text)\n",
    "        input_text = re.sub(r',+', ',', input_text)\n",
    "        input_text = re.sub(r',\\s*', ' ', input_text)  # Replace \", \" with \" \"\n",
    "        input_text = re.sub(r'\\s+', ' ', input_text).strip()\n",
    "        input_text = re.sub(r'^,+|,+$', '', input_text)\n",
    "\n",
    "        squared_amplitude = re.sub(r'[\"\\']', '', squared_amplitude)\n",
    "        squared_amplitude = re.sub(r'\\s+', ' ', squared_amplitude).strip()\n",
    "\n",
    "        data.append({\n",
    "            'text': input_text,\n",
    "            'label': squared_amplitude\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to some_random_run.csv\n",
      "First 5 data entries:\n",
      "{'text': 'e_gam_[STATE_ID](X)^(*) e_del_[STATE_ID](X)^(*) to e_eps_[STATE_ID](X) e_eta_[STATE_ID](X) V_1 e(X) e(X) A(X) V_0 e(X) e(X) A(X) -1/2*i*e^2*gamma_{+%\\\\sigma_165 %%%gam_145_[STATE_ID]_[STATE_ID] %%%gam_146_[STATE_ID]_[STATE_ID]}*gamma_{%\\\\sigma_165 %%%gam_147_[STATE_ID]_[STATE_ID] %%%del_137_[STATE_ID]_[STATE_ID]}*e_{i_3 %%%gam_146_[STATE_ID]_[STATE_ID]}(X)_u*e_{k_3 %%%del_137_[STATE_ID]_[STATE_ID]}(X)_u*e_{l_3 %%%gam_145_[STATE_ID]_[STATE_ID]}(X)_u^(*)*e_{i_5 %%%gam_147_[STATE_ID]_[STATE_ID]}(X)_u^(*)/(X)', 'label': '2*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23 + -1/2*m_e^2*s_24 + 1/2*s_12*s_34)*(m_e^2 + -s_13 + 1/2*reg_prop)^(-2)'}\n",
      "{'text': 'e_gam_[STATE_ID](X)^(*) e_del_[STATE_ID](X)^(*) to e_eps_[STATE_ID](X) e_eta_[STATE_ID](X) V_0 e(X) e(X) A(X) V_1 e(X) e(X) A(X) 1/2*i*e^2*gamma_{+%\\\\sigma_172 %%%gam_162_[STATE_ID]_[STATE_ID] %%%del_144_[STATE_ID]_[STATE_ID]}*gamma_{%\\\\sigma_172 %%%gam_163_[STATE_ID]_[STATE_ID] %%%gam_164_[STATE_ID]_[STATE_ID]}*e_{i_3 %%%gam_164_[STATE_ID]_[STATE_ID]}(X)_u*e_{k_3 %%%del_144_[STATE_ID]_[STATE_ID]}(X)_u*e_{l_3 %%%gam_162_[STATE_ID]_[STATE_ID]}(X)_u^(*)*e_{i_5 %%%gam_163_[STATE_ID]_[STATE_ID]}(X)_u^(*)/(X)', 'label': '2*e^4*(m_e^4 + -1/2*m_e^2*s_14 + -1/2*m_e^2*s_23 + 1/2*s_13*s_24 + 1/2*s_12*s_34)*(m_e^2 + -s_23 + 1/2*reg_prop)^(-2)'}\n",
      "{'text': 'e_gam_[STATE_ID](X)^(*) e_del_[STATE_ID](X)^(*) to e_eps_[STATE_ID](X) e_del_[STATE_ID](X) V_1 e(X) e(X) A(X) V_0 e(X) e(X) A(X) -1/2*i*e^2*gamma_{+%\\\\sigma_293 %%%gam_358_[STATE_ID]_[STATE_ID] %%%gam_359_[STATE_ID]_[STATE_ID]}*gamma_{%\\\\sigma_293 %%%gam_360_[STATE_ID]_[STATE_ID] %%%del_271_[STATE_ID]_[STATE_ID]}*e_{i_27 %%%gam_360_[STATE_ID]_[STATE_ID]}(X)_u^(*)*e_{l_15 %%%gam_358_[STATE_ID]_[STATE_ID]}(X)_u^(*)*e_{k_15 %%%del_271_[STATE_ID]_[STATE_ID]}(X)_u*e_{j_5 %%%gam_359_[STATE_ID]_[STATE_ID]}(X)_u/(X)', 'label': '2*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23 + -1/2*m_e^2*s_24 + 1/2*s_12*s_34)*(m_e^2 + -s_13 + 1/2*reg_prop)^(-2)'}\n",
      "{'text': 'e_gam_[STATE_ID](X)^(*) e_del_[STATE_ID](X)^(*) to e_eps_[STATE_ID](X) e_del_[STATE_ID](X) V_0 e(X) e(X) A(X) V_1 e(X) e(X) A(X) 1/2*i*e^2*gamma_{+%\\\\sigma_301 %%%gam_377_[STATE_ID]_[STATE_ID] %%%del_278_[STATE_ID]_[STATE_ID]}*gamma_{%\\\\sigma_301 %%%gam_378_[STATE_ID]_[STATE_ID] %%%gam_379_[STATE_ID]_[STATE_ID]}*e_{i_27 %%%gam_378_[STATE_ID]_[STATE_ID]}(X)_u^(*)*e_{l_15 %%%gam_377_[STATE_ID]_[STATE_ID]}(X)_u^(*)*e_{k_15 %%%del_278_[STATE_ID]_[STATE_ID]}(X)_u*e_{j_5 %%%gam_379_[STATE_ID]_[STATE_ID]}(X)_u/(X)', 'label': '2*e^4*(m_e^4 + -1/2*m_e^2*s_14 + -1/2*m_e^2*s_23 + 1/2*s_13*s_24 + 1/2*s_12*s_34)*(m_e^2 + -s_23 + 1/2*reg_prop)^(-2)'}\n",
      "{'text': 'e_gam_[STATE_ID](X)^(*) e_del_[STATE_ID](X)^(*) to e_eps_[STATE_ID](X) e_eta_[STATE_ID](X) V_1 e(X) e(X) A(X) V_0 e(X) e(X) A(X) -i*e^2*gamma_{+%\\\\sigma_435 %%%gam_574_[STATE_ID]_[STATE_ID] %%%gam_575_[STATE_ID]_[STATE_ID]}*gamma_{%\\\\sigma_435 %%%gam_576_[STATE_ID]_[STATE_ID] %%%del_406_[STATE_ID]_[STATE_ID]}*e_{i_43 %%%gam_576_[STATE_ID]_[STATE_ID]}(X)_u^(*)*e_{l_31 %%%gam_574_[STATE_ID]_[STATE_ID]}(X)_u^(*)*e_{k_31 %%%del_406_[STATE_ID]_[STATE_ID]}(X)_u*e_{j_21 %%%gam_575_[STATE_ID]_[STATE_ID]}(X)_u/(X)*s_13 + s_33 + reg_prop)', 'label': '8*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23 + -1/2*m_e^2*s_24 + 1/2*s_12*s_34)*(m_e^2 + (-2)*s_13 + s_33 + reg_prop)^(-2)'}\n",
      "\n",
      "Number of unique tokens in the dataset: 199725\n",
      "Sample tokens: ['%%%eta_186404_[STATE_ID]_[STATE_ID]}*A_{k_27419', '-2/9*i*e^2*gamma_{+%\\\\sigma_81273', 'A_\\\\rho_514219(X)', '%%%del_29585_[STATE_ID]_[STATE_ID]}(X)_u/(X)', '+%\\\\nu_98171}(X)^(*)*d_{k_31509', '%%%gam_41949_[STATE_ID]_[STATE_ID]}*gamma_{%\\\\sigma_19827', '%%%del_22781_[STATE_ID]_[STATE_ID]}(X)_v*t_{j_11757', '%%%gam_324403_[STATE_ID]_[STATE_ID]}*A_{l_220317', '%%%eta_85346_[STATE_ID]_[STATE_ID]}*gamma_{%\\\\nu_89805', '+%\\\\sigma_142042}(X)*u_{i_27039', '%%%gam_88626_[STATE_ID]_[STATE_ID]}*A_{j_32565', '%%%gam_188651_[STATE_ID]_[STATE_ID]}(X)_v^(*)*t_{j_65395', '%%%eta_7215_[STATE_ID]_[STATE_ID]}(X)_v^(*)*mu_{i_5876', '2/9*i*e^2*(X)*A_{k_52411', 'p_3_%\\\\nu_218591*gamma_{+%\\\\nu_218591', '(X)*p_3_%\\\\tau_111315*gamma_{%\\\\tau_111313', '%%%eta_225175_[STATE_ID]_[STATE_ID]}(X)_u^(*))/(X)', '+%\\\\lambda_38438}(X)*A_{k_10509', '%%%del_70174_[STATE_ID]_[STATE_ID]}(X)_u*s_{i_56587', '-1/3*i*e^2*gamma_{+%\\\\tau_76339']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(data)\n",
    "output_csv_path = \"some_random_run.csv\"  \n",
    "df.to_csv(output_csv_path, index=False)\n",
    "print(f\"Preprocessed data saved to {output_csv_path}\")\n",
    "\n",
    "print(\"First 5 data entries:\")\n",
    "for entry in data[:5]:\n",
    "    print(entry)\n",
    "\n",
    "all_texts = df['text'].astype(str).tolist() + df['label'].astype(str).tolist()\n",
    "\n",
    "all_tokens = set()\n",
    "for text in all_texts:\n",
    "    tokens = text.split()\n",
    "    all_tokens.update(tokens)\n",
    "\n",
    "print(f\"\\nNumber of unique tokens in the dataset: {len(all_tokens)}\")\n",
    "print(f\"Sample tokens: {list(all_tokens)[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data , columns= ['text' , 'label']) \n",
    "data.to_csv(output_file , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
