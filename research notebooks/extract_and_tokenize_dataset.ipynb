{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The QED_data.txt, is a compiled version of all the collected datafrom the given dataset \n",
    "* It contains the compiled version of QED-2-to-2-diag-TreeLevel-{0 - 9}.txt \n",
    "* The Dataset will be converted into the csv format \n",
    "* the CSV format will consists of two attributes named, text and label \n",
    "* text attribute is the attribute, Where it consists information about the Particle Interaction, to Vertex to Amplitude \n",
    "* The Label column is the Squared Amplitude that is the prediction task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../QED_data/QED_data.txt'\n",
    "output_file = '../QED_data/processed_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'Interaction:(.*?)\\s*:\\s*.*:\\s*(.*?)\\s*:\\s*(.*)'\n",
    "\n",
    "data = []\n",
    "current_block = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to some_random_run.csv\n",
      "First 5 data entries:\n",
      "{'text': 'e_gam_[STATE_ID](X)^(*) e_del_[STATE_ID](X)^(*) to e_eps_[STATE_ID](X) e_eta_[STATE_ID](X) V_1 e(X) e(X) A(X) V_0 e(X) e(X) A(X) -1/2*i*e^2*gamma_{+%\\\\sigma_165 %%%gam_145_[STATE_ID]_[STATE_ID] %%%gam_146_[STATE_ID]_[STATE_ID]}*gamma_{%\\\\sigma_165 %%%gam_147_[STATE_ID]_[STATE_ID] %%%del_137_[STATE_ID]_[STATE_ID]}*e_{i_3 %%%gam_146_[STATE_ID]_[STATE_ID]}(X)_u*e_{k_3 %%%del_137_[STATE_ID]_[STATE_ID]}(X)_u*e_{l_3 %%%gam_145_[STATE_ID]_[STATE_ID]}(X)_u^(*)*e_{i_5 %%%gam_147_[STATE_ID]_[STATE_ID]}(X)_u^(*)/(X)', 'label': '2*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23 + -1/2*m_e^2*s_24 + 1/2*s_12*s_34)*(m_e^2 + -s_13 + 1/2*reg_prop)^(-2)'}\n",
      "{'text': 'e_gam_[STATE_ID](X)^(*) e_del_[STATE_ID](X)^(*) to e_eps_[STATE_ID](X) e_eta_[STATE_ID](X) V_0 e(X) e(X) A(X) V_1 e(X) e(X) A(X) 1/2*i*e^2*gamma_{+%\\\\sigma_172 %%%gam_162_[STATE_ID]_[STATE_ID] %%%del_144_[STATE_ID]_[STATE_ID]}*gamma_{%\\\\sigma_172 %%%gam_163_[STATE_ID]_[STATE_ID] %%%gam_164_[STATE_ID]_[STATE_ID]}*e_{i_3 %%%gam_164_[STATE_ID]_[STATE_ID]}(X)_u*e_{k_3 %%%del_144_[STATE_ID]_[STATE_ID]}(X)_u*e_{l_3 %%%gam_162_[STATE_ID]_[STATE_ID]}(X)_u^(*)*e_{i_5 %%%gam_163_[STATE_ID]_[STATE_ID]}(X)_u^(*)/(X)', 'label': '2*e^4*(m_e^4 + -1/2*m_e^2*s_14 + -1/2*m_e^2*s_23 + 1/2*s_13*s_24 + 1/2*s_12*s_34)*(m_e^2 + -s_23 + 1/2*reg_prop)^(-2)'}\n",
      "{'text': 'e_gam_[STATE_ID](X)^(*) e_del_[STATE_ID](X)^(*) to e_eps_[STATE_ID](X) e_del_[STATE_ID](X) V_1 e(X) e(X) A(X) V_0 e(X) e(X) A(X) -1/2*i*e^2*gamma_{+%\\\\sigma_293 %%%gam_358_[STATE_ID]_[STATE_ID] %%%gam_359_[STATE_ID]_[STATE_ID]}*gamma_{%\\\\sigma_293 %%%gam_360_[STATE_ID]_[STATE_ID] %%%del_271_[STATE_ID]_[STATE_ID]}*e_{i_27 %%%gam_360_[STATE_ID]_[STATE_ID]}(X)_u^(*)*e_{l_15 %%%gam_358_[STATE_ID]_[STATE_ID]}(X)_u^(*)*e_{k_15 %%%del_271_[STATE_ID]_[STATE_ID]}(X)_u*e_{j_5 %%%gam_359_[STATE_ID]_[STATE_ID]}(X)_u/(X)', 'label': '2*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23 + -1/2*m_e^2*s_24 + 1/2*s_12*s_34)*(m_e^2 + -s_13 + 1/2*reg_prop)^(-2)'}\n",
      "{'text': 'e_gam_[STATE_ID](X)^(*) e_del_[STATE_ID](X)^(*) to e_eps_[STATE_ID](X) e_del_[STATE_ID](X) V_0 e(X) e(X) A(X) V_1 e(X) e(X) A(X) 1/2*i*e^2*gamma_{+%\\\\sigma_301 %%%gam_377_[STATE_ID]_[STATE_ID] %%%del_278_[STATE_ID]_[STATE_ID]}*gamma_{%\\\\sigma_301 %%%gam_378_[STATE_ID]_[STATE_ID] %%%gam_379_[STATE_ID]_[STATE_ID]}*e_{i_27 %%%gam_378_[STATE_ID]_[STATE_ID]}(X)_u^(*)*e_{l_15 %%%gam_377_[STATE_ID]_[STATE_ID]}(X)_u^(*)*e_{k_15 %%%del_278_[STATE_ID]_[STATE_ID]}(X)_u*e_{j_5 %%%gam_379_[STATE_ID]_[STATE_ID]}(X)_u/(X)', 'label': '2*e^4*(m_e^4 + -1/2*m_e^2*s_14 + -1/2*m_e^2*s_23 + 1/2*s_13*s_24 + 1/2*s_12*s_34)*(m_e^2 + -s_23 + 1/2*reg_prop)^(-2)'}\n",
      "{'text': 'e_gam_[STATE_ID](X)^(*) e_del_[STATE_ID](X)^(*) to e_eps_[STATE_ID](X) e_eta_[STATE_ID](X) V_1 e(X) e(X) A(X) V_0 e(X) e(X) A(X) -i*e^2*gamma_{+%\\\\sigma_435 %%%gam_574_[STATE_ID]_[STATE_ID] %%%gam_575_[STATE_ID]_[STATE_ID]}*gamma_{%\\\\sigma_435 %%%gam_576_[STATE_ID]_[STATE_ID] %%%del_406_[STATE_ID]_[STATE_ID]}*e_{i_43 %%%gam_576_[STATE_ID]_[STATE_ID]}(X)_u^(*)*e_{l_31 %%%gam_574_[STATE_ID]_[STATE_ID]}(X)_u^(*)*e_{k_31 %%%del_406_[STATE_ID]_[STATE_ID]}(X)_u*e_{j_21 %%%gam_575_[STATE_ID]_[STATE_ID]}(X)_u/(X)*s_13 + s_33 + reg_prop)', 'label': '8*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23 + -1/2*m_e^2*s_24 + 1/2*s_12*s_34)*(m_e^2 + (-2)*s_13 + s_33 + reg_prop)^(-2)'}\n",
      "\n",
      "Number of unique tokens in the dataset: 199725\n",
      "Sample tokens: ['%%%eta_103110_[STATE_ID]_[STATE_ID]}(X)_v^(*)', '%%%gam_457939_[STATE_ID]_[STATE_ID]', '+%\\\\tau_265323}(X)*b_{i_55265', '%%%gam_249223_[STATE_ID]_[STATE_ID]', '%%%eta_258534_[STATE_ID]_[STATE_ID]', '%%%gam_136986_[STATE_ID]_[STATE_ID]}*gamma_{%\\\\sigma_140654', 's_del_33850(X)', '%%%eps_104825_[STATE_ID]_[STATE_ID]}*A_{k_28075', '%%%gam_359595_[STATE_ID]_[STATE_ID]}*A_{j_172521', '+%\\\\tau_715216}(X)^(*)*A_{l_270259', '%%%eta_222592_[STATE_ID]_[STATE_ID]}*A_{j_91497', '-p_3_%\\\\lambda_316727*gamma_{+%\\\\lambda_316727', '%%%eta_6706_[STATE_ID]_[STATE_ID]}(X)_v^(*)*mu_{j_5625', '%%%eps_133124_[STATE_ID]_[STATE_ID]}(X)_u*s_{i_145065', '%%%gam_413829_[STATE_ID]_[STATE_ID]}(X)_u^(*))/(X)', '%%%gam_167522_[STATE_ID]_[STATE_ID]}*gamma_{%\\\\tau_395685', '4/9*i*e^2*(X)*A_{k_52541', '%%%gam_49533_[STATE_ID]_[STATE_ID]}(X)_u^(*)/(X)', '%%%eta_55314_[STATE_ID]_[STATE_ID]', '%%%eta_51085_[STATE_ID]_[STATE_ID]}(X)_v^(*)*s_{l_30491']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize list to store data\n",
    "data = []\n",
    "  # Replace with your file path\n",
    "\n",
    "# Read the file and process each line\n",
    "with open(input_file, 'r') as file:\n",
    "    for line in file:\n",
    "        # Remove leading/trailing whitespace\n",
    "        line = line.strip()\n",
    "        if not line:  # Skip empty lines\n",
    "            continue\n",
    "\n",
    "        # Find the last colon in the line to separate text and label\n",
    "        last_colon_index = line.rfind(':')\n",
    "        if last_colon_index == -1:  # No colon found, skip the line\n",
    "            print(f\"Skipping line (no colon found): {line}\")\n",
    "            continue\n",
    "\n",
    "        # Split the line into text (before last colon) and label (after last colon)\n",
    "        input_text = line[:last_colon_index].strip()\n",
    "        squared_amplitude = line[last_colon_index + 1:].strip()\n",
    "\n",
    "        # Cleaning steps for input_text\n",
    "        # 1. Remove irrelevant terms\n",
    "        input_text = re.sub(r'\\bInteraction\\b', '', input_text)\n",
    "        input_text = re.sub(r'\\bOffShell\\b', '', input_text)\n",
    "        input_text = re.sub(r'\\bVertex\\b', '', input_text)\n",
    "\n",
    "        # 2. Preserve state names and replace numeric IDs\n",
    "        input_text = re.sub(r'(e|c|u|X)_(\\w+)_\\d+', r'\\1_\\2_[STATE_ID]', input_text)\n",
    "\n",
    "        # 3. Handle gamma terms (e.g., gamma_{+%\\sigma_8689,%gam_11966,%eps_1132})\n",
    "        # Replace each %state_numeric with %state_[STATE_ID]\n",
    "        input_text = re.sub(r'%\\w+_(\\d+)', r'%\\g<0>_[STATE_ID]', input_text)\n",
    "        input_text = re.sub(r'%(\\w+)_\\d+_[STATE_ID]', r'%\\1_[STATE_ID]', input_text)\n",
    "\n",
    "        # 4. Clean tensor terms (e.g., e_{k_919,%del_8324} â†’ e_{k_[STATE_ID],%del_[STATE_ID]})\n",
    "        input_text = re.sub(r'(\\w+)_{(\\w+)_(\\d+),(%\\w+_\\d+)}', r'\\1_{\\2_[STATE_ID],\\4_[STATE_ID]}', input_text)\n",
    "        input_text = re.sub(r'%\\w+_(\\d+)', r'%\\g<0>_[STATE_ID]', input_text)\n",
    "        input_text = re.sub(r'%(\\w+)_\\d+_[STATE_ID]', r'%\\1_[STATE_ID]', input_text)\n",
    "\n",
    "        # 5. Clean parentheses and conjugates\n",
    "        input_text = re.sub(r'\\((.*?)\\)', '(X)', input_text)\n",
    "        input_text = re.sub(r'\\^\\((.*?)\\)', '^(*)', input_text)\n",
    "\n",
    "        # 6. Fix complex function calls\n",
    "        input_text = re.sub(r'u_\\(\\*\\)', 'u_(*)', input_text)\n",
    "        input_text = re.sub(r'v_\\(\\*\\)', 'v_(*)', input_text)\n",
    "\n",
    "        # 7. Remove colons (including double colons) between terms\n",
    "        input_text = re.sub(r':+', ' ', input_text)\n",
    "\n",
    "        # 8. Remove trailing quotes and extra commas\n",
    "        input_text = re.sub(r'[\"\\']', '', input_text)  # Remove quotes\n",
    "        input_text = re.sub(r',+', ',', input_text)\n",
    "        input_text = re.sub(r',\\s*', ' ', input_text)  # Replace \", \" with \" \"\n",
    "        input_text = re.sub(r'\\s+', ' ', input_text).strip()\n",
    "        input_text = re.sub(r'^,+|,+$', '', input_text)\n",
    "\n",
    "        # Cleaning for squared_amplitude (remove quotes, normalize spaces)\n",
    "        squared_amplitude = re.sub(r'[\"\\']', '', squared_amplitude)\n",
    "        squared_amplitude = re.sub(r'\\s+', ' ', squared_amplitude).strip()\n",
    "\n",
    "        # Append cleaned data\n",
    "        data.append({\n",
    "            'text': input_text,\n",
    "            'label': squared_amplitude\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame and save as CSV\n",
    "df = pd.DataFrame(data)\n",
    "output_csv_path = \"some_random_run.csv\"  # Replace with desired path\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "print(f\"Preprocessed data saved to {output_csv_path}\")\n",
    "\n",
    "# Debugging: Print first 5 entries\n",
    "print(\"First 5 data entries:\")\n",
    "for entry in data[:5]:\n",
    "    print(entry)\n",
    "\n",
    "# Step 2: Calculate unique tokens\n",
    "# Combine all text from both columns\n",
    "all_texts = df['text'].astype(str).tolist() + df['label'].astype(str).tolist()\n",
    "\n",
    "# Tokenize using whitespace and count unique tokens\n",
    "all_tokens = set()\n",
    "for text in all_texts:\n",
    "    tokens = text.split()\n",
    "    all_tokens.update(tokens)\n",
    "\n",
    "# Print the results\n",
    "print(f\"\\nNumber of unique tokens in the dataset: {len(all_tokens)}\")\n",
    "print(f\"Sample tokens: {list(all_tokens)[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to  ../QED_data/processed_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "data = pd.DataFrame(data , columns= ['text' , 'label']) \n",
    "data.to_csv(output_file , index = False)\n",
    "print('Data saved to ' , output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
