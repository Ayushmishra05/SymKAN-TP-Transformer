{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenizing the Converted CSV Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "M35cdK1oqK88"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors\n",
        "\n",
        "tokenizer = Tokenizer(models.BPE())\n",
        "\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
        "\n",
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", 1),\n",
        "        (\"[SEP]\", 2),\n",
        "        (\"[PAD]\", 0),\n",
        "        (\"[MASK]\", 3), \n",
        "        (\"[STATE_ID]\" , 4)\n",
        "    ],\n",
        ")\n",
        "\n",
        "trainer = trainers.BpeTrainer(\n",
        "    vocab_size=512,\n",
        "    min_frequency=2,\n",
        "    special_tokens=[\"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\" , \"[STATE_ID]\"]\n",
        ")\n",
        "\n",
        "files = [\"../QED_data/QED_data.txt\"]\n",
        "\n",
        "tokenizer.train(files, trainer)\n",
        "\n",
        "tokenizer.save(\"custom_tokenizer.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTOh9Jtfv1yW",
        "outputId": "404bef00-51f8-40f3-c565-d8dad93606eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('custom_tokenizer\\\\tokenizer_config.json',\n",
              " 'custom_tokenizer\\\\special_tokens_map.json',\n",
              " 'custom_tokenizer\\\\tokenizer.json')"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "hf_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"custom_tokenizer.json\")\n",
        "\n",
        "hf_tokenizer.cls_token = \"[CLS]\"\n",
        "hf_tokenizer.sep_token = \"[SEP]\"\n",
        "hf_tokenizer.pad_token = \"[PAD]\"\n",
        "hf_tokenizer.mask_token = \"[MASK]\"\n",
        "hf_tokenizer.mask_token = \"[STATE_ID]\"\n",
        "\n",
        "hf_tokenizer.save_pretrained(\"custom_tokenizer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['USE_TF'] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPod9ODb62pr",
        "outputId": "72f68a1b-bc53-4ad1-ddb3-a20291c8577b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizer'. \n",
            "The class this function is called from is 'RobertaTokenizerFast'.\n"
          ]
        }
      ],
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "\n",
        "tokenizer_path = 'custom_tokenizer'\n",
        "\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_path)\n",
        "\n",
        "sample = \"e_[ID](X)^(*) e_[ID](X)^(*) to e_[ID](X) e_[ID](X)\"\n",
        "tokens = tokenizer(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go-_o4h27V_3",
        "outputId": "8b6902f4-6ba7-49fb-cf36-4c278b1683f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([  1,  24, 189, 158,  33, 424, 207,   6,  30,   7, 107,  82, 189, 219,\n",
            "         33, 496, 193,   6,  30,   7, 169, 201,  33,  65,  33,   4,   6,  30,\n",
            "          7, 107, 201,  33, 199,  33,   4,   6,  30, 139,  98,  33,  13,  82,\n",
            "          6,   2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor([  1,   5,  65,  33, 115,  87, 215,  71,  49,  33,  16,  89,  53, 248,\n",
            "         46,  33, 112,  32,  15,  61,  67,  51,  33, 108,  61, 114,   8,  51,\n",
            "         33,  86,  61,  67,  51,  33, 103,  61,  67,  94,  33,  95,   7, 110,\n",
            "         67,   2])}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# ✅ Load the tokenizer\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"custom_tokenizer\")\n",
        "tokenizer.pad_token = tokenizer.eos_token if tokenizer.eos_token is not None else \"[PAD]\"\n",
        "\n",
        "# ✅ Load the dataset\n",
        "csv_path = '../QED_data/processed_dataset.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# ✅ Clean the DataFrame\n",
        "# Convert text and label columns to strings, handle missing values\n",
        "df['text'] = df['text'].astype(str).replace('nan', '')  # Convert to string, replace NaN with empty string\n",
        "df['label'] = df['label'].astype(str).replace('nan', '')\n",
        "\n",
        "# ✅ Split into train, validation, and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
        "\n",
        "# ✅ Tokenization Function\n",
        "MAX_LENGTH = 44\n",
        "\n",
        "def tokenize_function(example):\n",
        "    # Ensure inputs are strings and not empty\n",
        "    text = example['text'] if example['text'] else \"\"\n",
        "    label = example['label'] if example['label'] else \"\"\n",
        "    \n",
        "    # Tokenize text\n",
        "    input_tokens = tokenizer(\n",
        "        text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors=\"np\"\n",
        "    )\n",
        "\n",
        "    # Tokenize label\n",
        "    label_tokens = tokenizer(\n",
        "        label,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors=\"np\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_tokens['input_ids'][0],\n",
        "        'attention_mask': input_tokens['attention_mask'][0],\n",
        "        'labels': label_tokens['input_ids'][0]\n",
        "    }\n",
        "\n",
        "# ✅ Step 1: Tokenize Manually\n",
        "train_data = train_df.apply(tokenize_function, axis=1).tolist()\n",
        "val_data = val_df.apply(tokenize_function, axis=1).tolist()\n",
        "test_data = test_df.apply(tokenize_function, axis=1).tolist()\n",
        "\n",
        "# ✅ Step 2: Convert to Dictionary\n",
        "def convert_to_dict(data):\n",
        "    return {\n",
        "        'input_ids': np.stack([x['input_ids'] for x in data]),\n",
        "        'attention_mask': np.stack([x['attention_mask'] for x in data]),\n",
        "        'labels': np.stack([x['labels'] for x in data])\n",
        "    }\n",
        "\n",
        "train_dict = convert_to_dict(train_data)\n",
        "val_dict = convert_to_dict(val_data)\n",
        "test_dict = convert_to_dict(test_data)\n",
        "\n",
        "# ✅ Step 3: Create Dataset from Scratch\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_dict(train_dict),\n",
        "    \"validation\": Dataset.from_dict(val_dict),\n",
        "    \"test\": Dataset.from_dict(test_dict)\n",
        "})\n",
        "\n",
        "# ✅ Step 4: Set Format for PyTorch\n",
        "dataset.set_format(type='torch')\n",
        "\n",
        "# ✅ Sample Check\n",
        "print(dataset['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "PAyufV7O94fH"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_loader = DataLoader(dataset['train'], batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(dataset['validation'], batch_size=batch_size)\n",
        "test_loader = DataLoader(dataset['test'], batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving DataLoaders in the Python File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(r'../src/Dataloaders/train_loader.pkl', 'wb') as f:\n",
        "    pickle.dump(train_loader, f)\n",
        "\n",
        "with open(r'../src/Dataloaders/test_loader.pkl', 'wb') as f:\n",
        "    pickle.dump(test_loader, f)\n",
        "\n",
        "with open(r'../src/Dataloaders/val_loader.pkl', 'wb') as f:\n",
        "    pickle.dump(val_loader, f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'[SEP]'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode([1, 35632, 48766, 9874, 20758, 28768, 11745, 9693, 45096, 43001, 28449, 42744, 19394, 20729, 3068, 43965, 29878, 40972, 46786, 18185, 42672, 4199, 26545, 17362, 36242, 38463, 48635, 7562, 6366, 23576, 47614, 20166, 4413, 15086, 30211, 27408, 1640, 32132, 32393, 17635, 2346, 49387, 22450, 48115])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
