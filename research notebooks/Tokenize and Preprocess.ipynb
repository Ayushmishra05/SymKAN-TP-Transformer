{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenizing the Converted CSV Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessed data saved to preprocessed_2.txt\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the preprocessed CSV\n",
        "csv_path = 'processed_2.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Clean the DataFrame\n",
        "df['text'] = df['text'].astype(str).replace('nan', '')\n",
        "df['label'] = df['label'].astype(str).replace('nan', '')\n",
        "\n",
        "# Combine text and label into a list\n",
        "all_texts = df['text'].tolist() + df['label'].tolist()\n",
        "\n",
        "# Save to a text file\n",
        "output_txt_path = 'preprocessed_2.txt'\n",
        "with open(output_txt_path, 'w', encoding='utf-8') as f:\n",
        "    for text in all_texts:\n",
        "        if text:  # Skip empty strings\n",
        "            f.write(text + '\\n')\n",
        "\n",
        "print(f\"Preprocessed data saved to {output_txt_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "M35cdK1oqK88"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors\n",
        "\n",
        "tokenizer = Tokenizer(models.BPE())\n",
        "\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
        "\n",
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", 1),\n",
        "        (\"[SEP]\", 2),\n",
        "        (\"[PAD]\", 0),\n",
        "        (\"[MASK]\", 3), \n",
        "        (\"[STATE_ID]\" , 4)\n",
        "    ],\n",
        ")\n",
        "\n",
        "trainer = trainers.BpeTrainer(\n",
        "    vocab_size=512,\n",
        "    min_frequency=2,\n",
        "    special_tokens=[\"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\" , \"[STATE_ID]\"]\n",
        ")\n",
        "\n",
        "files = [\"preprocessed_2.txt\"]\n",
        "\n",
        "tokenizer.train(files, trainer)\n",
        "\n",
        "tokenizer.save(\"custom_tokenizer.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTOh9Jtfv1yW",
        "outputId": "404bef00-51f8-40f3-c565-d8dad93606eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('custom_tokenizer\\\\tokenizer_config.json',\n",
              " 'custom_tokenizer\\\\special_tokens_map.json',\n",
              " 'custom_tokenizer\\\\tokenizer.json')"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "hf_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"custom_tokenizer.json\")\n",
        "\n",
        "hf_tokenizer.cls_token = \"[CLS]\"\n",
        "hf_tokenizer.sep_token = \"[SEP]\"\n",
        "hf_tokenizer.pad_token = \"[PAD]\"\n",
        "hf_tokenizer.mask_token = \"[MASK]\"\n",
        "hf_tokenizer.mask_token = \"[STATE_ID]\"\n",
        "\n",
        "hf_tokenizer.save_pretrained(\"custom_tokenizer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['USE_TF'] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPod9ODb62pr",
        "outputId": "72f68a1b-bc53-4ad1-ddb3-a20291c8577b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizer'. \n",
            "The class this function is called from is 'RobertaTokenizerFast'.\n"
          ]
        }
      ],
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "\n",
        "tokenizer_path = 'custom_tokenizer'\n",
        "\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_path)\n",
        "\n",
        "sample = \"e_[ID](X)^(*) e_[ID](X)^(*) to e_[ID](X) e_[ID](X)\"\n",
        "tokens = tokenizer(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# ✅ Load the tokenizer\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"custom_tokenizer\")\n",
        "tokenizer.pad_token = tokenizer.eos_token if tokenizer.eos_token is not None else \"[PAD]\"\n",
        "\n",
        "# ✅ Load the dataset\n",
        "csv_path = 'processed_2.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# ✅ Clean the DataFrame\n",
        "# Convert text and label columns to strings, handle missing values\n",
        "df['text'] = df['text'].astype(str).replace('nan', '')  # Convert to string, replace NaN with empty string\n",
        "df['label'] = df['label'].astype(str).replace('nan', '')\n",
        "\n",
        "# ✅ Split into train, validation, and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go-_o4h27V_3",
        "outputId": "8b6902f4-6ba7-49fb-cf36-4c278b1683f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([  1,  22,  90,  93, 171,  35, 142,  35, 498, 138,   6,  30,   7,  94,\n",
            "        171,  35, 123,  35, 221, 440,   6,  30,   7, 157,  94, 171,  35, 142,\n",
            "         35, 445, 217,   6,  30, 135,  94, 171,  35,  95,  35, 452, 189,   6,\n",
            "         30, 135, 119,  35,  13,  94, 171,   6,  30,   7,  94, 171,   6,  30,\n",
            "          7,  77,   6,  30,   7, 119,  35,  12,  94, 171,   6,  30,   7,  94,\n",
            "        171,   6,  30,   7,  77,   6,  30,   7,  76,  13,  11,  21,   8,  43,\n",
            "          8,  40,  34,  14,   8, 106, 155, 121,  35, 211, 197,  21,  71,  74,\n",
            "         35,  17, 233, 186,  35,   4,  35,   4,  71,  74,  35,  17, 233, 200,\n",
            "         35,   4,  35,   4,  57,   8, 106, 124, 121,  35, 211, 197,  21,  71,\n",
            "         74,  35,  17, 233, 177,  35,   4,  35,   4,  71, 123,  35, 199, 411,\n",
            "         35,   4,  35,   4,  57,   8,  37,  72,  43,  35, 164, 146,  13,  71,\n",
            "         74,  35,  17, 233, 200,  35,   4,  35,   4, 125,  30,  84,  55,   8,\n",
            "         37,  72,  46,  35, 164,  18, 177,  71,  74,  35,  17, 233, 177,  35,\n",
            "          4,  35,   4, 125,  30,  84,  55, 139,  37,  72,  44,  35, 164, 146,\n",
            "         13,  71, 123,  35, 199, 411,  35,   4,  35,   4, 125,  30,  84,  55,\n",
            "          8,  37,  72,  45,  35, 164,  18, 177,  71,  74,  35,  17, 233, 186,\n",
            "         35,   4,  35,   4, 125,  30,  84,  55, 224,  30,  78,  52,  35,  87,\n",
            "         68, 133,  35, 103,  68, 167,  35, 102,   7,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0]), 'labels': tensor([  1,  20,  11, 143,   8,  40,  34,  16,  96,  47,  35,  37,  34,  16,\n",
            "         68,  76,  13,  11,  14,   8,  47,  35,  37,  34,  14,   8,  52,  35,\n",
            "         87,  68, 140,  11,  14,   8,  52,  35, 114,   8,  52,  35,  86,  68,\n",
            "         76,  13,  11,  14,   8,  47,  35,  37,  34,  14,   8,  52,  35, 117,\n",
            "         68, 140,  11,  14,   8,  52,  35,  97,   8,  52,  35, 116, 163,  52,\n",
            "         35, 111,  68, 132,  14,  78,  52,  35,  87,  68, 133,  35, 103,  68,\n",
            "        167,  35, 102, 151,  14,   7,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2])}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "MAX_LENGTH_TEXT = 509  # 99th percentile for text\n",
        "MAX_LENGTH_LABEL = 421 \n",
        "\n",
        "def tokenize_function(example):\n",
        "    # Ensure inputs are strings and not empty\n",
        "    text = example['text'] if example['text'] else \"\"\n",
        "    label = example['label'] if example['label'] else \"\"\n",
        "    \n",
        "    # Tokenize text\n",
        "    input_tokens = tokenizer(\n",
        "        text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH_TEXT,\n",
        "        return_tensors=\"np\"\n",
        "    )\n",
        "\n",
        "    # Tokenize label\n",
        "    label_tokens = tokenizer(\n",
        "        label,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH_LABEL,\n",
        "        return_tensors=\"np\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_tokens['input_ids'][0],\n",
        "        'attention_mask': input_tokens['attention_mask'][0],\n",
        "        'labels': label_tokens['input_ids'][0]\n",
        "    }\n",
        "\n",
        "# ✅ Step 1: Tokenize Manually\n",
        "train_data = train_df.apply(tokenize_function, axis=1).tolist()\n",
        "val_data = val_df.apply(tokenize_function, axis=1).tolist()\n",
        "test_data = test_df.apply(tokenize_function, axis=1).tolist()\n",
        "\n",
        "# ✅ Step 2: Convert to Dictionary\n",
        "def convert_to_dict(data):\n",
        "    return {\n",
        "        'input_ids': np.stack([x['input_ids'] for x in data]),\n",
        "        'attention_mask': np.stack([x['attention_mask'] for x in data]),\n",
        "        'labels': np.stack([x['labels'] for x in data])\n",
        "    }\n",
        "\n",
        "train_dict = convert_to_dict(train_data)\n",
        "val_dict = convert_to_dict(val_data)\n",
        "test_dict = convert_to_dict(test_data)\n",
        "\n",
        "# ✅ Step 3: Create Dataset from Scratch\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_dict(train_dict),\n",
        "    \"validation\": Dataset.from_dict(val_dict),\n",
        "    \"test\": Dataset.from_dict(test_dict)\n",
        "})\n",
        "\n",
        "# ✅ Step 4: Set Format for PyTorch\n",
        "dataset.set_format(type='torch')\n",
        "\n",
        "# ✅ Sample Check\n",
        "print(dataset['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequence length statistics for 'text' column:\n",
            "Max sequence length (text): 534\n",
            "90th percentile (text): 482.0\n",
            "95th percentile (text): 493.0\n",
            "99th percentile (text): 509.0\n",
            "\n",
            "Sequence length statistics for 'label' column:\n",
            "Max sequence length (label): 421\n",
            "90th percentile (label): 351.0\n",
            "95th percentile (label): 413.0\n",
            "99th percentile (label): 418.0\n"
          ]
        }
      ],
      "source": [
        "text_lengths = [len(tokenizer(text, add_special_tokens=True)['input_ids']) for text in df['text'].tolist()]\n",
        "label_lengths = [len(tokenizer(label, add_special_tokens=True)['input_ids']) for label in df['label'].tolist()]\n",
        "\n",
        "# Print statistics for text\n",
        "print(\"Sequence length statistics for 'text' column:\")\n",
        "print(f\"Max sequence length (text): {max(text_lengths)}\")\n",
        "print(f\"90th percentile (text): {np.percentile(text_lengths, 90)}\")\n",
        "print(f\"95th percentile (text): {np.percentile(text_lengths, 95)}\")\n",
        "print(f\"99th percentile (text): {np.percentile(text_lengths, 99)}\")\n",
        "\n",
        "# Print statistics for label\n",
        "print(\"\\nSequence length statistics for 'label' column:\")\n",
        "print(f\"Max sequence length (label): {max(label_lengths)}\")\n",
        "print(f\"90th percentile (label): {np.percentile(label_lengths, 90)}\")\n",
        "print(f\"95th percentile (label): {np.percentile(label_lengths, 95)}\")\n",
        "print(f\"99th percentile (label): {np.percentile(label_lengths, 99)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "PAyufV7O94fH"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_loader = DataLoader(dataset['train'], batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(dataset['validation'], batch_size=batch_size)\n",
        "test_loader = DataLoader(dataset['test'], batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving DataLoaders in the Python File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(r'../src/Dataloaders/train_loader.pkl', 'wb') as f:\n",
        "    pickle.dump(train_loader, f)\n",
        "\n",
        "with open(r'../src/Dataloaders/test_loader.pkl', 'wb') as f:\n",
        "    pickle.dump(test_loader, f)\n",
        "\n",
        "with open(r'../src/Dataloaders/val_loader.pkl', 'wb') as f:\n",
        "    pickle.dump(val_loader, f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'del 314'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode([123, 567, 812, 345])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
