{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ss3_6qAdqCCt",
        "outputId": "d2b01441-fa37-4aa5-8114-503a5112836f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tokenizers in d:\\decoderkan\\venv\\lib\\site-packages (0.21.1)\n",
            "Requirement already satisfied: transformers in d:\\decoderkan\\venv\\lib\\site-packages (4.49.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in d:\\decoderkan\\venv\\lib\\site-packages (from tokenizers) (0.29.3)\n",
            "Requirement already satisfied: filelock in d:\\decoderkan\\venv\\lib\\site-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in d:\\decoderkan\\venv\\lib\\site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\decoderkan\\venv\\lib\\site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in d:\\decoderkan\\venv\\lib\\site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in d:\\decoderkan\\venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in d:\\decoderkan\\venv\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in d:\\decoderkan\\venv\\lib\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in d:\\decoderkan\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in d:\\decoderkan\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\decoderkan\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: colorama in d:\\decoderkan\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\decoderkan\\venv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\decoderkan\\venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\decoderkan\\venv\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\decoderkan\\venv\\lib\\site-packages (from requests->transformers) (2025.1.31)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenizing the Converted CSV Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M35cdK1oqK88"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors\n",
        "\n",
        "tokenizer = Tokenizer(models.BPE())\n",
        "\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
        "\n",
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", 1),\n",
        "        (\"[SEP]\", 2),\n",
        "        (\"[PAD]\", 0),\n",
        "        (\"[MASK]\", 3)\n",
        "    ],\n",
        ")\n",
        "\n",
        "trainer = trainers.BpeTrainer(\n",
        "    vocab_size=5000,\n",
        "    min_frequency=2,\n",
        "    special_tokens=[\"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
        ")\n",
        "\n",
        "files = [\"../QED_data/QED_data.txt\"]\n",
        "\n",
        "tokenizer.train(files, trainer)\n",
        "\n",
        "tokenizer.save(\"custom_tokenizer.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTOh9Jtfv1yW",
        "outputId": "404bef00-51f8-40f3-c565-d8dad93606eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\DecoderKAN\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('custom_tokenizer\\\\tokenizer_config.json',\n",
              " 'custom_tokenizer\\\\special_tokens_map.json',\n",
              " 'custom_tokenizer\\\\tokenizer.json')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "hf_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"custom_tokenizer.json\")\n",
        "\n",
        "hf_tokenizer.cls_token = \"[CLS]\"\n",
        "hf_tokenizer.sep_token = \"[SEP]\"\n",
        "hf_tokenizer.pad_token = \"[PAD]\"\n",
        "hf_tokenizer.mask_token = \"[MASK]\"\n",
        "\n",
        "hf_tokenizer.save_pretrained(\"custom_tokenizer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjQ_-eeawOuS",
        "outputId": "d8273a4c-50e0-495b-d726-281f8d8077ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in d:\\decoderkan\\venv\\lib\\site-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in d:\\decoderkan\\venv\\lib\\site-packages (from datasets) (2.1.3)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-19.0.1-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in d:\\decoderkan\\venv\\lib\\site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in d:\\decoderkan\\venv\\lib\\site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in d:\\decoderkan\\venv\\lib\\site-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Downloading aiohttp-3.11.13-cp312-cp312-win_amd64.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in d:\\decoderkan\\venv\\lib\\site-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in d:\\decoderkan\\venv\\lib\\site-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in d:\\decoderkan\\venv\\lib\\site-packages (from datasets) (6.0.2)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Using cached frozenlist-1.5.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
            "  Using cached multidict-6.1.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
            "  Downloading propcache-0.3.0-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
            "  Using cached yarl-1.18.3-cp312-cp312-win_amd64.whl.metadata (71 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\decoderkan\\venv\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\decoderkan\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\decoderkan\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\decoderkan\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\decoderkan\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: colorama in d:\\decoderkan\\venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\decoderkan\\venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\decoderkan\\venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in d:\\decoderkan\\venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in d:\\decoderkan\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.4.0-py3-none-any.whl (487 kB)\n",
            "   ---------------------------------------- 0.0/487.4 kB ? eta -:--:--\n",
            "   --------------- ------------------------ 194.6/487.4 kB 4.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------  481.3/487.4 kB 6.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 487.4/487.4 kB 5.1 MB/s eta 0:00:00\n",
            "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "Downloading aiohttp-3.11.13-cp312-cp312-win_amd64.whl (438 kB)\n",
            "   ---------------------------------------- 0.0/438.1 kB ? eta -:--:--\n",
            "   --------------------------------------  430.1/438.1 kB 13.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 438.1/438.1 kB 9.1 MB/s eta 0:00:00\n",
            "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
            "Downloading pyarrow-19.0.1-cp312-cp312-win_amd64.whl (25.3 MB)\n",
            "   ---------------------------------------- 0.0/25.3 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.5/25.3 MB 17.2 MB/s eta 0:00:02\n",
            "   - -------------------------------------- 1.0/25.3 MB 12.5 MB/s eta 0:00:02\n",
            "   -- ------------------------------------- 1.4/25.3 MB 11.4 MB/s eta 0:00:03\n",
            "   --- ------------------------------------ 1.9/25.3 MB 11.0 MB/s eta 0:00:03\n",
            "   --- ------------------------------------ 2.4/25.3 MB 10.7 MB/s eta 0:00:03\n",
            "   ---- ----------------------------------- 2.8/25.3 MB 10.4 MB/s eta 0:00:03\n",
            "   ----- ---------------------------------- 3.2/25.3 MB 10.3 MB/s eta 0:00:03\n",
            "   ----- ---------------------------------- 3.7/25.3 MB 10.1 MB/s eta 0:00:03\n",
            "   ------ --------------------------------- 4.1/25.3 MB 10.0 MB/s eta 0:00:03\n",
            "   ------- -------------------------------- 4.5/25.3 MB 10.2 MB/s eta 0:00:03\n",
            "   ------- -------------------------------- 4.9/25.3 MB 9.8 MB/s eta 0:00:03\n",
            "   -------- ------------------------------- 5.3/25.3 MB 9.9 MB/s eta 0:00:03\n",
            "   -------- ------------------------------- 5.7/25.3 MB 9.8 MB/s eta 0:00:03\n",
            "   --------- ------------------------------ 6.1/25.3 MB 9.7 MB/s eta 0:00:02\n",
            "   ---------- ----------------------------- 6.6/25.3 MB 9.8 MB/s eta 0:00:02\n",
            "   ----------- ---------------------------- 7.1/25.3 MB 9.9 MB/s eta 0:00:02\n",
            "   ----------- ---------------------------- 7.5/25.3 MB 9.8 MB/s eta 0:00:02\n",
            "   ------------ --------------------------- 8.0/25.3 MB 9.8 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 8.4/25.3 MB 9.8 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 8.9/25.3 MB 9.9 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 9.4/25.3 MB 9.9 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 9.8/25.3 MB 9.8 MB/s eta 0:00:02\n",
            "   ---------------- ----------------------- 10.3/25.3 MB 9.8 MB/s eta 0:00:02\n",
            "   ---------------- ----------------------- 10.7/25.3 MB 9.8 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 11.1/25.3 MB 9.6 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 11.5/25.3 MB 9.6 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 11.9/25.3 MB 9.6 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 12.3/25.3 MB 9.6 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 12.8/25.3 MB 9.5 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 13.2/25.3 MB 9.5 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 13.6/25.3 MB 9.6 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 14.0/25.3 MB 9.5 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 14.4/25.3 MB 9.5 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 14.9/25.3 MB 9.6 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 15.3/25.3 MB 9.5 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 15.8/25.3 MB 9.8 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 16.2/25.3 MB 9.8 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 16.7/25.3 MB 9.8 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 17.2/25.3 MB 9.6 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 17.6/25.3 MB 9.6 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 18.0/25.3 MB 9.5 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 18.4/25.3 MB 9.6 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 18.4/25.3 MB 9.6 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 18.5/25.3 MB 9.0 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 18.5/25.3 MB 9.0 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 18.5/25.3 MB 9.0 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 18.5/25.3 MB 9.0 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 18.5/25.3 MB 9.0 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 18.5/25.3 MB 9.0 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 18.5/25.3 MB 9.0 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 18.5/25.3 MB 9.0 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 18.5/25.3 MB 9.0 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 18.5/25.3 MB 9.0 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 18.5/25.3 MB 9.0 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 18.5/25.3 MB 9.0 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 18.7/25.3 MB 6.1 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 19.2/25.3 MB 6.1 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 19.7/25.3 MB 6.1 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 20.1/25.3 MB 6.1 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 20.6/25.3 MB 6.1 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 21.0/25.3 MB 6.1 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 21.4/25.3 MB 6.1 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 21.9/25.3 MB 6.2 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 22.4/25.3 MB 6.2 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 22.7/25.3 MB 6.2 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 23.3/25.3 MB 6.2 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 23.7/25.3 MB 6.2 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 24.2/25.3 MB 6.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.6/25.3 MB 6.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  25.1/25.3 MB 6.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  25.2/25.3 MB 6.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  25.2/25.3 MB 6.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 25.3/25.3 MB 5.8 MB/s eta 0:00:00\n",
            "Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
            "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "   ---------------------------------------- 0.0/63.8 kB ? eta -:--:--\n",
            "   ---------------------------------------- 63.8/63.8 kB 3.3 MB/s eta 0:00:00\n",
            "Using cached frozenlist-1.5.0-cp312-cp312-win_amd64.whl (51 kB)\n",
            "Using cached multidict-6.1.0-cp312-cp312-win_amd64.whl (28 kB)\n",
            "Downloading propcache-0.3.0-cp312-cp312-win_amd64.whl (44 kB)\n",
            "   ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
            "   ---------------------------------------- 44.4/44.4 kB ? eta 0:00:00\n",
            "Using cached yarl-1.18.3-cp312-cp312-win_amd64.whl (90 kB)\n",
            "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, attrs, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.13 aiosignal-1.3.2 attrs-25.3.0 datasets-3.4.0 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.12.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.3.0 pyarrow-19.0.1 xxhash-3.5.0 yarl-1.18.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['USE_TF'] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPod9ODb62pr",
        "outputId": "72f68a1b-bc53-4ad1-ddb3-a20291c8577b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizer'. \n",
            "The class this function is called from is 'RobertaTokenizerFast'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 37, 32, 24, 5, 29, 138, 193, 32, 24, 5, 29, 138, 168, 193, 32, 24, 5, 29, 6, 193, 32, 24, 5, 29, 6, 2]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'[SEP] e _ I ( X )^(*) Ġe _ I ( X )^(*) Ġto Ġe _ I ( X ) Ġe _ I ( X ) [PAD]'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "\n",
        "tokenizer_path = 'custom_tokenizer'\n",
        "\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_path)\n",
        "\n",
        "sample = \"e_[ID](X)^(*) e_[ID](X)^(*) to e_[ID](X) e_[ID](X)\"\n",
        "tokens = tokenizer(sample)\n",
        "print(tokens['input_ids'])\n",
        "\n",
        "tokenizer.decode(tokens['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go-_o4h27V_3",
        "outputId": "8b6902f4-6ba7-49fb-cf36-4c278b1683f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([   1,  199,  205,   32,  147,   32, 2518,    5,   29,    6,  106,  205,\n",
            "          32,  123,   32,  232,  456,    5,   29,    6,  168,  106,  205,   32,\n",
            "         147,   32,  466,  228,    5,   29,  138,  106,  205,   32,   87,   32,\n",
            "         464,  187,    5,   29,  138,    2,    2,    2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]), 'labels': tensor([  1,  20,  11, 153,   7,  37,  31,  16, 103,  45,  32,  34,  31,  16,\n",
            "         60,  66,  13,  11,  14,   7,  45,  32,  34,  31,  14,   7,  50,  32,\n",
            "         86,  60, 139,  11,  14,   7,  50,  32, 114,   7,  50,  32,  85,  60,\n",
            "         66,   2])}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# ✅ Load the dataset\n",
        "csv_path = '../QED_data/processed_dataset.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# ✅ Split into train, validation, and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
        "\n",
        "# ✅ Tokenization Function\n",
        "MAX_LENGTH = 44\n",
        "\n",
        "def tokenize_function(example):\n",
        "    input_tokens = tokenizer(\n",
        "        example['text'],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors=\"np\"  # 🔥 Change to numpy → Avoid Arrow interference!\n",
        "    )\n",
        "\n",
        "    label_tokens = tokenizer(\n",
        "        example['label'],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors=\"np\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_tokens['input_ids'][0],\n",
        "        'attention_mask': input_tokens['attention_mask'][0],\n",
        "        'labels': label_tokens['input_ids'][0]\n",
        "    }\n",
        "\n",
        "# ✅ Step 1: Tokenize Manually\n",
        "train_data = train_df.apply(tokenize_function, axis=1).tolist()\n",
        "val_data = val_df.apply(tokenize_function, axis=1).tolist()\n",
        "test_data = test_df.apply(tokenize_function, axis=1).tolist()\n",
        "\n",
        "# ✅ Step 2: Convert to Dictionary\n",
        "def convert_to_dict(data):\n",
        "    return {\n",
        "        'input_ids': np.stack([x['input_ids'] for x in data]),\n",
        "        'attention_mask': np.stack([x['attention_mask'] for x in data]),\n",
        "        'labels': np.stack([x['labels'] for x in data])\n",
        "    }\n",
        "\n",
        "train_dict = convert_to_dict(train_data)\n",
        "val_dict = convert_to_dict(val_data)\n",
        "test_dict = convert_to_dict(test_data)\n",
        "\n",
        "# ✅ Step 3: Create Dataset from Scratch → New Schema!\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_dict(train_dict),\n",
        "    \"validation\": Dataset.from_dict(val_dict),\n",
        "    \"test\": Dataset.from_dict(test_dict)\n",
        "})\n",
        "\n",
        "# ✅ Step 4: Set Format for PyTorch\n",
        "dataset.set_format(type='torch')\n",
        "\n",
        "# ✅ Sample Check\n",
        "print(dataset['train'][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PAyufV7O94fH"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_loader = DataLoader(dataset['train'], batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(dataset['validation'], batch_size=batch_size)\n",
        "test_loader = DataLoader(dataset['test'], batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving DataLoaders in the Python File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(r'../src/Dataloaders/train_loader.pkl', 'wb') as f:\n",
        "    pickle.dump(train_loader, f)\n",
        "\n",
        "with open(r'../src/Dataloaders/test_loader.pkl', 'wb') as f:\n",
        "    pickle.dump(test_loader, f)\n",
        "\n",
        "with open(r'../src/Dataloaders/val_loader.pkl', 'wb') as f:\n",
        "    pickle.dump(val_loader, f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
