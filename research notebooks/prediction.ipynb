{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DecoderKAN\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "with open(r'../src/Dataloaders/train_loader.pkl', 'rb') as f:\n",
    "    train_loader = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in train_loader:\n",
    "    data = val['input_ids'] \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,  199,  204,   32,  123,   32, 2983,    5,   29,    6,  204,   32,\n",
       "          147,   32, 2814,    5,   29,  138,  168,  106,  202,   32,  147,   32,\n",
       "          105,  820,    5,   29,  138,  202,   32,  123,   32,  375,  191,    5,\n",
       "           29,    6,    2,    2,    2,    2,    2,    2],\n",
       "        [   1,  199,   81,  188,  118,   32, 2188,    5,   29,    6,   81,  188,\n",
       "          111,   32, 4083,    5,   29,    6,  168,  205,   32,   64,   32, 2239,\n",
       "          443,    5,   29,    6,  106,  205,   32,  123,   32,  575,  403,    5,\n",
       "           29,  138,    2,    2,    2,    2,    2,    2],\n",
       "        [   1,  199,  193,   32,   24,    5,   29,    6,  106,   81,  188,  157,\n",
       "           32,  214, 3412,    5,   29,    6,  168,  106,  193,   32,   24,    5,\n",
       "           29,  138,  106,   81,  188,  157,   32,  214, 1654,    5,   29,    6,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2],\n",
       "        [   1,   36,   32,  176,   32, 1280,    5,   29,  138,  106,  203,   32,\n",
       "          198,   32, 2013,    5,   29,    6,  168,   81,  188,  157,   32, 1129,\n",
       "          237,    5,   29,    6,   81,  188,  157,   32,  767,  160,    5,   29,\n",
       "            6,    2,    2,    2,    2,    2,    2,    2],\n",
       "        [   1,   34,   32,   64,   32,  313,    5,   29,  138,  106,  205,   32,\n",
       "          123,   32,  480,    5,   29,    6,  168,  106,  193,   32,   24,    5,\n",
       "           29,  138,  193,   32,   24,    5,   29,    6,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2],\n",
       "        [   1,  199,  200,   32,  123,   32, 2743,    5,   29,    6,  200,   32,\n",
       "          147,   32, 3544,    5,   29,  138,  168,  106,  201,   32,  123,   32,\n",
       "          330,  153,    5,   29,  138,  201,   32,  147,   32,  334,  191,    5,\n",
       "           29,    6,    2,    2,    2,    2,    2,    2],\n",
       "        [   1,  199,  200,   32,  147,   32, 1738,    5,   29,    6,  200,   32,\n",
       "           87,   32, 3200,    5,   29,  138,  168,   81,  188,  118,   32, 1165,\n",
       "           89,    5,   29,    6,  106,   81,  188,  118,   32,  208,  681,    5,\n",
       "           29,    6,    2,    2,    2,    2,    2,    2],\n",
       "        [   1,  199,  203,   32,  176,   32,  249,  406,    5,   29,    6,  134,\n",
       "           32,  198,   32,  190,  581,    5,   29,  138,  168,  106,  203,   32,\n",
       "           64,   32, 1315,  237,    5,   29,  138,  134,   32,  123,   32, 1440,\n",
       "          190,    5,   29,    6,    2,    2,    2,    2],\n",
       "        [   1,   52,   32,  176,   32, 3189,    5,   29,  138,  106,  202,   32,\n",
       "          198,   32, 3805,    5,   29,    6,  168,  134,   32,   64,   32,  407,\n",
       "          160,    5,   29,    6,  106,  134,   32,  123,   32,  107,  372,    5,\n",
       "           29,  138,    2,    2,    2,    2,    2,    2],\n",
       "        [   1,  199,  134,   32,  147,   32, 4745,    5,   29,    6,  204,   32,\n",
       "           87,   32,  137, 3060,    5,   29,  138,  168,  106,  134,   32,  198,\n",
       "           32, 1100,  221,    5,   29,  138,  204,   32,   64,   32, 1312,  272,\n",
       "            5,   29,    6,    2,    2,    2,    2,    2],\n",
       "        [   1,  199,  205,   32,  147,   32, 2518,    5,   29,    6,  205,   32,\n",
       "          198,   32, 3605,    5,   29,  138,  168,  205,   32,   64,   32,  206,\n",
       "          705,    5,   29,    6,  106,  205,   32,  123,   32, 1005,  274,    5,\n",
       "           29,  138,    2,    2,    2,    2,    2,    2],\n",
       "        [   1,  199,  200,   32,  147,   32, 2533,    5,   29,    6,  200,   32,\n",
       "          198,   32, 4124,    5,   29,  138,  168,  200,   32,   64,   32,  208,\n",
       "          108,    5,   29,    6,  106,  200,   32,   87,   32,  114,  418,    5,\n",
       "           29,  138,    2,    2,    2,    2,    2,    2],\n",
       "        [   1,   36,   32,  123,   32, 2549,    5,   29,  138,  106,  203,   32,\n",
       "          147,   32, 2812,    5,   29,    6,  168,  202,   32,   64,   32,  302,\n",
       "          150,    5,   29,    6,  106,  202,   32,  123,   32,  136,  374,    5,\n",
       "           29,  138,    2,    2,    2,    2,    2,    2],\n",
       "        [   1,  199,  134,   32,  147,   32, 2082,    5,   29,    6,  134,   32,\n",
       "          198,   32, 3070,    5,   29,  138,  168,   81,  188,  126,   32,  208,\n",
       "          425,    5,   29,    6,  106,   81,  188,  126,   32,  255,  416,    5,\n",
       "           29,    6,    2,    2,    2,    2,    2,    2],\n",
       "        [   1,  199,   81,  188,  157,   32, 2103,    5,   29,    6,   81,  188,\n",
       "          178,   32,  280,  105,    5,   29,    6,  168,  193,   32,   24,    5,\n",
       "           29,    6,  106,  193,   32,   24,    5,   29,  138,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2],\n",
       "        [   1,  199,  193,   32,   24,    5,   29,    6,  193,   32,   24,    5,\n",
       "           29,  138,  168,  106,  201,   32,  198,   32,  348,  236,    5,   29,\n",
       "          138,  201,   32,  176,   32,  388,  214,    5,   29,    6,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\DecoderKAN\\\\research notebooks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\DecoderKAN\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded with vocab size: 512\n",
      "Pad token: None, EOS token: None\n",
      "Model loaded on cpu from transformer_qed_sequence_test.pth\n",
      "Input text: AntiPart e_[ID](X) e_[ID](X)^(*) to s_eps_18941(X) AntiPart s_eta_22311(X)^(*)\n",
      "Predicted token IDs: [1, 332, 259, 309, 136, 269, 377, 126, 114, 126, 358, 187, 115, 130, 422, 115, 130, 422, 115, 130, 422, 115, 130, 422, 115, 130, 422, 115, 130, 422, 115, 130, 422, 115, 130, 281, 197, 497, 204, 98, 71, 424, 145, 253]\n",
      "Decoded output: [SEP] 129 96 216 19 *((- 158 igma Ġ2 igma 339 87 14 ert 176 14 ert 176 14 ert 176 14 ert 176 14 ert 176 14 ert 176 14 ert 176 14 ert 100 80 807 Ġd ĠV }( 307 ^(*)* Ġi\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from src.model import build_transformer  # Ensure this matches your model.py\n",
    "from transformers import PreTrainedTokenizerFast  # For loading the tokenizer\n",
    "\n",
    "# Set device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "VOCAB_SIZE = 512\n",
    "MAX_SEQ_LEN = 44\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"src/tokenizer/QED_TOKENIZER/\")\n",
    "print(f\"Tokenizer loaded with vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Ensure padding and truncation settings\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Often needed for consistency\n",
    "print(f\"Pad token: {tokenizer.pad_token}, EOS token: {tokenizer.eos_token}\")\n",
    "\n",
    "# Function to create causal mask (same as training)\n",
    "def create_causal_mask(size, device):\n",
    "    mask = torch.triu(torch.ones(size, size), diagonal=1).bool().to(device)\n",
    "    return ~mask\n",
    "\n",
    "# Load the saved model\n",
    "model = build_transformer(vocab_size=VOCAB_SIZE, d_model=512, num_heads=8)\n",
    "checkpoint = torch.load(\"transformer_qed_sequence_test.pth\", map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print(f\"Model loaded on {DEVICE} from transformer_qed_sequence_test.pth\")\n",
    "\n",
    "# Example input: Tokenize a physics interaction\n",
    "# Replace this string with your actual input (e.g., from your CSV)\n",
    "input_text = \"AntiPart e_[ID](X) e_[ID](X)^(*) to s_eps_18941(X) AntiPart s_eta_22311(X)^(*)\"\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "encoded = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", max_length=MAX_SEQ_LEN, truncation=True)\n",
    "src = encoded['input_ids'].to(DEVICE)  # [1, 44]\n",
    "src_mask = encoded['attention_mask'].unsqueeze(1).unsqueeze(2).expand(1, 1, MAX_SEQ_LEN, MAX_SEQ_LEN).to(DEVICE)\n",
    "\n",
    "# Start token for target sequence\n",
    "tgt_start = torch.tensor([[tokenizer.bos_token_id if tokenizer.bos_token_id is not None else 1]], dtype=torch.long).to(DEVICE)  # [1, 1]\n",
    "\n",
    "# Inference: Generate sequence autoregressively\n",
    "predicted_sequence = tgt_start\n",
    "with torch.no_grad():\n",
    "    for _ in range(MAX_SEQ_LEN - 1):  # Generate up to MAX_SEQ_LEN - 1 tokens\n",
    "        tgt_seq_len = predicted_sequence.size(1)\n",
    "        causal_mask = create_causal_mask(tgt_seq_len, DEVICE)\n",
    "        tgt_padding_mask = (predicted_sequence != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = causal_mask.unsqueeze(0).unsqueeze(0).expand(1, 1, tgt_seq_len, tgt_seq_len)\n",
    "        tgt_mask = tgt_mask & tgt_padding_mask.expand(1, 1, tgt_seq_len, tgt_seq_len)\n",
    "        cross_mask = (src != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2).expand(1, 1, tgt_seq_len, MAX_SEQ_LEN)\n",
    "\n",
    "        output = model(src, predicted_sequence, src_mask, tgt_mask, cross_mask)  # [1, tgt_seq_len, VOCAB_SIZE]\n",
    "        next_token_logits = output[:, -1, :]  # [1, VOCAB_SIZE]\n",
    "        next_token = next_token_logits.argmax(dim=-1, keepdim=True)  # [1, 1]\n",
    "        predicted_sequence = torch.cat([predicted_sequence, next_token], dim=1)  # [1, tgt_seq_len + 1]\n",
    "\n",
    "        # Stop if end token is predicted\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "# Decode the predicted sequence\n",
    "predicted_tokens = predicted_sequence.tolist()[0]  # List of token IDs, e.g., [1, 108, 11, ...]\n",
    "decoded_output = tokenizer.decode(predicted_tokens)\n",
    "print(f\"Input text: {input_text}\")\n",
    "print(f\"Predicted token IDs: {predicted_tokens}\")\n",
    "print(f\"Decoded output: {decoded_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,  199,  204,   32,  123,   32, 2983,    5,   29,    6,  204,   32,\n",
       "          147,   32, 2814,    5,   29,  138,  168,  106,  202,   32,  147,   32,\n",
       "          105,  820,    5,   29,  138,  202,   32,  123,   32,  375,  191,    5,\n",
       "           29,    6,    2,    2,    2,    2,    2,    2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
