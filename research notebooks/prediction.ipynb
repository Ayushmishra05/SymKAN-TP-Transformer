{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open(r'src/Dataloaders/train_loader.pkl', 'rb') as f:\n",
    "    train_loader = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in train_loader:\n",
    "    data = val['input_ids'] \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\DecoderKAN'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\DecoderKAN\\src\n"
     ]
    }
   ],
   "source": [
    "cd src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added d:\\DecoderKAN\\src to sys.path\n",
      "Using device: cpu\n",
      "Tokenizer vocab size: 512\n",
      "First few tokens: [('105', 286), ('299', 404), ('17', 138), ('166', 449), ('29', 152)]\n",
      "Does [PAD] exist? True\n",
      "Does [EOS] exist? False\n",
      "Does [BOS] exist? False\n",
      "Does [SEP] exist? True\n",
      "Pad token: [PAD] (ID: 2)\n",
      "EOS token: [SEP] (ID: 1)\n",
      "BOS token: [CLS] (ID: 0)\n",
      "Model device: cpu\n",
      "Input IDs device: cpu\n",
      "Input text: AntiPart e_alpha_[STATE_ID](X) e_del_[STATE_ID](X)^(*) to u_gam_[STATE_ID](X)\n",
      "Predicted token IDs: [0, 71, 49, 33, 17, 183, 13, 71, 49, 33, 16, 89, 54, 111, 46, 33, 38, 32, 15, 61, 114, 8, 51, 33, 90, 61, 135, 33, 108, 61, 155, 33, 95, 7, 110, 224, 12, 22, 8, 38, 32, 17, 104, 2, 2]\n",
      "Decoded output: }( p _ 4 67 0 }( p _ 3 )_ v /( m _ e ^ 2 Ġ+ Ġ2 * s _ 12 Ġ+ Ġs _ 22 Ġ+ Ġreg _ prop ) Ġ: Ġ4 / 9 * e ^ 4 *(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "import tptransformer\n",
    "importlib.reload(tptransformer)\n",
    "\n",
    "# Add the src directory to sys.path\n",
    "src_path = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "sys.path.append(src_path)\n",
    "print(f\"Added {src_path} to sys.path\")\n",
    "\n",
    "import torch\n",
    "from tptransformer import build_transformer  # Import from src.tptransformer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Hyperparameters (must match training)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "VOCAB_SIZE = 512\n",
    "MAX_SEQ_LEN = 44\n",
    "PAD_IDX = 0\n",
    "DROPOUT = 0.1\n",
    "FILTER_DIM = 2048\n",
    "N_LAYERS = 6\n",
    "N_HEADS = 8\n",
    "HIDDEN_DIM = 512\n",
    "\n",
    "# Define HyperParams class (same as training)\n",
    "class HyperParams:\n",
    "    def __init__(self):\n",
    "        self.input_dim = VOCAB_SIZE\n",
    "        self.filter = FILTER_DIM\n",
    "        self.n_layers = N_LAYERS\n",
    "        self.n_heads = N_HEADS\n",
    "        self.hidden = HIDDEN_DIM\n",
    "        self.dropout = DROPOUT\n",
    "\n",
    "params = HyperParams()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"../src/tokenizer/QED_TOKENIZER\")\n",
    "\n",
    "# Check vocab size\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "if len(tokenizer) != VOCAB_SIZE:\n",
    "    raise ValueError(f\"Tokenizer vocab size ({len(tokenizer)}) does not match VOCAB_SIZE ({VOCAB_SIZE}). Please ensure the tokenizer is reverted to 512 tokens.\")\n",
    "\n",
    "# Debug: Inspect the first few tokens in the vocab\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(f\"First few tokens: {list(vocab.items())[:5]}\")\n",
    "print(f\"Does [PAD] exist? {'[PAD]' in vocab}\")\n",
    "print(f\"Does [EOS] exist? {'[EOS]' in vocab}\")\n",
    "print(f\"Does [BOS] exist? {'[BOS]' in vocab}\")\n",
    "print(f\"Does [SEP] exist? {'[SEP]' in vocab}\")\n",
    "\n",
    "# Set special tokens without adding new ones\n",
    "# PAD token\n",
    "if tokenizer.pad_token is None:\n",
    "    if '[PAD]' in tokenizer.get_vocab():\n",
    "        tokenizer.pad_token = '[PAD]'\n",
    "    else:\n",
    "        tokenizer.pad_token = tokenizer.convert_ids_to_tokens(0)\n",
    "\n",
    "# EOS token\n",
    "if tokenizer.eos_token is None:\n",
    "    if '[EOS]' in tokenizer.get_vocab():\n",
    "        tokenizer.eos_token = '[EOS]'\n",
    "    elif '[SEP]' in tokenizer.get_vocab():\n",
    "        tokenizer.eos_token = '[SEP]'  # Use [SEP] as EOS (already set by default)\n",
    "    else:\n",
    "        tokenizer.eos_token = tokenizer.convert_ids_to_tokens(1)\n",
    "\n",
    "# BOS token\n",
    "if tokenizer.bos_token is None:\n",
    "    if '[BOS]' in tokenizer.get_vocab():\n",
    "        tokenizer.bos_token = '[BOS]'\n",
    "    elif '[CLS]' in tokenizer.get_vocab():\n",
    "        tokenizer.bos_token = '[CLS]'  # Use [CLS] as BOS if available\n",
    "    else:\n",
    "        # Use a different token ID (e.g., 0) that isn't PAD or EOS\n",
    "        candidate_id = 0\n",
    "        while candidate_id in [tokenizer.pad_token_id, tokenizer.eos_token_id]:\n",
    "            candidate_id += 1\n",
    "        tokenizer.bos_token = tokenizer.convert_ids_to_tokens(candidate_id)\n",
    "\n",
    "# Update token IDs\n",
    "PAD_IDX = tokenizer.pad_token_id\n",
    "EOS_IDX = tokenizer.eos_token_id\n",
    "SOS_IDX = tokenizer.bos_token_id\n",
    "\n",
    "print(f\"Pad token: {tokenizer.pad_token} (ID: {PAD_IDX})\")\n",
    "print(f\"EOS token: {tokenizer.eos_token} (ID: {EOS_IDX})\")\n",
    "print(f\"BOS token: {tokenizer.bos_token} (ID: {SOS_IDX})\")\n",
    "\n",
    "# Verify tokens are distinct\n",
    "if PAD_IDX == EOS_IDX or PAD_IDX == SOS_IDX or EOS_IDX == SOS_IDX:\n",
    "    raise ValueError(\"PAD, EOS, and BOS token IDs must be distinct!\")\n",
    "\n",
    "# Initialize model\n",
    "model = build_transformer(params, pad_idx=PAD_IDX)\n",
    "checkpoint = torch.load(\"../transformer_qed_sequence_full.pth\", map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Example input\n",
    "input_text = \"AntiPart e_alpha_[STATE_ID](X) e_del_[STATE_ID](X)^(*) to u_gam_[STATE_ID](X)\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", max_length=MAX_SEQ_LEN, truncation=True)['input_ids'].to(DEVICE)\n",
    "print(f\"Input IDs device: {input_ids.device}\")\n",
    "\n",
    "# Generate prediction\n",
    "output_ids = model.greedy_inference(model, input_ids, SOS_IDX, EOS_IDX, MAX_SEQ_LEN)\n",
    "decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input text: {input_text}\")\n",
    "print(f\"Predicted token IDs: {output_ids[0].tolist()}\")\n",
    "print(f\"Decoded output: {decoded_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
