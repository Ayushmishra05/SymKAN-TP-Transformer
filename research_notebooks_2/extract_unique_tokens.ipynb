{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in CSV: ['text', 'label']\n",
      "First few rows of CSV:\n",
      "                                                 text  \\\n",
      "0  e_gam_[STATE_ID](X)^(*) e_del_[STATE_ID](X)^(*...   \n",
      "1  e_gam_[STATE_ID](X)^(*) e_del_[STATE_ID](X)^(*...   \n",
      "2  e_gam_[STATE_ID](X)^(*) e_del_[STATE_ID](X)^(*...   \n",
      "3  e_gam_[STATE_ID](X)^(*) e_del_[STATE_ID](X)^(*...   \n",
      "4  e_gam_[STATE_ID](X)^(*) e_del_[STATE_ID](X)^(*...   \n",
      "\n",
      "                                               label  \n",
      "0  2*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23...  \n",
      "1  2*e^4*(m_e^4 + -1/2*m_e^2*s_14 + -1/2*m_e^2*s_...  \n",
      "2  2*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23...  \n",
      "3  2*e^4*(m_e^4 + -1/2*m_e^2*s_14 + -1/2*m_e^2*s_...  \n",
      "4  8*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23...  \n",
      "Number of rows after filtering NaN: 15552\n",
      "Preprocessed CSV saved to preprocessed_data.csv\n",
      "\n",
      "Number of unique tokens in the dataset: 139\n",
      "Sample tokens: ['s_34', 'u_eta_[STATE_ID]', 'c_beta_[STATE_ID]', 'b_alpha', 'mu_alpha_[STATE_ID]', 'sigma', '8', 'eps_[STATE_ID]', 'tt_eta', 't_eta', 'AntiPart', 'd_alpha', 'lambda', 's_gam', 't_del', 'l', 's_24', 'u_beta_[STATE_ID]', 'A_tau', 'd_']\n",
      "\n",
      "Most common tokens (top 20):\n",
      "*: 600380\n",
      "[STATE_ID]: 527366\n",
      "): 500040\n",
      "(: 489096\n",
      "+: 281402\n",
      "X: 276216\n",
      "^: 247371\n",
      "2: 231819\n",
      "-: 152129\n",
      "{: 141002\n",
      "}: 141002\n",
      "/: 112763\n",
      "1: 86884\n",
      "e: 77376\n",
      "i: 70812\n",
      "AntiPart: 62208\n",
      "8: 55607\n",
      "reg_prop: 51264\n",
      "gamma: 44234\n",
      "s_23: 43632\n",
      "Unique tokens saved to unique_tokens.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Load the CSV data\n",
    "csv_path = \"../QED_data/processed_2.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Print column names and first few rows for verification\n",
    "print(\"Columns in CSV:\", df.columns.tolist())\n",
    "print(\"First few rows of CSV:\\n\", df.head())\n",
    "\n",
    "# Filter out rows with missing data\n",
    "df = df.dropna(subset=['text', 'label'])\n",
    "print(f\"Number of rows after filtering NaN: {len(df)}\")\n",
    "\n",
    "# Convert all entries to strings\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['label'] = df['label'].astype(str)\n",
    "\n",
    "# Enhanced replace_state_ids function with improved [STATE_ID] handling\n",
    "def replace_state_ids(text):\n",
    "    # Step 1: Remove tokenizer artifacts\n",
    "    text = re.sub(r'%% ]}', '', text)\n",
    "    text = re.sub(r'%%+', '', text)\n",
    "    text = re.sub(r'Ġ', '', text)\n",
    "    text = re.sub(r'čĊ', '', text)\n",
    "    text = re.sub(r'igma', 'sigma', text)\n",
    "\n",
    "    # Step 2: Fix LaTeX issues\n",
    "    text = re.sub(r'\\\\ssigma', 'sigma', text)\n",
    "    text = re.sub(r'\\\\s', '', text)\n",
    "    text = re.sub(r'\\+%sigma', '+sigma', text)\n",
    "    text = re.sub(r'%sigma', 'sigma', text)\n",
    "\n",
    "    # Step 3: Replace 'del' with 'delta'\n",
    "    text = re.sub(r'\\bdel\\b', 'delta', text)\n",
    "\n",
    "    # Step 4: Simplify repeated [STATE_ID][STATE_ID] patterns (more robust)\n",
    "    while '[STATE_ID][STATE_ID]' in text:\n",
    "        text = re.sub(r'\\[STATE_ID\\]\\[STATE_ID\\]', '[STATE_ID]', text)\n",
    "    text = re.sub(r'\\[STATE_ID\\](?:\\[STATE_ID\\]_?)+', '[STATE_ID]', text)\n",
    "\n",
    "    # Step 5: Replace state IDs in formats like _120386_ (e.g., delta_120386_)\n",
    "    text = re.sub(r'(_\\d+_)', ' [STATE_ID]', text)\n",
    "\n",
    "    # Step 6: Replace state IDs in particle names\n",
    "    prefixes = r'(sigma|gam|delta|eta|nu|mu|eps|alpha|beta|tau|rho|lambda|t_eps|t_alpha|t_eta|t_gam|e_eps|e_eta|e_beta|s_eps|s_alpha|s_eta|s_gam|s_delta|s_beta|c_eps|c_eta|c_beta|c_gam|c_delta|e_gam|e_delta)'\n",
    "    text = re.sub(rf'{prefixes}_(\\d+)(?:_\\[STATE_ID\\](?:_\\[STATE_ID\\])?)?', r'\\1 [STATE_ID]', text)\n",
    "\n",
    "    # Step 7: Replace state IDs in gamma_{...}\n",
    "    text = re.sub(rf'\\\\({prefixes})_(\\d+)(?:_\\[STATE_ID\\](?:_\\[STATE_ID\\])?)?', r'\\\\\\1 [STATE_ID]', text)\n",
    "\n",
    "    # Step 8: Replace indices with state IDs (e.g., i_151807, k_151795, j_151787)\n",
    "    text = re.sub(r'(j|i|k|l)_(\\d+)', r'\\1 [STATE_ID]', text)\n",
    "\n",
    "    # Step 9: Simplify any remaining [STATE_ID]_[STATE_ID] patterns\n",
    "    text = re.sub(r'\\[STATE_ID\\]_\\[STATE_ID\\]', '[STATE_ID]', text)\n",
    "\n",
    "    # Step 10: Remove extra spaces and normalize\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the replacement to input and target columns\n",
    "df['text'] = df['text'].apply(replace_state_ids)\n",
    "df['label'] = df['label'].apply(replace_state_ids)\n",
    "\n",
    "# Save the preprocessed CSV\n",
    "preprocessed_csv_path = \"preprocessed_data.csv\"\n",
    "df.to_csv(preprocessed_csv_path, index=False)\n",
    "print(f\"Preprocessed CSV saved to {preprocessed_csv_path}\")\n",
    "\n",
    "# Define the corrected pre-tokenizer\n",
    "def hep_pre_tokenize(text):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        matched = False\n",
    "        # Skip the \"to\" keyword\n",
    "        if text[i:i+2].lower() == \"to\":\n",
    "            i += 2\n",
    "            continue\n",
    "\n",
    "        # Match special tokens like [STATE_ID]\n",
    "        if text[i] == '[':\n",
    "            end = text.find(']', i)\n",
    "            if end != -1:\n",
    "                token = text[i:end+1]\n",
    "                if token == '[STATE_ID]':\n",
    "                    tokens.append(token)\n",
    "                    i = end + 1\n",
    "                    matched = True\n",
    "\n",
    "        # Match LaTeX-like structures (e.g., gamma_{...}, A_{...}, e_{...}_u^(*))\n",
    "        if not matched and text[i:i+6].startswith('gamma_'):\n",
    "            start = i\n",
    "            i += 6\n",
    "            if i < len(text) and text[i] == '{':\n",
    "                i += 1\n",
    "                brace_count = 1\n",
    "                content = []\n",
    "                while i < len(text) and brace_count > 0:\n",
    "                    if text[i] == '{':\n",
    "                        brace_count += 1\n",
    "                    elif text[i] == '}':\n",
    "                        brace_count -= 1\n",
    "                    if brace_count > 0:\n",
    "                        content.append(text[i])\n",
    "                    i += 1\n",
    "                tokens.append('gamma')\n",
    "                tokens.append('{')\n",
    "                content_str = ''.join(content)\n",
    "                # Split content into parts, handling %\\\\mu [STATE_ID] patterns\n",
    "                content_parts = []\n",
    "                j = 0\n",
    "                while j < len(content_str):\n",
    "                    if content_str[j:j+3] == '%\\\\':\n",
    "                        end = j + 3\n",
    "                        while end < len(content_str) and content_str[end].isalpha():\n",
    "                            end += 1\n",
    "                        latex_index = content_str[j:end]\n",
    "                        content_parts.append(latex_index)\n",
    "                        j = end\n",
    "                    elif content_str[j] == '[':\n",
    "                        end = content_str.find(']', j)\n",
    "                        if end != -1:\n",
    "                            content_parts.append(content_str[j:end+1])\n",
    "                            j = end + 1\n",
    "                        else:\n",
    "                            j += 1\n",
    "                    elif content_str[j] == '+':\n",
    "                        content_parts.append('+')\n",
    "                        j += 1\n",
    "                    else:\n",
    "                        j += 1\n",
    "                for part in content_parts:\n",
    "                    tokens.append(part)\n",
    "                tokens.append('}')\n",
    "                matched = True\n",
    "\n",
    "        if not matched and text[i:i+2].startswith('A_'):\n",
    "            start = i\n",
    "            i += 2\n",
    "            if i < len(text) and text[i] == '\\\\':\n",
    "                i += 1\n",
    "                symbol_match = re.match(r'[a-zA-Z]+', text[i:])\n",
    "                if symbol_match:\n",
    "                    token = symbol_match.group(0)\n",
    "                    tokens.append(f'A_{token}')\n",
    "                    i += len(token)\n",
    "                    matched = True\n",
    "\n",
    "        if not matched and text[i:i+2].startswith('e_'):\n",
    "            start = i\n",
    "            i += 2\n",
    "            if i < len(text) and text[i] == '{':\n",
    "                i += 1\n",
    "                brace_count = 1\n",
    "                content = []\n",
    "                while i < len(text) and brace_count > 0:\n",
    "                    if text[i] == '{':\n",
    "                        brace_count += 1\n",
    "                    elif text[i] == '}':\n",
    "                        brace_count -= 1\n",
    "                    if brace_count > 0:\n",
    "                        content.append(text[i])\n",
    "                    i += 1\n",
    "                tokens.append('e')\n",
    "                tokens.append('{')\n",
    "                content_str = ''.join(content)\n",
    "                content_parts = content_str.split()\n",
    "                for part in content_parts:\n",
    "                    tokens.append(part)\n",
    "                tokens.append('}')\n",
    "                if i + 2 <= len(text) and text[i:i+2] == '_u':\n",
    "                    tokens.append('_u')\n",
    "                    i += 2\n",
    "                elif i + 2 <= len(text) and text[i:i+2] == '_v':\n",
    "                    tokens.append('_v')\n",
    "                    i += 2\n",
    "                if i + 4 <= len(text) and text[i:i+4] == '^(*)':\n",
    "                    tokens.append('^')\n",
    "                    tokens.append('(')\n",
    "                    tokens.append('*')\n",
    "                    tokens.append(')')\n",
    "                    i += 4\n",
    "                matched = True\n",
    "\n",
    "        # Match operators and parentheses\n",
    "        if not matched:\n",
    "            operators = r'(\\+|-|\\*|/|\\^|\\(|\\)|\\[|\\]|\\{|\\})'\n",
    "            match = re.match(operators, text[i:])\n",
    "            if match:\n",
    "                token = match.group(0)\n",
    "                tokens.append(token)\n",
    "                i += len(token)\n",
    "                matched = True\n",
    "\n",
    "        # Match symbols (e.g., e_gam, mu_eps, s_12, m_e)\n",
    "        if not matched:\n",
    "            symbol_match = re.match(r'[a-zA-Z_][a-zA-Z0-9_]*(?:\\[[a-zA-Z0-9_]+\\])?', text[i:])\n",
    "            if symbol_match:\n",
    "                token = symbol_match.group(0)\n",
    "                # Avoid partial matches like 'b_' when 'b_alpha' is the correct token\n",
    "                if token.endswith('_') and i + len(token) < len(text):\n",
    "                    next_part = re.match(r'[a-zA-Z0-9_]+', text[i + len(token):])\n",
    "                    if next_part:\n",
    "                        token += next_part.group(0)\n",
    "                tokens.append(token)\n",
    "                i += len(token)\n",
    "                matched = True\n",
    "\n",
    "        # Match digits as individual tokens\n",
    "        if not matched and text[i].isdigit():\n",
    "            num_start = i\n",
    "            while i < len(text) and text[i].isdigit():\n",
    "                i += 1\n",
    "            num = text[num_start:i]\n",
    "            tokens.extend(list(num))\n",
    "            matched = True\n",
    "\n",
    "        # Match AntiPart as a separate token\n",
    "        if not matched and text[i:i+8].lower() == 'antipart':\n",
    "            tokens.append('AntiPart')\n",
    "            i += 8\n",
    "            matched = True\n",
    "\n",
    "        # If no match, move to the next character\n",
    "        if not matched:\n",
    "            i += 1\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Collect all tokens from the dataset\n",
    "all_texts = df['text'].tolist() + df['label'].tolist()\n",
    "all_tokens = set()\n",
    "token_counter = Counter()\n",
    "\n",
    "for text in all_texts:\n",
    "    if text:\n",
    "        tokens = hep_pre_tokenize(text)\n",
    "        all_tokens.update(tokens)\n",
    "        token_counter.update(tokens)\n",
    "\n",
    "# Print the number of unique tokens and some examples\n",
    "print(f\"\\nNumber of unique tokens in the dataset: {len(all_tokens)}\")\n",
    "print(f\"Sample tokens: {list(all_tokens)[:20]}\")\n",
    "print(\"\\nMost common tokens (top 20):\")\n",
    "for token, count in token_counter.most_common(20):\n",
    "    print(f\"{token}: {count}\")\n",
    "\n",
    "# Save the unique tokens for reference\n",
    "with open(\"unique_tokens.txt\", \"w\") as f:\n",
    "    for token in sorted(all_tokens):\n",
    "        f.write(f\"{token}\\n\")\n",
    "print(\"Unique tokens saved to unique_tokens.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
