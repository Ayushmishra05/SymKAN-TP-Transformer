{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in CSV: ['text', 'label']\n",
      "First few rows of CSV:\n",
      "                                                 text  \\\n",
      "0  e_gam_[STATE_ID](X)^(*) e_del_[STATE_ID](X)^(*...   \n",
      "1  e_gam_[STATE_ID](X)^(*) e_del_[STATE_ID](X)^(*...   \n",
      "2  e_gam_[STATE_ID](X)^(*) e_del_[STATE_ID](X)^(*...   \n",
      "3  e_gam_[STATE_ID](X)^(*) e_del_[STATE_ID](X)^(*...   \n",
      "4  e_gam_[STATE_ID](X)^(*) e_del_[STATE_ID](X)^(*...   \n",
      "\n",
      "                                               label  \n",
      "0  2*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23...  \n",
      "1  2*e^4*(m_e^4 + -1/2*m_e^2*s_14 + -1/2*m_e^2*s_...  \n",
      "2  2*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23...  \n",
      "3  2*e^4*(m_e^4 + -1/2*m_e^2*s_14 + -1/2*m_e^2*s_...  \n",
      "4  8*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23...  \n",
      "Number of rows after filtering NaN: 15552\n",
      "Preprocessed CSV saved to preprocessed_data.csv\n",
      "Number of unique tokens in the preprocessed dataset: 70\n",
      "Sample tokens: ['u', '+', 'gam', 'i', 'eta', '/', 'alpha', ')', 'e_del', '_eta', 'X', '5', '6', 'm_u', 's_33', '(', 'm_t', '8', '_u', 'e']\n",
      "\n",
      "Preprocessed first non-empty row:\n",
      "Input: e_gam_[STATE_ID](X)^(*) e_del_[STATE_ID](X)^(*) to e_eps_[STATE_ID](X) e_eta_[STATE_ID](X) V_1 e(X) e(X) A(X) V_0 e(X) e(X) A(X) -1/2*i*e^2*gamma_{+sigma [STATE_ID] gam [STATE_ID][STATE_ID] gam [STATE_ID][STATE_ID]}*gamma_{sigma [STATE_ID] gam [STATE_ID][STATE_ID] del [STATE_ID][STATE_ID]}*e_{i [STATE_ID] gam [STATE_ID][STATE_ID]}(X)_u*e_{k [STATE_ID] del [STATE_ID][STATE_ID]}(X)_u*e_{l [STATE_ID] gam [STATE_ID][STATE_ID]}(X)_u^(*)*e_{i [STATE_ID] gam [STATE_ID][STATE_ID]}(X)_u^(*)/(X)\n",
      "Target: 2*e^4*(m_e^4 + -1/2*m_e^2*s_13 + 1/2*s_14*s_23 + -1/2*m_e^2*s_24 + 1/2*s_12*s_34)*(m_e^2 + -s_13 + 1/2*reg_prop)^(-2)\n",
      "\n",
      "Tokenized Input: ['e_gam', '[STATE_ID]', '(', 'X', ')', '^', '(', '*', ')', 'e_del', '[STATE_ID]', '(', 'X', ')', '^', '(', '*', ')', 'e_eps', '[STATE_ID]', '(', 'X', ')', 'e_eta', '[STATE_ID]', '(', 'X', ')', 'V_1', 'e', '(', 'X', ')', 'e', '(', 'X', ')', 'A', '(', 'X', ')', 'V_0', 'e', '(', 'X', ')', 'e', '(', 'X', ')', 'A', '(', 'X', ')', '-', '1', '/', '2', '*', 'i', '*', 'e', '^', '2', '*', 'gamma', '{', '+', 'sigma', '[STATE_ID]', 'gam', '[STATE_ID]', '[STATE_ID]', 'gam', '[STATE_ID]', '[STATE_ID]', '}', '*', 'gamma', '{', 'sigma', '[STATE_ID]', 'gam', '[STATE_ID]', '[STATE_ID]', 'e', 'l', '[STATE_ID]', '[STATE_ID]', '}', '*', 'e', '{', 'i', '[STATE_ID]', 'gam', '[STATE_ID]', '[STATE_ID]', '}', '(', 'X', ')', '_u', '*', 'e', '{', 'k', '[STATE_ID]', 'e', 'l', '[STATE_ID]', '[STATE_ID]', '}', '(', 'X', ')', '_u', '*', 'e', '{', 'l', '[STATE_ID]', 'gam', '[STATE_ID]', '[STATE_ID]', '}', '(', 'X', ')', '_u', '^', '(', '*', ')', '*', 'e', '{', 'i', '[STATE_ID]', 'gam', '[STATE_ID]', '[STATE_ID]', '}', '(', 'X', ')', '_u', '^', '(', '*', ')', '/', '(', 'X', ')']\n",
      "Tokenized Target: ['2', '*', 'e', '^', '4', '*', '(', 'm_e', '^', '4', '+', '-', '1', '/', '2', '*', 'm_e', '^', '2', '*', 's_13', '+', '1', '/', '2', '*', 's_14', '*', 's_23', '+', '-', '1', '/', '2', '*', 'm_e', '^', '2', '*', 's_24', '+', '1', '/', '2', '*', 's_12', '*', 's_34', ')', '*', '(', 'm_e', '^', '2', '+', '-', 's_13', '+', '1', '/', '2', '*', 'reg_prop', ')', '^', '(', '-', '2', ')']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the CSV data\n",
    "csv_path = \"../QED_data/processed_2.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Print column names and first few rows for verification\n",
    "print(\"Columns in CSV:\", df.columns.tolist())\n",
    "print(\"First few rows of CSV:\\n\", df.head())\n",
    "\n",
    "# Filter out rows with missing data\n",
    "df = df.dropna(subset=['text', 'label'])\n",
    "print(f\"Number of rows after filtering NaN: {len(df)}\")\n",
    "\n",
    "# Convert all entries to strings\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['label'] = df['label'].astype(str)\n",
    "\n",
    "# Enhanced replace_state_ids function\n",
    "def replace_state_ids(text):\n",
    "    # Step 1: Remove tokenizer artifacts\n",
    "    text = re.sub(r'%% ]}', '', text)\n",
    "    text = re.sub(r'%%+', '', text)\n",
    "    text = re.sub(r'Ġ', '', text)\n",
    "    text = re.sub(r'čĊ', '', text)\n",
    "    text = re.sub(r'igma', 'sigma', text)\n",
    "\n",
    "    # Step 2: Fix LaTeX issues\n",
    "    text = re.sub(r'\\\\ssigma', 'sigma', text)\n",
    "    text = re.sub(r'\\\\s', '', text)\n",
    "    text = re.sub(r'\\+%sigma', '+sigma', text)\n",
    "    text = re.sub(r'%sigma', 'sigma', text)\n",
    "\n",
    "    # Step 3: Simplify repeated [STATE_ID][STATE_ID]_[STATE_ID] patterns\n",
    "    text = re.sub(r'\\[STATE_ID\\](?:\\[STATE_ID\\]_?)+', '[STATE_ID]', text)\n",
    "\n",
    "    # Step 4: Replace state IDs in formats like _120386_ (e.g., del_120386_)\n",
    "    text = re.sub(r'(_\\d+_)', ' [STATE_ID]', text)\n",
    "\n",
    "    # Step 5: Replace state IDs in particle names\n",
    "    prefixes = r'(sigma|gam|del|eta|nu|mu|eps|alpha|beta|tau|rho|lambda|t_eps|t_alpha|t_eta|t_gam|e_eps|e_eta|e_beta|s_eps|s_alpha|s_eta|s_gam|s_del|s_beta|c_eps|c_eta|c_beta|c_gam|c_del|e_gam|e_del)'\n",
    "    text = re.sub(rf'{prefixes}_(\\d+)(?:_\\[STATE_ID\\](?:_\\[STATE_ID\\])?)?', r'\\1 [STATE_ID]', text)\n",
    "\n",
    "    # Step 6: Replace state IDs in gamma_{...}\n",
    "    text = re.sub(rf'\\\\({prefixes})_(\\d+)(?:_\\[STATE_ID\\](?:_\\[STATE_ID\\])?)?', r'\\\\\\1 [STATE_ID]', text)\n",
    "\n",
    "    # Step 7: Replace indices with state IDs (e.g., i_151807, k_151795, j_151787)\n",
    "    text = re.sub(r'(j|i|k|l)_(\\d+)', r'\\1 [STATE_ID]', text)\n",
    "\n",
    "    # Step 8: Simplify any remaining [STATE_ID]_[STATE_ID] patterns\n",
    "    text = re.sub(r'\\[STATE_ID\\]_\\[STATE_ID\\]', '[STATE_ID]', text)\n",
    "\n",
    "    # Step 9: Remove extra spaces and normalize\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the replacement to input and target columns\n",
    "df['text'] = df['text'].apply(replace_state_ids)\n",
    "df['label'] = df['label'].apply(replace_state_ids)\n",
    "\n",
    "# Save the preprocessed CSV\n",
    "preprocessed_csv_path = \"preprocessed_data.csv\"\n",
    "df.to_csv(preprocessed_csv_path, index=False)\n",
    "print(f\"Preprocessed CSV saved to {preprocessed_csv_path}\")\n",
    "\n",
    "# Define the vocabulary\n",
    "# Comprehensive QED-specific vocabulary\n",
    "vocab = {\n",
    "    # Special tokens\n",
    "    \"[CLS]\": 0,\n",
    "    \"[SEP]\": 1,\n",
    "    \"[PAD]\": 2,\n",
    "    \"[MASK]\": 3,\n",
    "    \"[STATE_ID]\": 4,\n",
    "    \"[UNK]\": 5,\n",
    "\n",
    "    # Mathematical operators\n",
    "    \"+\": 6,\n",
    "    \"-\": 7,\n",
    "    \"*\": 8,\n",
    "    \"/\": 9,\n",
    "    \"^\": 10,\n",
    "    \"(\": 11,\n",
    "    \")\": 12,\n",
    "    \"[\": 13,\n",
    "    \"]\": 14,\n",
    "    \"{\": 15,\n",
    "    \"}\": 16,\n",
    "\n",
    "    # Numbers (for coefficients and indices)\n",
    "    \"0\": 17,\n",
    "    \"1\": 18,\n",
    "    \"2\": 19,\n",
    "    \"3\": 20,\n",
    "    \"4\": 21,\n",
    "    \"5\": 22,\n",
    "    \"6\": 23,\n",
    "    \"7\": 24,\n",
    "    \"8\": 25,\n",
    "    \"9\": 26,\n",
    "\n",
    "    # Common variables and constants\n",
    "    \"e\": 27,        # Electric charge or electron\n",
    "    \"alpha\": 28,    # Fine-structure constant\n",
    "    \"hbar\": 29,     # Reduced Planck constant\n",
    "    \"c\": 30,        # Speed of light\n",
    "    \"G\": 31,        # Gravitational constant (if needed)\n",
    "    \"pi\": 32,       # Pi constant\n",
    "    \"i\": 33,        # Imaginary unit\n",
    "\n",
    "    # Particle masses\n",
    "    \"m_e\": 34,      # Electron mass\n",
    "    \"m_mu\": 35,     # Muon mass\n",
    "    \"m_tau\": 36,    # Tau mass\n",
    "    \"m_u\": 37,      # Up quark mass\n",
    "    \"m_d\": 38,      # Down quark mass\n",
    "    \"m_c\": 39,      # Charm quark mass\n",
    "    \"m_s\": 40,      # Strange quark mass\n",
    "    \"m_t\": 41,      # Top quark mass\n",
    "    \"m_b\": 42,      # Bottom quark mass\n",
    "\n",
    "    # Mandelstam variables\n",
    "    \"s\": 43,\n",
    "    \"t\": 44,\n",
    "    \"u\": 45,\n",
    "    \"s_11\": 46,\n",
    "    \"s_12\": 47,\n",
    "    \"s_13\": 48,\n",
    "    \"s_14\": 49,\n",
    "    \"s_21\": 50,\n",
    "    \"s_22\": 51,\n",
    "    \"s_23\": 52,\n",
    "    \"s_24\": 53,\n",
    "    \"s_31\": 54,\n",
    "    \"s_32\": 55,\n",
    "    \"s_33\": 56,\n",
    "    \"s_34\": 57,\n",
    "    \"s_41\": 58,\n",
    "    \"s_42\": 59,\n",
    "    \"s_43\": 60,\n",
    "    \"s_44\": 61,\n",
    "\n",
    "    # Momenta and coordinates\n",
    "    \"p_1\": 62,\n",
    "    \"p_2\": 63,\n",
    "    \"p_3\": 64,\n",
    "    \"p_4\": 65,\n",
    "    \"k_1\": 66,\n",
    "    \"k_2\": 67,\n",
    "    \"q\": 68,\n",
    "    \"X\": 69,        # Generic variable (e.g., in e(X))\n",
    "\n",
    "    # QED-specific terms\n",
    "    \"reg_prop\": 70, # Regularized propagator\n",
    "    \"Delta\": 71,    # Delta function or difference\n",
    "    \"A\": 72,        # Photon field (e.g., A(X))\n",
    "    \"V_0\": 73,      # Potential term\n",
    "    \"V_1\": 74,      # Potential term\n",
    "\n",
    "    # Greek letters (common in QED)\n",
    "    \"gamma\": 75,    # Dirac matrices\n",
    "    \"sigma\": 76,    # Pauli matrices or other\n",
    "    \"epsilon\": 77,  # Levi-Civita tensor or small parameter\n",
    "    \"delta\": 78,    # Kronecker delta\n",
    "    \"theta\": 79,    # Angle or parameter\n",
    "    \"phi\": 80,      # Scalar field or angle\n",
    "    \"omega\": 81,    # Frequency or field\n",
    "    \"eta\": 82,      # Pseudoscalar meson or parameter\n",
    "    \"mu\": 83,       # Muon or Lorentz index\n",
    "    \"nu\": 84,       # Neutrino or Lorentz index\n",
    "    \"rho\": 85,      # Rho meson or density\n",
    "    \"tau\": 86,      # Tau lepton or parameter\n",
    "    \"lambda\": 87,   # Parameter or field\n",
    "    \"beta\": 88,     # Parameter or velocity\n",
    "    \"alpha\": 89,    # Fine-structure constant (already included, but repeated for clarity)\n",
    "    \"gam\": 90,      # Short form for gamma in subscripts\n",
    "\n",
    "    # Particles and fields\n",
    "    \"e_gam\": 91,    # Electron-gamma interaction\n",
    "    \"e_del\": 92,    # Electron-delta interaction\n",
    "    \"e_eps\": 93,    # Electron-epsilon interaction\n",
    "    \"e_eta\": 94,    # Electron-eta interaction\n",
    "    \"e_mu\": 95,     # Electron-muon interaction\n",
    "    \"e_nu\": 96,     # Electron-neutrino interaction\n",
    "    \"photon\": 97,   # Photon\n",
    "    \"Z\": 98,        # Z boson\n",
    "    \"W\": 99,        # W boson\n",
    "\n",
    "    # Subscripts and superscripts\n",
    "    \"_u\": 100,      # Up-type subscript\n",
    "    \"_d\": 101,      # Down-type subscript\n",
    "    \"_mu\": 102,     # Muon subscript\n",
    "    \"_nu\": 103,     # Neutrino subscript\n",
    "    \"_e\": 104,      # Electron subscript\n",
    "    \"_gam\": 105,    # Gamma subscript\n",
    "    \"_del\": 106,    # Delta subscript\n",
    "    \"_eps\": 107,    # Epsilon subscript\n",
    "    \"_eta\": 108,    # Eta subscript\n",
    "\n",
    "    # Mathematical functions\n",
    "    \"sin\": 109,\n",
    "    \"cos\": 110,\n",
    "    \"tan\": 111,\n",
    "    \"exp\": 112,\n",
    "    \"log\": 113,\n",
    "\n",
    "    # Indices (for Dirac matrices, tensors, etc.)\n",
    "    \"i\": 114,       # Index (e.g., in e_{i})\n",
    "    \"j\": 115,\n",
    "    \"k\": 116,\n",
    "    \"l\": 117,\n",
    "}\n",
    "\n",
    "\n",
    "# Improved pre-tokenizer\n",
    "def hep_pre_tokenize(text, vocab):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        matched = False\n",
    "        # Skip the \"to\" keyword\n",
    "        if text[i:i+2] == \"to\":\n",
    "            i += 2\n",
    "            continue\n",
    "        # Try to match the longest possible symbol from the vocabulary\n",
    "        for j in range(len(text), i, -1):\n",
    "            substring = text[i:j]\n",
    "            if substring in vocab:\n",
    "                tokens.append(substring)\n",
    "                i = j\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            # Try to match LaTeX-like structures (e.g., gamma_{...}, e_{...}_u^(*))\n",
    "            if text[i:i+6].startswith('gamma_'):\n",
    "                start = i\n",
    "                i += 6\n",
    "                if i < len(text) and text[i] == '{':\n",
    "                    i += 1\n",
    "                    brace_count = 1\n",
    "                    content = []\n",
    "                    while i < len(text) and brace_count > 0:\n",
    "                        if text[i] == '{':\n",
    "                            brace_count += 1\n",
    "                        elif text[i] == '}':\n",
    "                            brace_count -= 1\n",
    "                        if brace_count > 0:\n",
    "                            content.append(text[i])\n",
    "                        i += 1\n",
    "                    tokens.append('gamma')\n",
    "                    tokens.append('{')\n",
    "                    content_parts = ''.join(content).split()\n",
    "                    for part in content_parts:\n",
    "                        if part in vocab:\n",
    "                            tokens.append(part)\n",
    "                        elif part.startswith('+'):\n",
    "                            tokens.append('+')\n",
    "                            sub_part = part[1:]\n",
    "                            if sub_part in vocab:\n",
    "                                tokens.append(sub_part)\n",
    "                    tokens.append('}')\n",
    "                    matched = True\n",
    "            elif text[i:i+2].startswith('e_'):\n",
    "                start = i\n",
    "                i += 2\n",
    "                if i < len(text) and text[i] == '{':\n",
    "                    i += 1\n",
    "                    brace_count = 1\n",
    "                    content = []\n",
    "                    while i < len(text) and brace_count > 0:\n",
    "                        if text[i] == '{':\n",
    "                            brace_count += 1\n",
    "                        elif text[i] == '}':\n",
    "                            brace_count -= 1\n",
    "                        if brace_count > 0:\n",
    "                            content.append(text[i])\n",
    "                        i += 1\n",
    "                    tokens.append('e')\n",
    "                    tokens.append('{')\n",
    "                    content_parts = ''.join(content).split()\n",
    "                    for part in content_parts:\n",
    "                        if part in vocab:\n",
    "                            tokens.append(part)\n",
    "                    tokens.append('}')\n",
    "                    if i + 2 <= len(text) and text[i:i+2] == '_u':\n",
    "                        tokens.append('_u')\n",
    "                        i += 2\n",
    "                    if i + 4 <= len(text) and text[i:i+4] == '^(*)':\n",
    "                        tokens.append('^')\n",
    "                        tokens.append('(')\n",
    "                        tokens.append('*')\n",
    "                        tokens.append(')')\n",
    "                        i += 4\n",
    "                    matched = True\n",
    "            if not matched:\n",
    "                operators = r'(\\+|-|\\*|/|\\^|\\(|\\)|\\[|\\]|\\{|\\})'\n",
    "                match = re.match(operators, text[i:])\n",
    "                if match:\n",
    "                    token = match.group(0)\n",
    "                    tokens.append(token)\n",
    "                    i += len(token)\n",
    "                    matched = True\n",
    "                else:\n",
    "                    if text[i].isdigit():\n",
    "                        num_start = i\n",
    "                        while i < len(text) and text[i].isdigit():\n",
    "                            i += 1\n",
    "                        num = text[num_start:i]\n",
    "                        tokens.extend(list(num))\n",
    "                        matched = True\n",
    "                    else:\n",
    "                        i += 1\n",
    "    return tokens\n",
    "\n",
    "# Updated tokenizer class\n",
    "class QEDTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.vocab_reverse = {v: k for k, v in vocab.items()}\n",
    "        self.pad_token = \"[PAD]\"\n",
    "        self.pad_token_id = vocab[\"[PAD]\"]\n",
    "\n",
    "    def pre_tokenize(self, text):\n",
    "        return hep_pre_tokenize(text, self.vocab)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = self.pre_tokenize(text)\n",
    "        final_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in self.vocab:\n",
    "                final_tokens.append(token)\n",
    "            elif token.isdigit():\n",
    "                final_tokens.extend(list(token))\n",
    "            else:\n",
    "                continue\n",
    "        return final_tokens\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        token_ids = [self.vocab.get(token, self.vocab[\"[PAD]\"]) for token in tokens]\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids, skip_special_tokens=True):\n",
    "        tokens = []\n",
    "        for tid in token_ids:\n",
    "            token = self.vocab_reverse.get(tid, \"\")\n",
    "            if skip_special_tokens and token in [\"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"[STATE_ID]\"]:\n",
    "                continue\n",
    "            tokens.append(token)\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = QEDTokenizer(vocab)\n",
    "\n",
    "# Count unique tokens in the preprocessed data\n",
    "input_texts = df['text'].tolist()\n",
    "target_texts = df['label'].tolist()\n",
    "all_texts = input_texts + target_texts\n",
    "\n",
    "all_tokens = set()\n",
    "for text in all_texts:\n",
    "    if text:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        all_tokens.update(tokens)\n",
    "\n",
    "print(f\"Number of unique tokens in the preprocessed dataset: {len(all_tokens)}\")\n",
    "print(f\"Sample tokens: {list(all_tokens)[:20]}\")\n",
    "\n",
    "# Print the preprocessed version of the first non-empty row for verification\n",
    "first_non_empty_row = df.iloc[0]\n",
    "print(\"\\nPreprocessed first non-empty row:\")\n",
    "print(\"Input:\", first_non_empty_row['text'])\n",
    "print(\"Target:\", first_non_empty_row['label'])\n",
    "\n",
    "# Test tokenization on the first row\n",
    "input_tokens = tokenizer.tokenize(first_non_empty_row['text'])\n",
    "target_tokens = tokenizer.tokenize(first_non_empty_row['label'])\n",
    "print(\"\\nTokenized Input:\", input_tokens)\n",
    "print(\"Tokenized Target:\", target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 77\n",
      "Sample vocabulary entries:\n",
      "[CLS]: 0\n",
      "[SEP]: 1\n",
      "[PAD]: 2\n",
      "[MASK]: 3\n",
      "[STATE_ID]: 4\n",
      "[UNK]: 5\n",
      "0: 6\n",
      "1: 7\n",
      "2: 8\n",
      "3: 9\n",
      "...\n",
      "s_24: 67\n",
      "s_33: 68\n",
      "s_34: 69\n",
      "s_44: 70\n",
      "sigma: 71\n",
      "t: 72\n",
      "tau: 73\n",
      "u: 74\n",
      "{: 75\n",
      "}: 76\n"
     ]
    }
   ],
   "source": [
    "# Create the vocabulary\n",
    "vocab = {}\n",
    "\n",
    "# Add special tokens\n",
    "special_tokens = [\"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"[STATE_ID]\", \"[UNK]\"]\n",
    "for i, token in enumerate(special_tokens):\n",
    "    vocab[token] = i\n",
    "\n",
    "# Add digits (0-9)\n",
    "for digit in range(10):\n",
    "    token = str(digit)\n",
    "    vocab[token] = len(vocab)\n",
    "\n",
    "# Add all unique tokens from the dataset\n",
    "for token in sorted(all_tokens):\n",
    "    if token not in vocab:  # Avoid duplicates (e.g., digits already added)\n",
    "        vocab[token] = len(vocab)\n",
    "\n",
    "# Print the vocabulary size and some entries\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(\"Sample vocabulary entries:\")\n",
    "for token, idx in list(vocab.items())[:10]:\n",
    "    print(f\"{token}: {idx}\")\n",
    "print(\"...\")\n",
    "for token, idx in list(vocab.items())[-10:]:\n",
    "    print(f\"{token}: {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
