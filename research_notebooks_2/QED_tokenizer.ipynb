{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sympy\n",
    "\n",
    "class QEDTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.vocab_reverse = {v: k for k, v in vocab.items()}\n",
    "        self.pad_token = \"[PAD]\"\n",
    "        self.pad_token_id = vocab[\"[PAD]\"]\n",
    "        self.unk_token = \"[UNK]\"\n",
    "        self.unk_token_id = vocab[\"[UNK]\"]\n",
    "        self.cls_token = \"[CLS]\"\n",
    "        self.cls_token_id = vocab[\"[CLS]\"]\n",
    "        self.sep_token = \"[SEP]\"\n",
    "        self.sep_token_id = vocab[\"[SEP]\"]\n",
    "\n",
    "    def pre_tokenize(self, text):\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            matched = False\n",
    "            # Skip the \"to\" keyword\n",
    "            if text[i:i+2].lower() == \"to\":\n",
    "                i += 2\n",
    "                continue\n",
    "\n",
    "            # Match special tokens like [STATE_ID]\n",
    "            if text[i] == '[':\n",
    "                end = text.find(']', i)\n",
    "                if end != -1:\n",
    "                    token = text[i:end+1]\n",
    "                    if token == '[STATE_ID]':\n",
    "                        tokens.append(token)\n",
    "                        i = end + 1\n",
    "                        matched = True\n",
    "\n",
    "            # Match LaTeX-like structures (e.g., gamma_{...}, A_{...}, e_{...}_u^(*))\n",
    "            if not matched and text[i:i+6].startswith('gamma_'):\n",
    "                start = i\n",
    "                i += 6\n",
    "                if i < len(text) and text[i] == '{':\n",
    "                    i += 1\n",
    "                    brace_count = 1\n",
    "                    content = []\n",
    "                    while i < len(text) and brace_count > 0:\n",
    "                        if text[i] == '{':\n",
    "                            brace_count += 1\n",
    "                        elif text[i] == '}':\n",
    "                            brace_count -= 1\n",
    "                        if brace_count > 0:\n",
    "                            content.append(text[i])\n",
    "                        i += 1\n",
    "                    tokens.append('gamma')\n",
    "                    tokens.append('{')\n",
    "                    content_str = ''.join(content)\n",
    "                    content_parts = []\n",
    "                    j = 0\n",
    "                    while j < len(content_str):\n",
    "                        if content_str[j:j+2] == r'\\l':\n",
    "                            end = j + 7  # Length of '\\lambda'\n",
    "                            latex_index = content_str[j:end]\n",
    "                            content_parts.append(latex_index)\n",
    "                            j = end\n",
    "                        elif content_str[j:j+2] == r'\\m':\n",
    "                            end = j + 3  # Length of '\\mu'\n",
    "                            latex_index = content_str[j:end]\n",
    "                            content_parts.append(latex_index)\n",
    "                            j = end\n",
    "                        elif content_str[j] == '[':\n",
    "                            end = content_str.find(']', j)\n",
    "                            if end != -1:\n",
    "                                content_parts.append(content_str[j:end+1])\n",
    "                                j = end + 1\n",
    "                            else:\n",
    "                                j += 1\n",
    "                        elif content_str[j] == '+':\n",
    "                            content_parts.append('+')\n",
    "                            j += 1\n",
    "                        else:\n",
    "                            j += 1\n",
    "                    for part in content_parts:\n",
    "                        tokens.append(part)\n",
    "                    tokens.append('}')\n",
    "                    matched = True\n",
    "\n",
    "            if not matched and text[i:i+2].startswith('A_'):\n",
    "                start = i\n",
    "                i += 2\n",
    "                if i < len(text) and text[i] == '\\\\':\n",
    "                    i += 1\n",
    "                    symbol_match = re.match(r'[a-zA-Z]+', text[i:])\n",
    "                    if symbol_match:\n",
    "                        token = symbol_match.group(0)\n",
    "                        tokens.append(f'A_{token}')\n",
    "                        i += len(token)\n",
    "                        matched = True\n",
    "\n",
    "            if not matched and text[i:i+2].startswith('e_'):\n",
    "                start = i\n",
    "                i += 2\n",
    "                if i < len(text) and text[i] == '{':\n",
    "                    i += 1\n",
    "                    brace_count = 1\n",
    "                    content = []\n",
    "                    while i < len(text) and brace_count > 0:\n",
    "                        if text[i] == '{':\n",
    "                            brace_count += 1\n",
    "                        elif text[i] == '}':\n",
    "                            brace_count -= 1\n",
    "                        if brace_count > 0:\n",
    "                            content.append(text[i])\n",
    "                        i += 1\n",
    "                    tokens.append('e')\n",
    "                    tokens.append('{')\n",
    "                    content_str = ''.join(content)\n",
    "                    content_parts = content_str.split()\n",
    "                    for part in content_parts:\n",
    "                        if part in {'i', 'j', 'k', 'l', 'gamma', '[STATE_ID]'}:\n",
    "                            tokens.append(part)\n",
    "                        else:\n",
    "                            tokens.append('gamma')  # Standardize 'gam' to 'gamma'\n",
    "                    tokens.append('}')\n",
    "                    if i + 2 <= len(text) and text[i:i+2] == '_u':\n",
    "                        tokens.append('_u')\n",
    "                        i += 2\n",
    "                    elif i + 2 <= len(text) and text[i:i+2] == '_v':\n",
    "                        tokens.append('_v')\n",
    "                        i += 2\n",
    "                    if i + 4 <= len(text) and text[i:i+4] == '^(*)':\n",
    "                        tokens.append('^')\n",
    "                        tokens.append('(')\n",
    "                        tokens.append('*')\n",
    "                        tokens.append(')')\n",
    "                        i += 4\n",
    "                    matched = True\n",
    "\n",
    "            # Handle tokens like del_7748_[STATE_ID]\n",
    "            if not matched:\n",
    "                state_id_pattern = re.match(r'([a-zA-Z]+)_(\\d+)_(\\[STATE_ID\\])', text[i:])\n",
    "                if state_id_pattern:\n",
    "                    prefix, number, state_id = state_id_pattern.groups()\n",
    "                    tokens.append(prefix)\n",
    "                    tokens.append(number)\n",
    "                    tokens.append(state_id)\n",
    "                    i += len(prefix) + 1 + len(number) + 1 + len(state_id)\n",
    "                    matched = True\n",
    "\n",
    "            # Handle particle names with indices (e.g., alpha_i, alpha_j)\n",
    "            if not matched:\n",
    "                particle_with_index = re.match(r'([a-zA-Z]+)_([a-zA-Z])(?=\\b|[^a-zA-Z0-9_])', text[i:])\n",
    "                if particle_with_index:\n",
    "                    particle, index = particle_with_index.groups()\n",
    "                    tokens.append(particle)\n",
    "                    tokens.append(index)\n",
    "                    i += len(particle) + 1 + len(index)\n",
    "                    matched = True\n",
    "\n",
    "            # Handle superscripts (e.g., x^2)\n",
    "            if not matched and text[i] == '^':\n",
    "                tokens.append('^')\n",
    "                i += 1\n",
    "                if i < len(text) and text[i].isdigit():\n",
    "                    num_match = re.match(r'\\d+', text[i:])\n",
    "                    if num_match:\n",
    "                        num = num_match.group(0)\n",
    "                        tokens.append(num)\n",
    "                        i += len(num)\n",
    "                    matched = True\n",
    "\n",
    "            # Handle fractions (e.g., \\frac{a}{b})\n",
    "            if not matched and text[i:i+5] == r'\\frac':\n",
    "                i += 5\n",
    "                if i < len(text) and text[i] == '{':\n",
    "                    i += 1\n",
    "                    brace_count = 1\n",
    "                    numerator = []\n",
    "                    while i < len(text) and brace_count > 0:\n",
    "                        if text[i] == '{':\n",
    "                            brace_count += 1\n",
    "                        elif text[i] == '}':\n",
    "                            brace_count -= 1\n",
    "                        if brace_count > 0:\n",
    "                            numerator.append(text[i])\n",
    "                        i += 1\n",
    "                    if i < len(text) and text[i] == '{':\n",
    "                        i += 1\n",
    "                        brace_count = 1\n",
    "                        denominator = []\n",
    "                        while i < len(text) and brace_count > 0:\n",
    "                            if text[i] == '{':\n",
    "                                brace_count += 1\n",
    "                            elif text[i] == '}':\n",
    "                                brace_count -= 1\n",
    "                            if brace_count > 0:\n",
    "                                denominator.append(text[i])\n",
    "                            i += 1\n",
    "                        num_str = ''.join(numerator)\n",
    "                        den_str = ''.join(denominator)\n",
    "                        if num_str.startswith('-'):\n",
    "                            tokens.append('-')\n",
    "                            num_str = num_str[1:]\n",
    "                        tokens.append('/')\n",
    "                        tokens.append(num_str)\n",
    "                        tokens.append(den_str)\n",
    "                        matched = True\n",
    "\n",
    "            # Match operators and parentheses\n",
    "            if not matched:\n",
    "                operators = r'(\\+|-|\\*|/|\\^|\\(|\\)|\\[|\\]|\\{|\\})'\n",
    "                match = re.match(operators, text[i:])\n",
    "                if match:\n",
    "                    token = match.group(0)\n",
    "                    tokens.append(token)\n",
    "                    i += len(token)\n",
    "                    matched = True\n",
    "\n",
    "            # Match symbols (e.g., e, mu_eps, s_12, m_e)\n",
    "            if not matched:\n",
    "                symbol_match = re.match(r'[a-zA-Z_][a-zA-Z0-9_]*', text[i:])\n",
    "                if symbol_match:\n",
    "                    token = symbol_match.group(0)\n",
    "                    # Split tokens like b_gam into b and gamma\n",
    "                    if '_' in token and token not in {'m_e', 'm_mu', 'm_nu', 'm_tau', 'm_b', 'm_c', 'm_d', 'm_s', 'm_t', 'm_u', 's_11', 's_12', 's_13', 's_14', 's_22', 's_23', 's_24', 's_33', 's_34', 's_44', '_u', '_v'}:\n",
    "                        parts = token.split('_')\n",
    "                        for part in parts:\n",
    "                            if part:\n",
    "                                tokens.append(part)\n",
    "                    else:\n",
    "                        tokens.append(token)\n",
    "                    i += len(token)\n",
    "                    matched = True\n",
    "\n",
    "            # Match multi-digit numbers as single tokens\n",
    "            if not matched and text[i].isdigit():\n",
    "                num_match = re.match(r'\\d+', text[i:])\n",
    "                if num_match:\n",
    "                    num = num_match.group(0)\n",
    "                    tokens.append(num)\n",
    "                    i += len(num)\n",
    "                    matched = True\n",
    "\n",
    "            # If no match, move to the next character\n",
    "            if not matched:\n",
    "                i += 1\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = self.pre_tokenize(text)\n",
    "        final_tokens = [self.cls_token] + tokens + [self.sep_token]\n",
    "        final_tokens = [token if token in self.vocab else self.unk_token for token in final_tokens]\n",
    "        return final_tokens\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        token_ids = [self.vocab[token] for token in tokens]\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids, for_sympy=False):\n",
    "        tokens = [self.vocab_reverse.get(tid, self.unk_token) for tid in token_ids]\n",
    "        tokens = [token for token in tokens if token not in {self.cls_token, self.sep_token, self.pad_token}]\n",
    "\n",
    "        if for_sympy:\n",
    "            sympy_tokens = []\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                token = tokens[i]\n",
    "                if token == '^':\n",
    "                    sympy_tokens.append('**')\n",
    "                elif token in {'_u', '_v'}:\n",
    "                    pass\n",
    "                elif token.startswith('%\\\\'):\n",
    "                    greek_letter = token[3:]\n",
    "                    sympy_tokens.append(greek_letter)\n",
    "                elif token == '[STATE_ID]':\n",
    "                    sympy_tokens.append('id1')\n",
    "                elif token in {'{', '}'}:\n",
    "                    pass\n",
    "                elif token == 'gamma':\n",
    "                    if i + 1 < len(tokens) and tokens[i + 1] == '{':\n",
    "                        brace_count = 1\n",
    "                        i += 2\n",
    "                        while i < len(tokens) and brace_count > 0:\n",
    "                            if tokens[i] == '{':\n",
    "                                brace_count += 1\n",
    "                            elif tokens[i] == '}':\n",
    "                                brace_count -= 1\n",
    "                            i += 1\n",
    "                        i -= 1\n",
    "                    sympy_tokens.append('gamma')\n",
    "                elif token.endswith('_[STATE_ID]'):\n",
    "                    base_token = token.replace('_[STATE_ID]', '')\n",
    "                    sympy_tokens.append(base_token)\n",
    "                else:\n",
    "                    sympy_tokens.append(token)\n",
    "                i += 1\n",
    "            return ' '.join(sympy_tokens)\n",
    "        else:\n",
    "            return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated vocabulary size: 116656\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'../QED_data/processed_2.csv')\n",
    "# Collect all tokens from the dataset\n",
    "all_texts = df['text'].tolist() + df['label'].tolist()\n",
    "all_tokens = set()\n",
    "for text in all_texts:\n",
    "    if text:\n",
    "        tokens = tokenizer.pre_tokenize(text)\n",
    "        all_tokens.update(tokens)\n",
    "\n",
    "# Create new vocabulary\n",
    "special_tokens = [\"[CLS]\", \"[SEP]\", \"[PAD]\", \"[STATE_ID]\", \"[UNK]\"]\n",
    "vocab = {token: i for i, token in enumerate(special_tokens + sorted(list(all_tokens)))}\n",
    "print(f\"Updated vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# Save the updated vocabulary\n",
    "with open(\"updated_dataset_vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab, f)\n",
    "with open(\"updated_dataset_vocab.txt\", \"w\") as f:\n",
    "    for token, idx in sorted(vocab.items(), key=lambda x: x[1]):\n",
    "        f.write(f\"{token}: {idx}\\n\")\n",
    "\n",
    "# Reinitialize the tokenizer\n",
    "tokenizer = QEDTokenizer(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 12441\n",
      "Validation dataset size: 3111\n",
      "\n",
      "Sample from dataset:\n",
      "Text (encoded): tensor([  0,  34,   5,  12,  37,  13,   5,  12,  37,  13,  38,  12,  14,  13,\n",
      "         34,   5,  12,  37,  13,  38,  12,  14,  13,   5,  12,  37,  13,  36,\n",
      "         34, 127,  12,  37,  13, 127,  12,  37,  13,  28,  12,  37,  13,  35,\n",
      "         34, 127,  12,  37,  13, 127,  12,  37,  13,  28,  12,  37,  13,  16,\n",
      "         22,  17,  27,  14,  76,  14,  68,  38,  20,  14,  75, 141,  15,   4,\n",
      "          4,   4,   4, 142,  14,  75, 141,   4,   4,   4,   4, 142,  14,   5,\n",
      "        141,   5,   5,   5, 142,  12,  37,  13,  39,  14,   5, 141,   5,   5,\n",
      "          5, 142,  12,  37,  13,  40,  38,  12,  14,  13,  14,   5, 141,   5,\n",
      "          5,   5, 142,  12,  37,  13,  39,  38,  12,  14,  13,  14,   5, 141,\n",
      "          5,   5,   5, 142,  12,  37,  13,  40,  17,  12,  37,  13,   1,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import pickle \n",
    "vocab = pickle.load(open('dataset_vocab.pkl', 'rb'))\n",
    "# Initialize the tokenizer\n",
    "tokenizer = QEDTokenizer(vocab)\n",
    "\n",
    "# Updated QEDDataset class to tokenize on-the-fly\n",
    "class QEDDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize and encode on-the-fly\n",
    "        text_encoded = self.tokenizer.encode(self.texts[idx])\n",
    "        label_encoded = self.tokenizer.encode(self.labels[idx])\n",
    "\n",
    "        # Truncate or pad to max_length\n",
    "        text_encoded = text_encoded[:self.max_length]\n",
    "        label_encoded = label_encoded[:self.max_length]\n",
    "\n",
    "        # Pad with [PAD] token if necessary\n",
    "        text_padding = [self.tokenizer.pad_token_id] * (self.max_length - len(text_encoded))\n",
    "        label_padding = [self.tokenizer.pad_token_id] * (self.max_length - len(label_encoded))\n",
    "\n",
    "        text_encoded += text_padding\n",
    "        label_encoded += label_padding\n",
    "\n",
    "        return {\n",
    "            'text': torch.tensor(text_encoded, dtype=torch.long),\n",
    "            'label': torch.tensor(label_encoded, dtype=torch.long),\n",
    "            'text_attention_mask': torch.tensor([1] * len(text_encoded) + [0] * len(text_padding), dtype=torch.long),\n",
    "            'label_attention_mask': torch.tensor([1] * len(label_encoded) + [0] * len(label_padding), dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r'../QED_data/processed_2.csv')\n",
    "# Split the dataset\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['text'].tolist(),\n",
    "    df['label'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "max_length = 512 # Adjust based on your dataset's sequence lengths\n",
    "train_dataset = QEDDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "val_dataset = QEDDataset(val_texts, val_labels, tokenizer, max_length)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Test a sample from the dataset\n",
    "sample = train_dataset[0]\n",
    "print(\"\\nSample from dataset:\")\n",
    "print(\"Text (encoded):\", sample['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label (encoded): tensor([  0,   5,  17,   5,  14,  68,  38,  22,  14,  12,   5,  14,  88,  38,\n",
      "         22,  15,  26,  14,  88,  38,  20,  14, 103,  15,  26,  14, 105,  14,\n",
      "        107,  15,  26,  14, 104,  14, 108,  15,  26,  14,  88,  38,  20,  14,\n",
      "        110,  13,  14,  12,  88,  38,  20,  15, 102,  15,  20,  14, 103,  15,\n",
      "         99,  13,  38,  12,  16,  20,  13,   1,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2])\n"
     ]
    }
   ],
   "source": [
    "print(\"Label (encoded):\", sample['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
