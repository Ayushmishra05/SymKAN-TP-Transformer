{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "from itertools import cycle\n",
    "import re\n",
    "import random\n",
    "from torchtext.vocab import vocab\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizer for processing symbolic mathematical expressions.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, index_token_pool_size, momentum_token_pool_size, special_symbols, UNK_IDX, to_replace):\n",
    "        self.amps = df.amplitude.tolist()\n",
    "        self.sqamps = df.squared_amplitude.tolist()\n",
    "\n",
    "        # Issue warnings if token pool sizes are too small\n",
    "        if index_token_pool_size < 100:\n",
    "            warnings.warn(f\"Index token pool size ({index_token_pool_size}) is small. Consider increasing it.\", UserWarning)\n",
    "        if momentum_token_pool_size < 100:\n",
    "            warnings.warn(f\"Momentum token pool size ({momentum_token_pool_size}) is small. Consider increasing it.\", UserWarning)\n",
    "        \n",
    "        # Generate token pools\n",
    "        self.tokens_pool = [f\"INDEX_{i}\" for i in range(index_token_pool_size)]\n",
    "        self.momentum_pool = [f\"MOMENTUM_{i}\" for i in range(momentum_token_pool_size)]\n",
    "        \n",
    "        # Regular expression patterns for token replacement\n",
    "        self.pattern_momentum = re.compile(r'\\b[ijkl]_\\d{1,}\\b')\n",
    "        self.pattern_num_123 = re.compile(r'\\b(?![ps]_)\\w+_\\d{1,}\\b')\n",
    "        self.pattern_special = re.compile(r'\\b\\w+_+\\w+\\b\\\\')\n",
    "        self.pattern_underscore_curly = re.compile(r'\\b\\w+_{')\n",
    "        self.pattern_prop = re.compile(r'Prop')\n",
    "        self.pattern_int = re.compile(r'int\\{')\n",
    "        self.pattern_operators = {\n",
    "            '+': re.compile(r'\\+'), '-': re.compile(r'-'), '*': re.compile(r'\\*'),\n",
    "            ',': re.compile(r','), '^': re.compile(r'\\^'), '%': re.compile(r'%'),\n",
    "            '}': re.compile(r'\\}'), '(': re.compile(r'\\('), ')': re.compile(r'\\)')\n",
    "        }\n",
    "        self.pattern_mass = re.compile(r'\\b\\w+_\\w\\b')\n",
    "        self.pattern_s = re.compile(r'\\b\\w+_\\d{2,}\\b')\n",
    "        self.pattern_reg_prop = re.compile(r'\\b\\w+_\\d{1}\\b')\n",
    "        self.pattern_antipart = re.compile(r'(\\w)_\\w+_\\d+\\(X\\)\\^\\(\\*\\)')\n",
    "        self.pattern_part = re.compile(r'(\\w)_\\w+_\\d+\\(X\\)')\n",
    "        self.pattern_index = re.compile(r'\\b\\w+_\\w+_\\d{2,}\\b')\n",
    "        \n",
    "        self.special_symbols = special_symbols\n",
    "        self.UNK_IDX = UNK_IDX\n",
    "        self.to_replace = to_replace\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_whitespace(expression):\n",
    "        \"\"\"Remove all forms of whitespace from the expression.\"\"\"\n",
    "        return re.sub(r'\\s+', '', expression)\n",
    "\n",
    "    @staticmethod\n",
    "    def split_expression(expression):\n",
    "        \"\"\"Split the expression by space delimiter.\"\"\"\n",
    "        return re.split(r' ', expression)\n",
    "\n",
    "    def build_tgt_vocab(self):\n",
    "        \"\"\"Build vocabulary for target sequences.\"\"\"\n",
    "        counter = Counter()\n",
    "        for eqn in tqdm(self.sqamps, desc='Processing target vocab'):\n",
    "            counter.update(self.tgt_tokenize(eqn))\n",
    "        voc = vocab(OrderedDict(counter), specials=self.special_symbols[:], special_first=True)\n",
    "        voc.set_default_index(self.UNK_IDX)\n",
    "        return voc\n",
    "\n",
    "    def build_src_vocab(self, seed):\n",
    "        \"\"\"Build vocabulary for source sequences.\"\"\"\n",
    "        counter = Counter()\n",
    "        for diag in tqdm(self.amps, desc='Processing source vocab'):\n",
    "            counter.update(self.src_tokenize(diag, seed))\n",
    "        voc = vocab(OrderedDict(counter), specials=self.special_symbols[:], special_first=True)\n",
    "        voc.set_default_index(self.UNK_IDX)\n",
    "        return voc\n",
    "    \n",
    "    def src_replace(self, ampl, seed):\n",
    "        \"\"\"Replace indexed and momentum variables with tokenized equivalents.\"\"\"\n",
    "        ampl = self.remove_whitespace(ampl)\n",
    "        \n",
    "        random.seed(seed)\n",
    "        token_cycle = cycle(random.sample(self.tokens_pool, len(self.tokens_pool)))\n",
    "        momentum_cycle = cycle(random.sample(self.momentum_pool, len(self.momentum_pool)))\n",
    "        \n",
    "        # Replace momentum tokens\n",
    "        temp_ampl = ampl\n",
    "        momentum_mapping = {match: next(momentum_cycle) for match in set(self.pattern_momentum.findall(ampl))}\n",
    "        for key, value in momentum_mapping.items():\n",
    "            temp_ampl = temp_ampl.replace(key, value)\n",
    "        \n",
    "        # Replace index tokens\n",
    "        num_123_mapping = {match: next(token_cycle) for match in set(self.pattern_num_123.findall(ampl))}\n",
    "        for key, value in num_123_mapping.items():\n",
    "            temp_ampl = temp_ampl.replace(key, value)\n",
    "\n",
    "        # Replace pattern index tokens\n",
    "        pattern_index_mapping = {match: f\"{'_'.join(match.split('_')[:-1])} {next(token_cycle)}\"\n",
    "                for match in set(self.pattern_index.findall(ampl))\n",
    "            }\n",
    "        for key, value in pattern_index_mapping.items():\n",
    "            temp_ampl = temp_ampl.replace(key, value)\n",
    "            \n",
    "        return temp_ampl\n",
    "    \n",
    "    def src_tokenize(self, ampl, seed):\n",
    "        \"\"\"Tokenize source expression, optionally applying replacements.\"\"\"\n",
    "        temp_ampl = self.src_replace(ampl, seed) if self.to_replace else ampl\n",
    "        temp_ampl = temp_ampl.replace('\\\\\\\\', '\\\\').replace('\\\\', ' \\\\ ').replace('%', '')\n",
    "\n",
    "        temp_ampl = self.pattern_underscore_curly.sub(lambda match: f' {match.group(0)} ', temp_ampl)\n",
    "\n",
    "        \n",
    "        for symbol, pattern in self.pattern_operators.items():\n",
    "            temp_ampl = pattern.sub(f' {symbol} ', temp_ampl)\n",
    "        \n",
    "        temp_ampl = re.sub(r' {2,}', ' ', temp_ampl)\n",
    "        return [token for token in self.split_expression(temp_ampl) if token]\n",
    "\n",
    "    def tgt_tokenize(self, sqampl):\n",
    "        \"\"\"Tokenize target expression.\"\"\"\n",
    "        sqampl = self.remove_whitespace(sqampl)\n",
    "        temp_sqampl = sqampl\n",
    "        \n",
    "        for symbol, pattern in self.pattern_operators.items():\n",
    "            temp_sqampl = pattern.sub(f' {symbol} ', temp_sqampl)\n",
    "        \n",
    "        for pattern in [self.pattern_reg_prop, self.pattern_mass, self.pattern_s]:\n",
    "            temp_sqampl = pattern.sub(lambda match: f' {match.group(0)} ', temp_sqampl)\n",
    "        \n",
    "        temp_sqampl = re.sub(r' {2,}', ' ', temp_sqampl)\n",
    "        return [token for token in self.split_expression(temp_sqampl) if token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing source vocab: 100%|██████████| 15552/15552 [00:03<00:00, 3892.58it/s]\n",
      "Processing target vocab: 100%|██████████| 15552/15552 [00:01<00:00, 10796.21it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"../QED_data/preprocessed_data.csv\")  \n",
    "\n",
    "index_token_pool_size = 200 \n",
    "momentum_token_pool_size = 200  \n",
    "special_symbols = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "UNK_IDX = 0  \n",
    "to_replace = True \n",
    "\n",
    "tokenizer = Tokenizer(df, index_token_pool_size, momentum_token_pool_size, special_symbols, UNK_IDX, to_replace)\n",
    "\n",
    "seed = 42 \n",
    "tokenized_amps = [tokenizer.src_tokenize(amp, seed) for amp in df[\"amplitude\"]]\n",
    "tokenized_sqamps = [tokenizer.tgt_tokenize(sqamp) for sqamp in df[\"squared_amplitude\"]]\n",
    "\n",
    "src_vocab = tokenizer.build_src_vocab(seed)\n",
    "tgt_vocab = tokenizer.build_tgt_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MathExprDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for handling tokenized mathematical expressions.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenized_src, tokenized_tgt, src_vocab, tgt_vocab):\n",
    "        self.tokenized_src = tokenized_src\n",
    "        self.tokenized_tgt = tokenized_tgt\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_tokens = self.tokenized_src[idx]\n",
    "        tgt_tokens = self.tokenized_tgt[idx]\n",
    "        \n",
    "        # Converting tokens to indices using vocab\n",
    "        src_indices = [self.src_vocab[token] for token in src_tokens]\n",
    "        tgt_indices = [self.tgt_vocab[token] for token in tgt_tokens]\n",
    "\n",
    "        return torch.tensor(src_indices), torch.tensor(tgt_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function that pads sequences to the maximum length in the batch.\n",
    "    \"\"\"\n",
    "    # Unzip the batch into source (src) and target (tgt) sequences\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "\n",
    "    # Pad sequences (ensure padding_value is a float)\n",
    "    src_batch_padded = pad_sequence(src_batch, batch_first=True, padding_value=float(src_vocab[\"<pad>\"]))\n",
    "    tgt_batch_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=float(tgt_vocab[\"<pad>\"]))\n",
    "\n",
    "    return src_batch_padded, tgt_batch_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_src, temp_src, train_tgt, temp_tgt = train_test_split(\n",
    "    tokenized_amps, tokenized_sqamps, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "val_src, test_src, val_tgt, test_tgt = train_test_split(\n",
    "    temp_src, temp_tgt, test_size=0.5, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MathExprDataset(tokenized_src=train_src,\n",
    "                                 tokenized_tgt=train_tgt,\n",
    "                                 src_vocab=src_vocab,\n",
    "                                 tgt_vocab=tgt_vocab)\n",
    "\n",
    "val_dataset = MathExprDataset(tokenized_src=val_src,\n",
    "                               tokenized_tgt=val_tgt,\n",
    "                               src_vocab=src_vocab,\n",
    "                               tgt_vocab=tgt_vocab)\n",
    "\n",
    "test_dataset = MathExprDataset(tokenized_src=test_src,\n",
    "                                tokenized_tgt=test_tgt,\n",
    "                                src_vocab=src_vocab,\n",
    "                                tgt_vocab=tgt_vocab)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Source Batch (src_batch): tensor([[79,  6,  7,  ...,  1,  1,  1],\n",
      "        [ 4, 78,  6,  ...,  1,  1,  1],\n",
      "        [ 4, 82,  6,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [79,  6,  7,  ...,  1,  1,  1],\n",
      "        [ 4,  7,  6,  ...,  1,  1,  1],\n",
      "        [ 7,  6,  8,  ...,  1,  1,  1]])\n",
      "Target Batch (tgt_batch): tensor([[51,  5,  6,  ...,  1,  1,  1],\n",
      "        [50,  5,  6,  ...,  1,  1,  1],\n",
      "        [53,  5,  6,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [51,  5,  6,  ...,  1,  1,  1],\n",
      "        [ 6,  7,  8,  ...,  1,  1,  1],\n",
      "        [12, 43,  5,  ..., 20,  1,  1]])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (src_batch, tgt_batch) in enumerate(train_dataloader):\n",
    "    print(f\"Batch {batch_idx+1}\")\n",
    "    \n",
    "    print(f\"Source Batch (src_batch): {src_batch}\")\n",
    "    \n",
    "    print(f\"Target Batch (tgt_batch): {tgt_batch}\")\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../src/Dataloaders/train_loader.pkl', 'wb') as fp:\n",
    "    pickle.dump(train_dataloader , fp) \n",
    "\n",
    "with open(r'../src/Dataloaders/test_loader.pkl', 'wb') as fp:\n",
    "    pickle.dump(train_dataloader , fp) \n",
    "\n",
    "with open(r'../src/Dataloaders/val_loader.pkl', 'wb') as fp:\n",
    "    pickle.dump(train_dataloader , fp) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
