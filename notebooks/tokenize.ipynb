{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "from itertools import cycle\n",
    "import re\n",
    "import random\n",
    "from torchtext.vocab import vocab\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pandas as pd \n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, df, index_token_pool_size, momentum_token_pool_size, special_symbols, UNK_IDX, to_replace):\n",
    "        self.amps = df.amplitude.tolist()\n",
    "        self.sqamps = df.squared_amplitude.tolist()\n",
    "        if index_token_pool_size < 100:\n",
    "            warnings.warn(f\"Index token pool size ({index_token_pool_size}) is small. Consider increasing it.\", UserWarning)\n",
    "        if momentum_token_pool_size < 100:\n",
    "            warnings.warn(f\"Momentum token pool size ({momentum_token_pool_size}) is small. Consider increasing it.\", UserWarning)\n",
    "        self.tokens_pool = [f\"INDEX_{i}\" for i in range(index_token_pool_size)]\n",
    "        self.momentum_pool = [f\"MOMENTUM_{i}\" for i in range(momentum_token_pool_size)]\n",
    "        self.pattern_momentum = re.compile(r'\\b[ijkl]_\\d{1,}\\b')\n",
    "        self.pattern_num_123 = re.compile(r'\\b(?![ps]_)\\w+_\\d{1,}\\b')\n",
    "        self.pattern_special = re.compile(r'\\b\\w+_+\\w+\\b\\\\')\n",
    "        self.pattern_underscore_curly = re.compile(r'\\b\\w+_{')\n",
    "        self.pattern_prop = re.compile(r'Prop')\n",
    "        self.pattern_int = re.compile(r'int\\{')\n",
    "        self.pattern_operators = {\n",
    "            '+': re.compile(r'\\+'), '-': re.compile(r'-'), '*': re.compile(r'\\*'),\n",
    "            ',': re.compile(r','), '^': re.compile(r'\\^'), '%': re.compile(r'%'),\n",
    "            '}': re.compile(r'\\}'), '(': re.compile(r'\\('), ')': re.compile(r'\\)')\n",
    "        }\n",
    "        self.pattern_mass = re.compile(r'\\b\\w+_\\w\\b')\n",
    "        self.pattern_s = re.compile(r'\\b\\w+_\\d{2,}\\b')\n",
    "        self.pattern_reg_prop = re.compile(r'\\b\\w+_\\d{1}\\b')\n",
    "        self.pattern_antipart = re.compile(r'(\\w)_\\w+_\\d+\\(X\\)\\^\\(\\*\\)')\n",
    "        self.pattern_part = re.compile(r'(\\w)_\\w+_\\d+\\(X\\)')\n",
    "        self.pattern_index = re.compile(r'\\b\\w+_\\w+_\\d{2,}\\b')\n",
    "        self.special_symbols = special_symbols\n",
    "        self.UNK_IDX = UNK_IDX\n",
    "        self.to_replace = to_replace\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_whitespace(expression):\n",
    "        return re.sub(r'\\s+', '', expression)\n",
    "\n",
    "    @staticmethod\n",
    "    def split_expression(expression):\n",
    "        return re.split(r' ', expression)\n",
    "\n",
    "    def build_tgt_vocab(self):\n",
    "        counter = Counter()\n",
    "        for eqn in tqdm(self.sqamps, desc='Processing target vocab'):\n",
    "            counter.update(self.tgt_tokenize(eqn))\n",
    "        voc = vocab(OrderedDict(counter), specials=self.special_symbols[:], special_first=True)\n",
    "        voc.set_default_index(self.UNK_IDX)\n",
    "        return voc\n",
    "\n",
    "    def build_src_vocab(self, seed):\n",
    "        counter = Counter()\n",
    "        for diag in tqdm(self.amps, desc='Processing source vocab'):\n",
    "            counter.update(self.src_tokenize(diag, seed))\n",
    "        voc = vocab(OrderedDict(counter), specials=self.special_symbols[:], special_first=True)\n",
    "        voc.set_default_index(self.UNK_IDX)\n",
    "        return voc\n",
    "    \n",
    "    def src_replace(self, ampl, seed):\n",
    "        ampl = self.remove_whitespace(ampl)\n",
    "        random.seed(seed)\n",
    "\n",
    "        momentum_indices = []\n",
    "        for match in self.pattern_momentum.findall(ampl):\n",
    "            if match not in momentum_indices:\n",
    "                momentum_indices.append(match)\n",
    "        momentum_mapping = {index: f\"MOMENTUM_{i}\" for i, index in enumerate(momentum_indices)}\n",
    "\n",
    "        all_indices = self.pattern_num_123.findall(ampl)\n",
    "        non_momentum_indices = []\n",
    "        for index in all_indices:\n",
    "            if index not in momentum_indices and index not in non_momentum_indices:\n",
    "                non_momentum_indices.append(index)\n",
    "        index_mapping = {index: f\"INDEX_{i}\" for i, index in enumerate(non_momentum_indices)}\n",
    "\n",
    "        pattern_index_matches = []\n",
    "        for match in self.pattern_index.findall(ampl):\n",
    "            if match not in pattern_index_matches:\n",
    "                pattern_index_matches.append(match)\n",
    "        pattern_index_mapping = {\n",
    "            match: f\"{'_'.join(match.split('_')[:-1])} INDEX_{i + len(non_momentum_indices)}\"\n",
    "            for i, match in enumerate(pattern_index_matches)\n",
    "        }\n",
    "\n",
    "        temp_ampl = ampl\n",
    "        for key, value in momentum_mapping.items():\n",
    "            temp_ampl = temp_ampl.replace(key, value)\n",
    "        for key, value in index_mapping.items():\n",
    "            temp_ampl = temp_ampl.replace(key, value)\n",
    "        for key, value in pattern_index_mapping.items():\n",
    "            temp_ampl = temp_ampl.replace(key, value)\n",
    "\n",
    "        return temp_ampl\n",
    "    \n",
    "    def src_tokenize(self, ampl, seed):\n",
    "        temp_ampl = self.src_replace(ampl, seed) if self.to_replace else ampl\n",
    "        temp_ampl = temp_ampl.replace('\\\\\\\\', '\\\\').replace('\\\\', ' \\\\ ').replace('%', '')\n",
    "        temp_ampl = self.pattern_underscore_curly.sub(lambda match: f' {match.group(0)} ', temp_ampl)\n",
    "        for symbol, pattern in self.pattern_operators.items():\n",
    "            temp_ampl = pattern.sub(f' {symbol} ', temp_ampl)\n",
    "        temp_ampl = re.sub(r' {2,}', ' ', temp_ampl)\n",
    "        return [token for token in self.split_expression(temp_ampl) if token]\n",
    "\n",
    "    def tgt_tokenize(self, sqampl):\n",
    "        sqampl = self.remove_whitespace(sqampl)\n",
    "        temp_sqampl = sqampl\n",
    "        for symbol, pattern in self.pattern_operators.items():\n",
    "            temp_sqampl = pattern.sub(f' {symbol} ', temp_sqampl)\n",
    "        for pattern in [self.pattern_reg_prop, self.pattern_mass, self.pattern_s]:\n",
    "            temp_sqampl = pattern.sub(lambda match: f' {match.group(0)} ', temp_sqampl)\n",
    "        temp_sqampl = re.sub(r' {2,}', ' ', temp_sqampl)\n",
    "        return [token for token in self.split_expression(temp_sqampl) if token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing source vocab: 100%|██████████| 15552/15552 [00:05<00:00, 2789.36it/s]\n",
      "Processing target vocab: 100%|██████████| 15552/15552 [00:03<00:00, 4389.53it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"../QED_data/preprocessed_data.csv\")  \n",
    "\n",
    "index_token_pool_size = 200 \n",
    "momentum_token_pool_size = 200  \n",
    "special_symbols = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "UNK_IDX = 0  \n",
    "to_replace = True \n",
    "\n",
    "tokenizer = Tokenizer(df, index_token_pool_size, momentum_token_pool_size, special_symbols, UNK_IDX, to_replace)\n",
    "\n",
    "seed = 42 \n",
    "tokenized_amps = [tokenizer.src_tokenize(amp, seed) for amp in df[\"amplitude\"]]\n",
    "tokenized_sqamps = [tokenizer.tgt_tokenize(sqamp) for sqamp in df[\"squared_amplitude\"]]\n",
    "\n",
    "src_vocab = tokenizer.build_src_vocab(seed)\n",
    "tgt_vocab = tokenizer.build_tgt_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MathExprDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for handling tokenized mathematical expressions.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenized_src, tokenized_tgt, src_vocab, tgt_vocab):\n",
    "        self.tokenized_src = tokenized_src\n",
    "        self.tokenized_tgt = tokenized_tgt\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_tokens = self.tokenized_src[idx]\n",
    "        tgt_tokens = self.tokenized_tgt[idx]\n",
    "        \n",
    "        # Converting tokens to indices using vocab\n",
    "        src_indices = [self.src_vocab[token] for token in src_tokens]\n",
    "        tgt_indices = [self.tgt_vocab[token] for token in tgt_tokens]\n",
    "\n",
    "        return torch.tensor(src_indices), torch.tensor(tgt_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function that pads sequences to the maximum length in the batch.\n",
    "    \"\"\"\n",
    "    # Unzip the batch into source (src) and target (tgt) sequences\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "\n",
    "    # Pad sequences (ensure padding_value is a float)\n",
    "    src_batch_padded = pad_sequence(src_batch, batch_first=True, padding_value=float(src_vocab[\"<pad>\"]))\n",
    "    tgt_batch_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=float(tgt_vocab[\"<pad>\"]))\n",
    "\n",
    "    return src_batch_padded, tgt_batch_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_src, temp_src, train_tgt, temp_tgt = train_test_split(\n",
    "    tokenized_amps, tokenized_sqamps, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "val_src, test_src, val_tgt, test_tgt = train_test_split(\n",
    "    temp_src, temp_tgt, test_size=0.5, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MathExprDataset(tokenized_src=train_src,\n",
    "                                 tokenized_tgt=train_tgt,\n",
    "                                 src_vocab=src_vocab,\n",
    "                                 tgt_vocab=tgt_vocab)\n",
    "\n",
    "val_dataset = MathExprDataset(tokenized_src=val_src,\n",
    "                               tokenized_tgt=val_tgt,\n",
    "                               src_vocab=src_vocab,\n",
    "                               tgt_vocab=tgt_vocab)\n",
    "\n",
    "test_dataset = MathExprDataset(tokenized_src=test_src,\n",
    "                                tokenized_tgt=test_tgt,\n",
    "                                src_vocab=src_vocab,\n",
    "                                tgt_vocab=tgt_vocab)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Source Batch (src_batch): tensor([[ 4,  5,  6,  ...,  1,  1,  1],\n",
      "        [74,  6,  7,  ...,  1,  1,  1],\n",
      "        [ 7,  6,  8,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [73,  6,  7,  ...,  1,  1,  1],\n",
      "        [ 7,  6,  8,  ...,  1,  1,  1],\n",
      "        [ 4,  7,  6,  ...,  1,  1,  1]])\n",
      "Target Batch (tgt_batch): tensor([[25,  5,  6,  ...,  1,  1,  1],\n",
      "        [54,  5,  6,  ...,  1,  1,  1],\n",
      "        [ 6,  7,  8,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [47,  5,  6,  ...,  1,  1,  1],\n",
      "        [12, 25,  5,  ..., 12, 21, 20],\n",
      "        [ 6,  7,  8,  ...,  1,  1,  1]])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (src_batch, tgt_batch) in enumerate(train_dataloader):\n",
    "    print(f\"Batch {batch_idx+1}\")\n",
    "    \n",
    "    print(f\"Source Batch (src_batch): {src_batch}\")\n",
    "    \n",
    "    print(f\"Target Batch (tgt_batch): {tgt_batch}\")\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../src/Dataloaders/train_loader.pkl', 'wb') as fp:\n",
    "    pickle.dump(train_dataloader , fp) \n",
    "\n",
    "with open(r'../src/Dataloaders/test_loader.pkl', 'wb') as fp:\n",
    "    pickle.dump(train_dataloader , fp) \n",
    "\n",
    "with open(r'../src/Dataloaders/val_loader.pkl', 'wb') as fp:\n",
    "    pickle.dump(train_dataloader , fp) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataset(input_file, output_file, seed=42):\n",
    "    df = pd.read_csv(input_file, sep=',')\n",
    "    \n",
    "    tokenizer = Tokenizer(\n",
    "        df=df,\n",
    "        index_token_pool_size=200,\n",
    "        momentum_token_pool_size=200,\n",
    "        special_symbols=['<pad>', '<unk>', '<bos>', '<eos>'],\n",
    "        UNK_IDX=1,\n",
    "        to_replace=True\n",
    "    )\n",
    "    \n",
    "    transformed_amps = []\n",
    "    for amp in df['amplitude']:\n",
    "        transformed_amp = tokenizer.src_replace(amp, seed=seed)\n",
    "        transformed_amp = transformed_amp.replace('%', '')\n",
    "        transformed_amps.append(transformed_amp)\n",
    "    \n",
    "    transformed_df = pd.DataFrame({\n",
    "        'amplitude': transformed_amps,\n",
    "        'squared_amplitude': df['squared_amplitude']\n",
    "    })\n",
    "    \n",
    "    transformed_df.to_csv(output_file, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    print(\"Data Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Saved\n"
     ]
    }
   ],
   "source": [
    "input_file = '../QED_data/preprocessed_data.csv'\n",
    "output_file = '../QED_data/normalized_data.csv'\n",
    "transform_dataset(input_file, output_file, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = pd.read_csv(output_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = len(normalized_data) \n",
    "\n",
    "train_data, test_data = train_test_split(normalized_data , test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(train_data , test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19904, 6222, 4978)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.size, test_data.size, val_data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(r'../QED_data/train_data.csv' , header=True, index=False)\n",
    "test_data.to_csv(r'../QED_data/test_data.csv', header=True, index=False)\n",
    "val_data.to_csv(r'../QED_data/val_data.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amplitude</th>\n",
       "      <th>squared_amplitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>-1/3*i*e^2*gamma_{\\INDEX_0,INDEX_1,INDEX_2}*ga...</td>\n",
       "      <td>1/9*e^4*(16*m_s^2*m_mu^2 + 8*m_s^2*s_12 + 8*s_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13127</th>\n",
       "      <td>-1/9*i*e^2*(p_1_\\INDEX_0*gamma_{\\INDEX_1,INDEX...</td>\n",
       "      <td>2/81*e^4*(s_14*s_34 + 2*m_b^2*s_44 + -1/2*s_13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9191</th>\n",
       "      <td>1/9*i*e^2*(p_1_\\INDEX_0*gamma_{\\INDEX_1,INDEX_...</td>\n",
       "      <td>2/81*e^4*(s_14*s_34 + 2*m_s^2*s_44 + -1/2*s_13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3744</th>\n",
       "      <td>1/3*i*e^2*gamma_{+\\INDEX_0,INDEX_1,INDEX_2}*ga...</td>\n",
       "      <td>1/9*e^4*(16*m_t^2*m_u^2 + 8*m_u^2*s_12 + 8*s_1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10631</th>\n",
       "      <td>-4/9*i*e^2*(p_1_\\INDEX_0*gamma_{\\INDEX_1,INDEX...</td>\n",
       "      <td>32/81*e^4*(s_14*s_34 + 2*m_tt^2*s_44 + -1/2*s_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>-1/6*i*e^2*gamma_{+\\INDEX_0,INDEX_1,INDEX_2}*g...</td>\n",
       "      <td>1/36*e^4*(16*m_b^2*m_e^2 + (-8)*m_b^2*s_13 + 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13558</th>\n",
       "      <td>i*e^2*(m_mu*gamma_{\\INDEX_0,INDEX_1,INDEX_2}*g...</td>\n",
       "      <td>-1/4*e^4*s_33*(64*m_mu^2 + (-16)*s_34)*(m_mu^2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>2/3*i*e^2*gamma_{+\\INDEX_0,INDEX_1,INDEX_2}*ga...</td>\n",
       "      <td>4/9*e^4*(16*m_e^2*m_tt^2 + (-8)*m_tt^2*s_13 + ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5575</th>\n",
       "      <td>-2/9*i*e^2*gamma_{\\INDEX_0,INDEX_1,INDEX_2}*ga...</td>\n",
       "      <td>4/81*e^4*(16*m_c^2*m_u^2 + 8*m_c^2*s_12 + 8*s_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11862</th>\n",
       "      <td>4/9*i*e^2*(m_c*gamma_{\\INDEX_0,INDEX_1,INDEX_2...</td>\n",
       "      <td>256/81*e^4*m_c^2*(m_c^2 + 1/4*s_13)*(m_c^2 + -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2489 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               amplitude  \\\n",
       "2221   -1/3*i*e^2*gamma_{\\INDEX_0,INDEX_1,INDEX_2}*ga...   \n",
       "13127  -1/9*i*e^2*(p_1_\\INDEX_0*gamma_{\\INDEX_1,INDEX...   \n",
       "9191   1/9*i*e^2*(p_1_\\INDEX_0*gamma_{\\INDEX_1,INDEX_...   \n",
       "3744   1/3*i*e^2*gamma_{+\\INDEX_0,INDEX_1,INDEX_2}*ga...   \n",
       "10631  -4/9*i*e^2*(p_1_\\INDEX_0*gamma_{\\INDEX_1,INDEX...   \n",
       "...                                                  ...   \n",
       "1421   -1/6*i*e^2*gamma_{+\\INDEX_0,INDEX_1,INDEX_2}*g...   \n",
       "13558  i*e^2*(m_mu*gamma_{\\INDEX_0,INDEX_1,INDEX_2}*g...   \n",
       "1301   2/3*i*e^2*gamma_{+\\INDEX_0,INDEX_1,INDEX_2}*ga...   \n",
       "5575   -2/9*i*e^2*gamma_{\\INDEX_0,INDEX_1,INDEX_2}*ga...   \n",
       "11862  4/9*i*e^2*(m_c*gamma_{\\INDEX_0,INDEX_1,INDEX_2...   \n",
       "\n",
       "                                       squared_amplitude  \n",
       "2221   1/9*e^4*(16*m_s^2*m_mu^2 + 8*m_s^2*s_12 + 8*s_...  \n",
       "13127  2/81*e^4*(s_14*s_34 + 2*m_b^2*s_44 + -1/2*s_13...  \n",
       "9191   2/81*e^4*(s_14*s_34 + 2*m_s^2*s_44 + -1/2*s_13...  \n",
       "3744   1/9*e^4*(16*m_t^2*m_u^2 + 8*m_u^2*s_12 + 8*s_1...  \n",
       "10631  32/81*e^4*(s_14*s_34 + 2*m_tt^2*s_44 + -1/2*s_...  \n",
       "...                                                  ...  \n",
       "1421   1/36*e^4*(16*m_b^2*m_e^2 + (-8)*m_b^2*s_13 + 8...  \n",
       "13558  -1/4*e^4*s_33*(64*m_mu^2 + (-16)*s_34)*(m_mu^2...  \n",
       "1301   4/9*e^4*(16*m_e^2*m_tt^2 + (-8)*m_tt^2*s_13 + ...  \n",
       "5575   4/81*e^4*(16*m_c^2*m_u^2 + 8*m_c^2*s_12 + 8*s_...  \n",
       "11862  256/81*e^4*m_c^2*(m_c^2 + 1/4*s_13)*(m_c^2 + -...  \n",
       "\n",
       "[2489 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
