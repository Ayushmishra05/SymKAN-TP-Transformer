{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "GN_jDSZPNxch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "fvWEmZPSPUka",
        "outputId": "c9766576-f292-4603-c2bf-e03f9cde482e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mayush89718\u001b[0m (\u001b[33mayush89718-alliance-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "import math"
      ],
      "metadata": {
        "id": "XmyRl6hv73Ch"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SineKAN1D(nn.Module):\n",
        "    \"\"\"\n",
        "    Input:  (..., input_dim)  e.g., (B, L)\n",
        "    Output: (..., output_dim) e.g., (B, O)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim, device='cuda', grid_size=8, is_first=False, add_bias=True, norm_freq=True):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.grid_size = grid_size\n",
        "\n",
        "        device = torch.device(device)\n",
        "        dtype = torch.get_default_dtype()\n",
        "\n",
        "        # Frequencies as a 1D learnable vector\n",
        "        freq = torch.arange(1, grid_size + 1, dtype=dtype, device=device)\n",
        "        if norm_freq:\n",
        "            freq = freq / ((grid_size + 1) ** (0 if is_first else 1))\n",
        "        self.freq = nn.Parameter(freq)  # (G,)\n",
        "\n",
        "        # Phase matrix (K, G) precomputed\n",
        "        input_phase = torch.linspace(0, math.pi, input_dim, dtype=dtype, device=device) * input_dim  # (K,)\n",
        "        grid_phase = torch.arange(1, grid_size + 1, dtype=dtype, device=device) / (grid_size + 1) * grid_size  # (G,)\n",
        "        phase = input_phase[:, None] + grid_phase[None, :]  # (K, G)\n",
        "        self.register_buffer('phase', phase)\n",
        "\n",
        "        # Amplitudes as (O, K, G). Final matmul done with nn.functional.linear on flattened KG.\n",
        "        if is_first:\n",
        "            amp = torch.empty(output_dim, input_dim, grid_size, dtype=dtype, device=device).normal_(0, 0.4)\n",
        "        else:\n",
        "            amp = torch.empty(output_dim, input_dim, grid_size, dtype=dtype, device=device).uniform_(-1, 1)\n",
        "        grid_norm = torch.arange(1, grid_size + 1, dtype=dtype, device=device)  # (G,)\n",
        "        amp = amp / output_dim / grid_norm[None, None, :]\n",
        "        self.amplitudes = nn.Parameter(amp)  # (O, K, G)\n",
        "\n",
        "        if add_bias:\n",
        "            self.bias = nn.Parameter(torch.ones(output_dim, dtype=dtype, device=device) / output_dim)\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "    @property\n",
        "    def _W(self):\n",
        "        # (O, K*G)\n",
        "        return self.amplitudes.reshape(self.output_dim, self.input_dim * self.grid_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Support arbitrary leading dims ending with input_dim\n",
        "        out_shape = x.shape[:-1] + (self.output_dim,)\n",
        "        x2 = x.reshape(-1, self.input_dim)  # (N, K)\n",
        "        # Compute sin with minimal broadcasting: (N, K, G)\n",
        "        s = torch.sin(x2[..., :, None] * self.freq[None, None, :] + self.phase[None, :, :])\n",
        "        # Dense linear over flattened (K*G)\n",
        "        y = nn.functional.linear(s.reshape(-1, self.input_dim * self.grid_size), self._W, self.bias)  # (N, O)\n",
        "        return y.reshape(out_shape)\n",
        "\n",
        "\n",
        "class SineKANSeqFeat(nn.Module):\n",
        "    \"\"\"\n",
        "    Input:  (B, L, F) where F == input_dim\n",
        "    Output: (B, L, O) broadcast along L\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim, device='cuda', grid_size=8, is_first=False, add_bias=True, norm_freq=True):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.grid_size = grid_size\n",
        "\n",
        "        device = torch.device(device)\n",
        "        dtype = torch.get_default_dtype()\n",
        "\n",
        "        freq = torch.arange(1, grid_size + 1, dtype=dtype, device=device)\n",
        "        if norm_freq:\n",
        "            freq = freq / ((grid_size + 1) ** (0 if is_first else 1))\n",
        "        self.freq = nn.Parameter(freq)  # (G,)\n",
        "\n",
        "        input_phase = torch.linspace(0, math.pi, input_dim, dtype=dtype, device=device) * input_dim  # (F,)\n",
        "        grid_phase = torch.arange(1, grid_size + 1, dtype=dtype, device=device) / (grid_size + 1) * grid_size  # (G,)\n",
        "        phase = input_phase[:, None] + grid_phase[None, :]  # (F, G)\n",
        "        self.register_buffer('phase', phase)\n",
        "\n",
        "        if is_first:\n",
        "            amp = torch.empty(output_dim, input_dim, grid_size, dtype=dtype, device=device).normal_(0, 0.4)\n",
        "        else:\n",
        "            amp = torch.empty(output_dim, input_dim, grid_size, dtype=dtype, device=device).uniform_(-1, 1)\n",
        "        grid_norm = torch.arange(1, grid_size + 1, dtype=dtype, device=device)\n",
        "        amp = amp / output_dim / grid_norm[None, None, :]\n",
        "        self.amplitudes = nn.Parameter(amp)  # (O, F, G)\n",
        "\n",
        "        if add_bias:\n",
        "            self.bias = nn.Parameter(torch.ones(output_dim, dtype=dtype, device=device) / output_dim)\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "    @property\n",
        "    def _W(self):\n",
        "        return self.amplitudes.reshape(self.output_dim, self.input_dim * self.grid_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L, F = x.shape\n",
        "        assert F == self.input_dim\n",
        "        # (B, L, F, G)\n",
        "        s = torch.sin(x.unsqueeze(-1) * self.freq.view(1, 1, 1, -1) + self.phase.view(1, 1, F, -1))\n",
        "        # Dense linear per time step\n",
        "        y = nn.functional.linear(s.reshape(B, L, F * self.grid_size), self._W, self.bias).reshape(B, L, self.output_dim)\n",
        "        return y"
      ],
      "metadata": {
        "id": "R6WmTFPwYum2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MoeLayer(nn.Module):\n",
        "  def __init__(self , d_model , n_experts , k):\n",
        "    super().__init__()\n",
        "    self.n_experts = n_experts\n",
        "    self.experts = nn.ModuleList([SineKAN1D(d_model , d_model , grid_size=8) for i in range(self.n_experts)])\n",
        "    self.gate = nn.Linear(d_model , self.n_experts)\n",
        "    self.k = k\n",
        "\n",
        "  def forward(self , x):\n",
        "    gate_logits = self.gate(x)\n",
        "    weights , selected_experts = torch.topk(gate_logits , k = self.k)\n",
        "    weights = F.softmax(weights , dim=-1)\n",
        "    out = torch.zeros_like(x)\n",
        "    for i , current_expert in enumerate(self.experts):\n",
        "      batch_idx , seq_idx , k_idx = torch.where(selected_experts == i)\n",
        "      token_x = x[batch_idx, seq_idx]\n",
        "      token_w = weights[batch_idx, seq_idx, k_idx].unsqueeze(-1)\n",
        "      expert_out = current_expert(token_x)\n",
        "      out[batch_idx, seq_idx] += token_w * expert_out\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "ZoL6ZpRBQR4h"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model , n_heads, max_seq_len , bias = False) -> None:\n",
        "    super().__init__()\n",
        "    assert d_model % n_heads == 0 , \"d_model not divisible by n_heads\"\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    self.bias = bias\n",
        "    self.qw = nn.Linear(self.d_model , self.d_model , bias = self.bias)\n",
        "    self.kw = nn.Linear(self.d_model , self.d_model , bias = self.bias)\n",
        "    self.vw = nn.Linear(self.d_model , self.d_model , bias = self.bias)\n",
        "    self.project = nn.Linear(self.d_model , self.d_model)\n",
        "    self.rope = RotaryPositionalEncoding(d_model=d_model, max_seq_len=max_seq_len)\n",
        "\n",
        "  # def cross_forward()\n",
        "\n",
        "  def forward(self, q , k , v ,  mask = None):\n",
        "    ## X dimension ==> (batch_size, seq_len, dim)\n",
        "\n",
        "    ## (batch_dim, seq_len, d_out)\n",
        "    q = self.qw(q)\n",
        "    k = self.kw(k)\n",
        "    v = self.vw(v)\n",
        "\n",
        "    k = self.rope(k)\n",
        "    v = self.rope(v)\n",
        "\n",
        "    # (batch seq_len d_out) -> (batch n_heads seq_len d_out)\n",
        "    q = rearrange(q , \"b s (h d) -> b h s d\", h = self.n_heads)\n",
        "    k = rearrange(k , \"b s (h d) -> b h s d\", h = self.n_heads)\n",
        "    v = rearrange(v , \"b s (h d) -> b h s d\", h = self.n_heads)\n",
        "\n",
        "    attention_scores = (q @ k.transpose(-2 , -1)) / (k.shape[-1]**0.5)\n",
        "\n",
        "    # if mask:\n",
        "    #   masks = torch.triu(torch.ones(seq_len , seq_len) , diagonal=1)\n",
        "    #   attention_scores = attention_scores.masked_fill(masks == 1 , float(\"-inf\"))\n",
        "    #   attention_weights = F.softmax(attention_scores/(k.shape[-1]**0.5) , dim=-1)\n",
        "    # else:\n",
        "    #   attention_weights = F.softmax(attention_scores/(k.shape[-1]**0.5) , dim=-1)\n",
        "\n",
        "\n",
        "    if mask is not None:\n",
        "      attention_scores = attention_scores.masked_fill(mask == 0 , -1e9)\n",
        "    attention_weights = F.softmax(attention_scores , dim = -1)\n",
        "\n",
        "    context_vector = attention_weights @ v\n",
        "\n",
        "    context_vector = rearrange(context_vector , \"b h s d -> b s (h d)\")\n",
        "    return self.project(context_vector)\n"
      ],
      "metadata": {
        "id": "lrrlhdwl8Pb7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_step(i_n, grid_size, A, K, C):\n",
        "    ratio = A * grid_size**(-K) + C\n",
        "    i_n1 = ratio * i_n\n",
        "    return i_n1\n",
        "\n",
        "class SineKANLayer(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, device='cuda', grid_size=5, is_first=False, add_bias=True, norm_freq=True):\n",
        "        super(SineKANLayer,self).__init__()\n",
        "        self.grid_size = grid_size\n",
        "        self.device = device\n",
        "        self.is_first = is_first\n",
        "        self.add_bias = add_bias\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.A, self.K, self.C = 0.9724108095811765, 0.9884401790754128, 0.999449553483052\n",
        "\n",
        "        self.grid_norm_factor = (torch.arange(grid_size) + 1)\n",
        "        self.grid_norm_factor = self.grid_norm_factor.reshape(1, 1, grid_size)\n",
        "\n",
        "        if is_first:\n",
        "            self.amplitudes = torch.nn.Parameter(torch.empty(output_dim, input_dim, 1).normal_(0, .4) / output_dim  / self.grid_norm_factor)\n",
        "        else:\n",
        "            self.amplitudes = torch.nn.Parameter(torch.empty(output_dim, input_dim, 1).uniform_(-1, 1) / output_dim  / self.grid_norm_factor)\n",
        "\n",
        "        grid_phase = torch.arange(1, grid_size + 1).reshape(1, 1, 1, grid_size) / (grid_size + 1)\n",
        "        self.input_phase = torch.linspace(0, math.pi, input_dim).reshape(1, 1, input_dim, 1).to(device)\n",
        "        phase = grid_phase.to(device) + self.input_phase\n",
        "\n",
        "        if norm_freq:\n",
        "            self.freq = torch.nn.Parameter(torch.arange(1, grid_size + 1).float().reshape(1, 1, 1, grid_size) / (grid_size + 1)**(1 - is_first))\n",
        "        else:\n",
        "            self.freq = torch.nn.Parameter(torch.arange(1, grid_size + 1).float().reshape(1, 1, 1, grid_size))\n",
        "\n",
        "        for i in range(1, self.grid_size):\n",
        "            phase = forward_step(phase, i, self.A, self.K, self.C)\n",
        "        # self.phase = torch.nn.Parameter(phase)\n",
        "        self.register_buffer('phase', phase)\n",
        "\n",
        "        if self.add_bias:\n",
        "            self.bias  = torch.nn.Parameter(torch.ones(1, output_dim) / output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_shape = x.shape\n",
        "        output_shape = x_shape[0:-1] + (self.output_dim,)\n",
        "        x = torch.reshape(x, (-1, self.input_dim))\n",
        "        x_reshaped = torch.reshape(x, (x.shape[0], 1, x.shape[1], 1))\n",
        "        s = torch.sin(x_reshaped * self.freq + self.phase)\n",
        "        y = torch.einsum('ijkl,jkl->ij', s, self.amplitudes)\n",
        "        if self.add_bias:\n",
        "            y += self.bias\n",
        "        y = torch.reshape(y, output_shape)\n",
        "        return y"
      ],
      "metadata": {
        "id": "dE0UcPzJF9wC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Expansion and Contraction Module\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_in) -> None:\n",
        "    super().__init__()\n",
        "    self.ff = nn.Sequential(\n",
        "        nn.Linear(d_in , 4 * d_in),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(4 * d_in , d_in)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.ff(x)"
      ],
      "metadata": {
        "id": "oou7b-7CBUnz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, d_model , n_heads , max_seq_len , dropout_ratio = 0.2 , bias = False) -> None:\n",
        "    super().__init__()\n",
        "    self.attention = MultiHeadAttention(d_model=d_model , n_heads=n_heads , max_seq_len=max_seq_len , bias = bias)\n",
        "    self.ff = FeedForward(d_in=d_model)\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "  def forward(self , x , src_mask):\n",
        "    attention_out = self.attention(x , x , x , src_mask)\n",
        "    x = x + self.norm1(self.dropout(attention_out))\n",
        "    ff_out = self.ff(x)\n",
        "    x = x + self.norm2(self.dropout(ff_out))\n",
        "    return x"
      ],
      "metadata": {
        "id": "nfWAOrfVEEIO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, d_model , max_seq_len , n_heads , dropout_ratio = 0.2 , bias = False) -> None:\n",
        "    super().__init__()\n",
        "    self.attention = MultiHeadAttention(d_model=d_model , n_heads=n_heads , max_seq_len=max_seq_len , bias = bias)\n",
        "    self.ff = FeedForward(d_in=d_model)\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.norm3 = nn.LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "  def forward(self, x , enc_output , src_mask, tgt_mask):\n",
        "    attention_outputs = self.attention(x , x, x, tgt_mask)\n",
        "    x = x + self.norm1(self.dropout(attention_outputs))\n",
        "    cross_attention_outputs = self.attention(x , enc_output , enc_output ,  src_mask)\n",
        "    x = x + self.norm2(self.dropout(cross_attention_outputs))\n",
        "    ff_output = self.ff(x)\n",
        "    x = x + self.norm3(self.dropout(ff_output))\n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7qspPdvgFtK6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self , d_model , max_seq_len):\n",
        "    super().__init__()\n",
        "    self.pos = torch.arange(0 , max_seq_len)\n",
        "    self.theta = 1 / ((10000 ** (torch.arange(0 , d_model , 2))) / d_model)\n",
        "\n",
        "    pe = torch.zeros(max_seq_len , d_model)\n",
        "\n",
        "    pe[...,0::2] = torch.sin(self.pos[: , None] / self.theta)\n",
        "    pe[...,1::2] = torch.cos(self.pos[: , None] / self.theta)\n",
        "\n",
        "    self.register_buffer(\"pe\" , pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    b , s , d = x.shape\n",
        "    return x + self.pe[:s , :]"
      ],
      "metadata": {
        "id": "QWZDxzGhFzBc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mask(src , tgt):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  src_mask = (src != 0).to(device)\n",
        "  src_mask = src_mask[: , None , None , :]\n",
        "  tgt_mask = (tgt != 0)[: , None , : , None].to(device)\n",
        "  seq_len = tgt_mask.shape[-2]\n",
        "  causal_mask = torch.tril(torch.ones(1 , seq_len , seq_len)).bool().to(device)\n",
        "  final_tgt_mask = tgt_mask & causal_mask\n",
        "  return src_mask , final_tgt_mask.to(device)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self , src_vocab_size , d_model , tgt_vocab_size , max_seq_len , n_heads , dropout_ratio , bias , n_encoders , n_decoders, n_experts , k) -> None:\n",
        "    super().__init__()\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.encod_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "    self.decod_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "    self.encoding = PositionalEncoding(d_model=d_model, max_seq_len=max_seq_len)\n",
        "    self.encoder = nn.ModuleList([Encoder(d_model=d_model, n_heads=n_heads, max_seq_len=max_seq_len , dropout_ratio=dropout_ratio , bias = bias) for i in range(n_encoders)])\n",
        "    self.decoder = nn.ModuleList([Decoder(d_model=d_model, n_heads=n_heads , max_seq_len=max_seq_len , dropout_ratio=dropout_ratio , bias = bias) for i in range(n_encoders)])\n",
        "    self.linear = nn.Linear(d_model , tgt_vocab_size)\n",
        "    self.dropout = nn.Dropout(dropout_ratio)\n",
        "    self.ff = MoeLayer(d_model=d_model , n_experts=n_experts , k=k)\n",
        "    self.ff2 = FeedForward(d_model)\n",
        "\n",
        "  def forward(self , src, tgt):\n",
        "    src_mask , tgt_mask = get_mask(src , tgt)\n",
        "    src_mask = src_mask.to(self.device)\n",
        "    tgt_mask = tgt_mask.to(self.device)\n",
        "    encod_embed = self.dropout(self.encod_embedding(src))\n",
        "    decod_embed = self.dropout(self.decod_embedding(tgt))\n",
        "\n",
        "    enc_output = encod_embed\n",
        "    for encoder in self.encoder:\n",
        "      enc_output = encoder(enc_output , src_mask)\n",
        "\n",
        "    dec_output = decod_embed\n",
        "    for decoder in self.decoder:\n",
        "      dec_output = decoder(dec_output , enc_output , src_mask , tgt_mask )\n",
        "    # print(\"Decoder output \" , dec_output)\n",
        "    out = self.ff(dec_output)\n",
        "\n",
        "    # out = self.ff(dec_output)\n",
        "    return self.linear(out)\n"
      ],
      "metadata": {
        "id": "kP0XdZMCF1_U"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RotaryPositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model , max_seq_len) -> None:\n",
        "    super().__init__()\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.d_model = d_model\n",
        "    self.half_d  = d_model // 2\n",
        "    self.pos = torch.arange(0 , max_seq_len)\n",
        "    self.theta = 1 / (10000 ** ((2 * torch.arange(0 , self.half_d)) / self.d_model))\n",
        "    self.angles = self.pos[: , None] * self.theta[None , :]\n",
        "    sin = torch.sin(self.angles)\n",
        "    cos = torch.cos(self.angles)\n",
        "    self.register_buffer(\"sin\" , sin)\n",
        "    self.register_buffer(\"cos\" , cos)\n",
        "\n",
        "  def forward(self , x):\n",
        "    assert x.shape[-2] <= self.max_seq_len , \"seq len should be less than max_seq_len\"\n",
        "    seq_len = x.shape[-2]\n",
        "    x1 = x[..., 0::2]\n",
        "    x2 = x[..., 1::2]\n",
        "    x_rot_1 = x1 * self.cos[:seq_len , :]  - x2 * self.sin[:seq_len , :]\n",
        "    x_rot_2 = x1 * self.sin[:seq_len , :]  + x2 * self.cos[:seq_len , :]\n",
        "\n",
        "    out = torch.zeros_like(x)\n",
        "    out[..., 0::2] = x_rot_1\n",
        "    out[..., 1::2] = x_rot_2\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "GUZG96bJGnNx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Union, Dict , OrderedDict\n",
        "import warnings\n",
        "import re\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset , DataLoader\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "yNVNvcDrYc44"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SymbolicQEDTokenizer:\n",
        "    def __init__(self, df=None, index_token_pool_size=100, special_symbols=None, unk_idx=1, to_replace=True):\n",
        "        self.amps = df.amp.tolist() if df is not None else None\n",
        "        self.sqamps = df.sqamp.tolist() if df is not None else None\n",
        "        if index_token_pool_size < 50:\n",
        "            warnings.warn(f\"Index token pool size ({index_token_pool_size}) may be insufficient. Consider using at least 50-100 tokens for symbolic tasks.\", UserWarning)\n",
        "        self.index_pool = [f\"INDEX_{i}\" for i in range(index_token_pool_size)]\n",
        "        self.particle_index_pool = [f\"PINDEX_{i}\" for i in range(index_token_pool_size)]\n",
        "        self.special_symbols = special_symbols or [\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\", \"<SEP>\"]\n",
        "        self.unk_idx = unk_idx\n",
        "        self.to_replace = to_replace\n",
        "        self.pattern_underscore_curly = re.compile(r'\\b[\\w]+(?:_[\\w]+)*_{')\n",
        "        self.pattern_mass = re.compile(r'\\bm_([a-z]+)\\b')\n",
        "        self.pattern_mandelstam = re.compile(r'\\bs_(\\d{2,})\\b')\n",
        "        self.pattern_momentum = re.compile(r'\\bp_(\\d+)\\b')\n",
        "        self.pattern_single_s = re.compile(r'\\bs_(\\d+)\\b(?!\\d)')\n",
        "        self.pattern_exponent = re.compile(r'\\^(\\w+|\\([^)]+\\))')\n",
        "        self.pattern_special = re.compile(r'_([uv])|\\\\(\\w+_\\d+|\\w+\\b)')\n",
        "        self.pattern_num_123 = re.compile(r'\\b(?![psijkl]_)(?!MOMENTUM_)(?!MASS_)(?!P_)(?!S_)(?!MANDELSTAM_)\\w+_\\d+\\b')\n",
        "        self.pattern_particle = re.compile(r'(?P<prefix>\\b(?:\\w+_)?)?(?P<target>[ijkl]_\\d+\\b)')\n",
        "\n",
        "    def preprocess_expression(self, expr):\n",
        "        expr = expr.replace(' * ', '*').replace(' / ', '/').replace(' ^ ', '^')\n",
        "        expr = expr.replace(' + ', '+').replace(' - ', '-')\n",
        "        expr = expr.replace(\"+-\", \"-\")\n",
        "        expr = expr.replace(\"-+\", \"-\")\n",
        "        expr = ' '.join(expr.split())\n",
        "        expr = expr.replace('me', 'm_e')\n",
        "        return expr\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_whitespace(expression: str):\n",
        "        return re.sub(r'\\s+', '', expression)\n",
        "\n",
        "    def protect_structures(self, ampl: str):\n",
        "        protected = []\n",
        "        return ampl, protected\n",
        "\n",
        "    def physics_aware_replace(self, ampl: str, is_source: bool = True):\n",
        "        ampl = self.remove_whitespace(ampl)\n",
        "        ampl = re.sub(r'\\bi\\b(?!\\w)', 'I_UNIT', ampl)\n",
        "        ampl = re.sub(r'\\be\\b(?=\\^|[+\\-*/()| ])', 'E_CHARGE', ampl)\n",
        "        ampl = ampl.replace('reg_prop', 'REG_PROP')\n",
        "        ampl = self.pattern_mandelstam.sub(r'MANDELSTAM_\\1', ampl)\n",
        "        ampl = self.pattern_momentum.sub(r'P_\\1', ampl)\n",
        "        ampl = self.pattern_single_s.sub(r'S_\\1', ampl)\n",
        "        ampl = ampl.replace('(*)', 'CONJ')\n",
        "        return ampl\n",
        "\n",
        "    def replace_indices(self, ampl: str, is_source: bool = True):\n",
        "        if not self.to_replace:\n",
        "            return ampl\n",
        "        index_pool = iter(self.index_pool)\n",
        "        particle_index_pool = iter(self.particle_index_pool)\n",
        "        index_pool_set = set(self.index_pool) if is_source else set()\n",
        "        ampl = self.pattern_mandelstam.sub(lambda m: f'MANDELSTAM_{m.group(1)}', ampl)\n",
        "\n",
        "        def get_unique_matches(pattern):\n",
        "            matches = list(OrderedDict.fromkeys(pattern.findall(ampl)))\n",
        "            return [m for m in matches if m not in index_pool_set]\n",
        "\n",
        "        def replace_particle_tokens():\n",
        "            nonlocal ampl\n",
        "            matches = list(OrderedDict.fromkeys(m.group('target') for m in sorted(self.pattern_particle.finditer(ampl), key=lambda m: m.start())))\n",
        "            try:\n",
        "                mapping = {m: next(particle_index_pool) for m in matches}\n",
        "            except StopIteration:\n",
        "                raise RuntimeError(\"particle_index_pool exhausted. Increase the size of the particle_index_pool.\")\n",
        "            for key in sorted(mapping.keys(), key=len, reverse=True):\n",
        "                ampl = ampl.replace(key, mapping[key])\n",
        "\n",
        "        matches = get_unique_matches(self.pattern_num_123)\n",
        "        try:\n",
        "            for match in matches:\n",
        "                ampl = ampl.replace(match, next(index_pool))\n",
        "        except StopIteration:\n",
        "            raise RuntimeError(\"index_pool exhausted. Increase pool size.\")\n",
        "        replace_particle_tokens()\n",
        "        return ampl\n",
        "\n",
        "    def tokenize_expression(self, ampl: str, protected: List[str], is_source: bool = True):\n",
        "        ampl = ampl.replace('\\\\\\\\', '\\\\')\n",
        "        def replace_special(match):\n",
        "            if match.group(1):\n",
        "                return f' _ {match.group(1)} '\n",
        "            elif match.group(2):\n",
        "                return f' \\\\ {match.group(2)} '\n",
        "        ampl = self.pattern_special.sub(replace_special, ampl)\n",
        "        if is_source:\n",
        "            ampl = self.pattern_underscore_curly.sub(lambda match: f' {match.group(0)} ', ampl)\n",
        "            for symbol in ['{', '}', ',']:\n",
        "                ampl = ampl.replace(symbol, f' {symbol} ')\n",
        "        for symbol in ['/', '+', '-', '*', '(', ')', '^']:\n",
        "            ampl = ampl.replace(symbol, f' {symbol} ')\n",
        "        ampl = self.pattern_exponent.sub(r' ^ \\1 ', ampl)\n",
        "        ampl = ampl.replace('_PINDEX', '_ PINDEX').replace('_INDEX', '_ INDEX')\n",
        "        ampl = ampl.replace('REG_PROP', ' reg_prop ')\n",
        "        ampl = re.sub(r' +', ' ', ampl).strip()\n",
        "        tokens = [token for token in ampl.split(' ') if token]\n",
        "        final_tokens = []\n",
        "        for token in tokens:\n",
        "            if token.startswith('PROTECTED_'):\n",
        "                try:\n",
        "                    idx = int(token.split('_')[1])\n",
        "                    final_tokens.append(protected[idx])\n",
        "                except (IndexError, ValueError):\n",
        "                    final_tokens.append(token)\n",
        "            else:\n",
        "                final_tokens.append(token)\n",
        "        return final_tokens\n",
        "\n",
        "    def src_tokenize(self, ampl: str):\n",
        "        try:\n",
        "            ampl = self.preprocess_expression(ampl)\n",
        "            ampl, protected = self.protect_structures(ampl)\n",
        "            ampl = self.physics_aware_replace(ampl, is_source=True)\n",
        "            ampl = self.replace_indices(ampl, is_source=True)\n",
        "            return self.tokenize_expression(ampl, protected, is_source=True)\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"Source tokenization failed for '{ampl}': {e}\")\n",
        "            return [self.special_symbols[self.unk_idx]]\n",
        "\n",
        "    def tgt_tokenize(self, sqampl: str):\n",
        "        try:\n",
        "            sqampl = self.preprocess_expression(sqampl)\n",
        "            sqampl, protected = self.protect_structures(sqampl)\n",
        "            sqampl = self.physics_aware_replace(sqampl, is_source=False)\n",
        "            sqampl = self.replace_indices(sqampl, is_source=False)\n",
        "            return self.tokenize_expression(sqampl, protected, is_source=False)\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"Target tokenization failed for '{sqampl}': {e}\")\n",
        "            return [self.special_symbols[self.unk_idx]]\n",
        "\n",
        "    def build_src_vocab(self):\n",
        "        if self.amps is None:\n",
        "            return set()\n",
        "        vocab_set = set()\n",
        "        start_time = time.time()\n",
        "        for expr in tqdm(self.amps, desc=\"Processing source vocab\"):\n",
        "            vocab_set.update(self.src_tokenize(expr))\n",
        "        end_time = time.time()\n",
        "        print(f\"Source vocab built in {end_time - start_time:.2f} seconds, size: {len(vocab_set)}\")\n",
        "        return vocab_set\n",
        "\n",
        "    def build_tgt_vocab(self):\n",
        "        if self.sqamps is None:\n",
        "            return set()\n",
        "        vocab_set = set()\n",
        "        start_time = time.time()\n",
        "        for expr in tqdm(self.sqamps, desc=\"Processing target vocab\"):\n",
        "            vocab_set.update(self.tgt_tokenize(expr))\n",
        "        end_time = time.time()\n",
        "        print(f\"Target vocab built in {end_time - start_time:.2f} seconds, size: {len(vocab_set)}\")\n",
        "        return vocab_set\n",
        "\n",
        "class SymbolicVocab:\n",
        "    def __init__(self, tokens: set, special_symbols: list, bos_idx: int, pad_idx: int, eos_idx: int, unk_idx: int, sep_idx: int):\n",
        "        self.token_list = special_symbols + sorted(list(tokens))\n",
        "        self.token_to_idx = {token: idx for idx, token in enumerate(self.token_list)}\n",
        "        self.idx_to_token = {idx: token for token, idx in self.token_to_idx.items()}\n",
        "        self.unk_idx = unk_idx\n",
        "        self.pad_idx = pad_idx\n",
        "        self.bos_idx = bos_idx\n",
        "        self.eos_idx = eos_idx\n",
        "        self.sep_idx = sep_idx\n",
        "        self.unk_tok = special_symbols[unk_idx]\n",
        "        self.pad_tok = special_symbols[pad_idx]\n",
        "        self.bos_tok = special_symbols[bos_idx]\n",
        "        self.eos_tok = special_symbols[eos_idx]\n",
        "        self.sep_tok = special_symbols[sep_idx]\n",
        "\n",
        "    def encode(self, tokens: list):\n",
        "        return [self.token_to_idx.get(token, self.unk_idx) for token in tokens]\n",
        "\n",
        "    def decode(self, indices: list, include_special_tokens: bool = True):\n",
        "        if include_special_tokens:\n",
        "            return [self.idx_to_token.get(idx, self.unk_tok) for idx in indices]\n",
        "        return [self.idx_to_token.get(idx, self.unk_tok) for idx in indices if idx not in {self.pad_idx, self.bos_idx, self.eos_idx, self.sep_idx}]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_list)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        if isinstance(item, int):\n",
        "            return self.idx_to_token.get(item, self.unk_tok)\n",
        "        return self.token_to_idx.get(item, self.unk_idx)\n",
        "\n",
        "class QEDDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        start_time = time.time()\n",
        "        self.src_vocab = SymbolicVocab(tokens=tokenizer.build_src_vocab(), special_symbols=tokenizer.special_symbols, bos_idx=2, pad_idx=0, eos_idx=3, unk_idx=1, sep_idx=4)\n",
        "        self.tgt_vocab = SymbolicVocab(tokens=tokenizer.build_tgt_vocab(), special_symbols=tokenizer.special_symbols, bos_idx=2, pad_idx=0, eos_idx=3, unk_idx=1, sep_idx=4)\n",
        "        end_time = time.time()\n",
        "        print(f\"Dataset initialized in {end_time - start_time:.2f} seconds, src_vocab_size: {len(self.src_vocab)}, tgt_vocab_size: {len(self.tgt_vocab)}\")\n",
        "        if len(self.src_vocab) == 5 or len(self.tgt_vocab) == 5:\n",
        "            warnings.warn(\"Vocabulary size is minimal (only special tokens). Check dataset or tokenization.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = str(self.data.iloc[idx][\"amp\"])\n",
        "        trg = str(self.data.iloc[idx][\"sqamp\"])\n",
        "        src_tokens = self.tokenizer.src_tokenize(src)\n",
        "        trg_tokens = self.tokenizer.tgt_tokenize(trg)\n",
        "        src_ids = self.src_vocab.encode(src_tokens)\n",
        "        trg_ids = self.tgt_vocab.encode(trg_tokens)\n",
        "        src_ids = src_ids[:self.max_length] + [self.src_vocab.pad_idx] * (self.max_length - len(src_ids))\n",
        "        trg_ids = trg_ids[:self.max_length] + [self.tgt_vocab.pad_idx] * (self.max_length - len(trg_ids))\n",
        "        return {\"input_ids\": torch.tensor(src_ids, dtype=torch.long), \"labels\": torch.tensor(trg_ids, dtype=torch.long)}\n"
      ],
      "metadata": {
        "id": "VY3DDt_tYaiJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.read_csv(r'/content/train_data.csv')\n",
        "\n",
        "start_time = time.time()\n",
        "tokenizer = SymbolicQEDTokenizer(df=data_df, index_token_pool_size=100, special_symbols=[\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\", \"<SEP>\"], to_replace=True)\n",
        "src_vocab_size = len(tokenizer.build_src_vocab()) + 5\n",
        "tgt_vocab_size = len(tokenizer.build_tgt_vocab()) + 5\n",
        "\n",
        "start_time = time.time()\n",
        "dataset = QEDDataset(data_df, tokenizer, 300)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ_ilRKhYyb9",
        "outputId": "f01b6383-6b46-4e7b-937b-598863b82307"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing source vocab: 100%|██████████| 9952/9952 [00:02<00:00, 3450.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source vocab built in 2.89 seconds, size: 78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing target vocab: 100%|██████████| 9952/9952 [00:01<00:00, 5357.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target vocab built in 1.86 seconds, size: 45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing source vocab: 100%|██████████| 9952/9952 [00:02<00:00, 3512.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source vocab built in 2.84 seconds, size: 78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing target vocab: 100%|██████████| 9952/9952 [00:01<00:00, 5397.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target vocab built in 1.85 seconds, size: 45\n",
            "Dataset initialized in 4.68 seconds, src_vocab_size: 83, tgt_vocab_size: 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset , batch_size = 64)"
      ],
      "metadata": {
        "id": "CrVTwd6zZ0WI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_loader:\n",
        "  print(batch['input_ids'].shape , batch['labels'].shape)\n",
        "  print(batch['input_ids'][0] , batch['labels'][0])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdUvLF1naUgx",
        "outputId": "e8098008-4517-4922-c783-862dddaa9c85"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 300]) torch.Size([64, 300])\n",
            "tensor([12, 11, 19,  7, 37,  7, 22, 53, 14,  7, 59, 81,  8, 52, 23,  9, 24,  9,\n",
            "        29, 82,  7, 59, 81, 52, 23,  9, 30,  9, 31, 82,  7, 55, 81, 44,  9, 31,\n",
            "        82,  5, 50,  6, 54, 80,  7, 55, 81, 45,  9, 30, 82,  5, 51,  6, 54, 78,\n",
            "        53, 21,  7, 56, 81, 46,  9, 29, 82,  5, 48,  6, 54, 78,  7, 56, 81, 47,\n",
            "         9, 24, 82,  5, 49,  6, 54, 80, 53, 21, 11,  5, 62, 53, 14,  8, 39,  8,\n",
            "        12, 11, 14,  7, 74,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]) tensor([11, 10, 23,  7, 25, 37, 19,  7,  5, 13,  7, 40, 37, 14,  7, 41, 37, 14,\n",
            "         8, 22,  7, 40, 37, 14,  7, 28,  8, 22,  7, 30,  7, 32,  8, 22,  7, 29,\n",
            "         7, 33,  8, 22,  7, 41, 37, 14,  7, 35,  6,  7,  5, 41, 37, 14,  8, 28,\n",
            "         8, 11, 10, 14,  7, 48,  6, 37,  5,  9, 14,  6,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BwXtJN5jw2c",
        "outputId": "6799c12b-1bfa-41f5-a2c8-32d2ac522b82"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "model = Transformer(\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    d_model=512,\n",
        "    tgt_vocab_size=tgt_vocab_size,\n",
        "    max_seq_len=300,\n",
        "    n_heads=4,\n",
        "    dropout_ratio=0.1,\n",
        "    bias=False,\n",
        "    n_encoders=4,\n",
        "    n_decoders=4,\n",
        "    n_experts=8,\n",
        "    k=3\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "epochs = 50\n",
        "optimizer = optim.Adam(model.parameters() , lr=3e-6)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "HZ6RK9LkNHBb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "e-biJ1fl36us"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVhOd_VxbY_O",
        "outputId": "50ef31ce-c3ff-4bac-a710-680072a48757"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Sep 25 15:31:07 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P0             31W /   70W |     298MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from einops import rearrange\n",
        "\n",
        "def token_accuracy(output, target):\n",
        "    \"\"\"\n",
        "    output: (batch, seq_len, vocab_size)\n",
        "    target: (batch, seq_len)\n",
        "    \"\"\"\n",
        "    preds = torch.argmax(output, dim=-1)       # (batch, seq_len)\n",
        "    acc = (preds == target).float().mean().item()\n",
        "    return acc\n",
        "\n",
        "def sequence_accuracy(output, target):\n",
        "    \"\"\"\n",
        "    output: (batch, seq_len, vocab_size)\n",
        "    target: (batch, seq_len)\n",
        "    \"\"\"\n",
        "    preds = torch.argmax(output, dim=-1)       # (batch, seq_len)\n",
        "    correct_sequences = (preds == target).all(dim=1)  # True if all tokens match\n",
        "    seq_acc = correct_sequences.float().mean().item()\n",
        "    return seq_acc\n",
        "\n",
        "# # Device\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = model.to(device)\n",
        "\n",
        "# Training\n",
        "\n",
        "epochs = 10\n",
        "start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_token_acc = 0.0\n",
        "    total_seq_acc = 0.0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        src = batch['input_ids'].to(device)\n",
        "        target = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, target)  # (batch, seq_len, vocab_size)\n",
        "\n",
        "        # Flatten for loss\n",
        "        output_flat = rearrange(output, 'b s c -> (b s) c')\n",
        "        target_flat = rearrange(target, 'b s -> (b s)')\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output_flat, target_flat)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Compute accuracies\n",
        "        total_token_acc += token_accuracy(output, target)\n",
        "        total_seq_acc += sequence_accuracy(output, target)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    avg_token_acc = total_token_acc / len(train_loader)\n",
        "    avg_seq_acc = total_seq_acc / len(train_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | \"\n",
        "          f\"Token Acc: {avg_token_acc:.4f} | Seq Acc: {avg_seq_acc:.4f}\")\n",
        "end_time = time.time()\n",
        "\n",
        "print(end_time - start_time)\n"
      ],
      "metadata": {
        "id": "qmTHzk9uhyoF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "402d9b0f-87e4-40fe-bfc1-e77931ced7e5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Loss: 1.5058 | Token Acc: 0.7788 | Seq Acc: 0.0000\n",
            "Epoch 2/10 | Loss: 0.2677 | Token Acc: 0.9486 | Seq Acc: 0.0069\n",
            "Epoch 3/10 | Loss: 0.1068 | Token Acc: 0.9855 | Seq Acc: 0.1518\n",
            "Epoch 4/10 | Loss: 0.0488 | Token Acc: 0.9961 | Seq Acc: 0.4174\n",
            "Epoch 5/10 | Loss: 0.0265 | Token Acc: 0.9979 | Seq Acc: 0.6086\n",
            "Epoch 6/10 | Loss: 0.0172 | Token Acc: 0.9982 | Seq Acc: 0.6783\n",
            "Epoch 7/10 | Loss: 0.0122 | Token Acc: 0.9989 | Seq Acc: 0.7335\n",
            "Epoch 8/10 | Loss: 0.0091 | Token Acc: 0.9991 | Seq Acc: 0.7960\n",
            "Epoch 9/10 | Loss: 0.0069 | Token Acc: 0.9994 | Seq Acc: 0.8736\n",
            "Epoch 10/10 | Loss: 0.0053 | Token Acc: 0.9997 | Seq Acc: 0.9247\n",
            "2611.1186583042145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FPzw-u0dcbXL"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}